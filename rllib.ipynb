{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "from gym.spaces import Discrete\n",
    "from gym import spaces\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.pg.pg import PGTrainer\n",
    "from ray.rllib.agents.pg.pg_tf_policy import PGTFPolicy\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.utils import try_import_tf\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--stop\", type=int, default=1000)\n",
    "\n",
    "tf = try_import_tf()\n",
    "\n",
    "ROCK = 0\n",
    "PAPER = 1\n",
    "SCISSORS = 2\n",
    "\n",
    "\n",
    "class RockPaperScissorsEnv(MultiAgentEnv):\n",
    "    \"\"\"Two-player environment for rock paper scissors.\n",
    "    The observation is simply the last opponent action.\"\"\"\n",
    "\n",
    "    def __init__(self, _):\n",
    "        self.action_space = Discrete(3)\n",
    "        self.observation_space = Discrete(3)\n",
    "        self.player1 = \"player1\"\n",
    "        self.player2 = \"player2\"\n",
    "        self.last_move = None\n",
    "        self.num_moves = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.last_move = (0, 0)\n",
    "        self.num_moves = 0\n",
    "        return {\n",
    "            self.player1: self.last_move[1],\n",
    "            self.player2: self.last_move[0],\n",
    "        }\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        move1 = action_dict[self.player1]\n",
    "        move2 = action_dict[self.player2]\n",
    "        self.last_move = (move1, move2)\n",
    "        obs = {\n",
    "            self.player1: self.last_move[1],\n",
    "            self.player2: self.last_move[0],\n",
    "        }\n",
    "        r1, r2 = {\n",
    "            (ROCK, ROCK): (0, 0),\n",
    "            (ROCK, PAPER): (-1, 1),\n",
    "            (ROCK, SCISSORS): (1, -1),\n",
    "            (PAPER, ROCK): (1, -1),\n",
    "            (PAPER, PAPER): (0, 0),\n",
    "            (PAPER, SCISSORS): (-1, 1),\n",
    "            (SCISSORS, ROCK): (-1, 1),\n",
    "            (SCISSORS, PAPER): (1, -1),\n",
    "            (SCISSORS, SCISSORS): (0, 0),\n",
    "        }[move1, move2]\n",
    "        rew = {\n",
    "            self.player1: r1,\n",
    "            self.player2: r2,\n",
    "        }\n",
    "        self.num_moves += 1\n",
    "        done = {\n",
    "            \"__all__\": self.num_moves >= 10,\n",
    "        }\n",
    "        return obs, rew, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEnv(MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    Simple test environment\n",
    "    The agents have a state represented by 2 variables. The agents can move the\n",
    "    value of any agentâ€™s state variable (including their own) up or down by 1. \n",
    "    \n",
    "    Action space \n",
    "    List of number of points to add to each variable in range [0, 5)\n",
    "    [agent1.var1, agent1.var2, agent2.var1, agent2.var2]\n",
    "    \n",
    "    Observation space\n",
    "    The value of every variable for every agent. The last value is the id of the agent.\n",
    "    [agent1.var1, agent1.var2, agent2.var1, agent2.var2, id]\n",
    "    \n",
    "    Reward\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, _):\n",
    "        self.n_agents = 2\n",
    "        self.n_vars = 2\n",
    "        self._step_count = None\n",
    "        \n",
    "        action_space = tuple([(spaces.Box(low=0, high=5, shape=(self.n_vars,))) for i in range(self.n_agents)])\n",
    "        self.action_space = spaces.Tuple(action_space)        \n",
    "        \n",
    "        observation_space = [(spaces.Box(low=-np.inf, high=np.inf, shape=(self.n_vars,))) for i in range(self.n_agents)]\n",
    "        observation_space.append(spaces.Discrete(self.n_agents))\n",
    "        observation_space = tuple(observation_space)\n",
    "        self.observation_space = spaces.Tuple(observation_space)\n",
    "\n",
    "        self.agents_var = None\n",
    "        self._agent_dones = None\n",
    "        self._total_episode_reward = None\n",
    "        self.steps_beyond_done = None\n",
    "        \n",
    "        self.agent_ids = []\n",
    "        self.agent_idx = {}\n",
    "        for i in range(self.n_agents):\n",
    "            self.agent_ids.append('agent_' + str(i))\n",
    "            self.agent_idx['agent_' + str(i)] = i\n",
    "        \n",
    "    def get_agent_obs(self):\n",
    "        _obs = {}\n",
    "        for agent_i in range(self.n_agents):\n",
    "            # add state\n",
    "            _agent_i_obs = copy.copy(self.agent_var)\n",
    "\n",
    "            #add agent id\n",
    "            _agent_i_obs.append(agent_i)\n",
    "\n",
    "            _obs[self.agent_ids[agent_i]] = _agent_i_obs\n",
    "\n",
    "        return _obs\n",
    "    \n",
    "    def reset(self):\n",
    "        self.agent_var = [([0] * self.n_vars) for i in range(self.n_agents)]\n",
    "        self._step_count = 0\n",
    "        self._total_episode_reward = [0 for _ in range(self.n_agents)]\n",
    "        self._agent_dones = [False for _ in range(self.n_agents)]\n",
    "        \n",
    "        return self.get_agent_obs()\n",
    "\n",
    "    def __update_agent_action(self, agent_i, action):\n",
    "        action = (action - np.mean(action)).astype(int)\n",
    "        # Make the actions have a bigger affect on the other agent than itself.\n",
    "        scale = [2] * self.n_agents\n",
    "        scale[self.agent_idx[agent_i]] = 1\n",
    "        action *= scale\n",
    "        self.agent_var = (np.array(self.agent_var) + action).tolist()\n",
    "        #print(self.agent_var)\n",
    "\n",
    "    def get_first_values(self, agent_var):\n",
    "        \"\"\"Gives the first value for each agent.\"\"\"\n",
    "        return np.array(agent_var)[:,0]\n",
    "    \n",
    "    def get_rewards(self, pre_agent_var):\n",
    "        \"\"\"Rewards\"\"\"\n",
    "        rewards = self.get_first_values(self.agent_var) - self.get_first_values(pre_agent_var)\n",
    "        reward_dict = {}\n",
    "        for i, r in enumerate(rewards):\n",
    "            reward_dict[self.agent_ids[i]] = r\n",
    "        for i in range(self.n_agents):\n",
    "            self._total_episode_reward[i] += rewards[i]\n",
    "        return reward_dict\n",
    "    \n",
    "    def step(self, action_dict):\n",
    "        assert len(action_dict) == self.n_agents\n",
    "\n",
    "        self._step_count += 1\n",
    "\n",
    "        pre_agent_var = self.agent_var\n",
    "\n",
    "        for agent_i, action in action_dict.items():\n",
    "            self.__update_agent_action(agent_i, action)\n",
    "\n",
    "        rewards = self.get_rewards(pre_agent_var)\n",
    "        \n",
    "        \n",
    "        done = {\n",
    "            \"__all__\": False,\n",
    "        }\n",
    "\n",
    "        return self.get_agent_obs(), rewards, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/anaconda3/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = SimpleEnv(None)\n",
    "env.reset()\n",
    "new_obs, rewards, dones, infos = env.step(action_dict={'agent_0': [0, 0], 'agent_1': [0, 0]})\n",
    "env.n_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log sync requires rsync to be installed.\n",
      "/home/victor/anaconda3/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "2020-04-28 00:52:06,007\tWARNING trainer_template.py:124 -- The experimental distributed execution API is enabled for this algorithm. Disable this by setting 'use_exec_api': False.\n",
      "Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'episode_reward_max': nan, 'episode_reward_min': nan, 'episode_reward_mean': nan, 'episode_len_mean': nan, 'episodes_this_iter': 0, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [], 'episode_lengths': []}, 'sampler_perf': {}, 'off_policy_estimator': {}, 'num_healthy_workers': 0, 'timesteps_total': 400, 'timers': {'sample_time_ms': 315.185, 'sample_throughput': 1269.097, 'learn_time_ms': 161.834, 'learn_throughput': 2471.672}, 'info': {'learner': {'model': {}}, 'num_steps_sampled': 400, 'num_steps_trained': 400}, 'done': False, 'episodes_total': 0, 'training_iteration': 1, 'experiment_id': 'f7f492935452438d80a6ccb66444ea54', 'date': '2020-04-28_00-52-06', 'timestamp': 1588049526, 'time_this_iter_s': 0.4772932529449463, 'time_total_s': 0.4772932529449463, 'pid': 8897, 'hostname': 'victor-solus', 'node_ip': '192.168.1.236', 'config': {'num_workers': 0, 'num_envs_per_worker': 1, 'rollout_fragment_length': 200, 'sample_batch_size': -1, 'batch_mode': 'truncate_episodes', 'num_gpus': 0, 'train_batch_size': 200, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_action_dist': None, 'custom_options': {}, 'custom_preprocessor': None}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env_config': {}, 'env': 'SimpleEnv', 'normalize_actions': False, 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 0.0004, 'monitor': False, 'log_level': 'WARN', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None, 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'use_pytorch': False, 'eager': False, 'eager_tracing': False, 'no_eager_on_workers': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'use_exec_api': True, 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'memory': 0, 'object_store_memory': 0, 'memory_per_worker': 0, 'object_store_memory_per_worker': 0, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}}, 'time_since_restore': 0.4772932529449463, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'perf': {'cpu_util_percent': 32.6, 'ram_util_percent': 48.2}}\n",
      "{'episode_reward_max': nan, 'episode_reward_min': nan, 'episode_reward_mean': nan, 'episode_len_mean': nan, 'episodes_this_iter': 0, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [], 'episode_lengths': []}, 'sampler_perf': {}, 'off_policy_estimator': {}, 'num_healthy_workers': 0, 'timesteps_total': 3200, 'timers': {'sample_time_ms': 291.625, 'sample_throughput': 1371.627, 'learn_time_ms': 23.197, 'learn_throughput': 17243.524}, 'info': {'learner': {'model': {}}, 'num_steps_sampled': 3200, 'num_steps_trained': 3200}, 'done': False, 'episodes_total': 0, 'training_iteration': 2, 'experiment_id': 'f7f492935452438d80a6ccb66444ea54', 'date': '2020-04-28_00-52-08', 'timestamp': 1588049528, 'time_this_iter_s': 2.0426623821258545, 'time_total_s': 2.519955635070801, 'pid': 8897, 'hostname': 'victor-solus', 'node_ip': '192.168.1.236', 'config': {'num_workers': 0, 'num_envs_per_worker': 1, 'rollout_fragment_length': 200, 'sample_batch_size': -1, 'batch_mode': 'truncate_episodes', 'num_gpus': 0, 'train_batch_size': 200, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_action_dist': None, 'custom_options': {}, 'custom_preprocessor': None}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env_config': {}, 'env': 'SimpleEnv', 'normalize_actions': False, 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 0.0004, 'monitor': False, 'log_level': 'WARN', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None, 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'use_pytorch': False, 'eager': False, 'eager_tracing': False, 'no_eager_on_workers': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'use_exec_api': True, 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'memory': 0, 'object_store_memory': 0, 'memory_per_worker': 0, 'object_store_memory_per_worker': 0, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}}, 'time_since_restore': 2.519955635070801, 'timesteps_since_restore': 0, 'iterations_since_restore': 2, 'perf': {'cpu_util_percent': 42.233333333333334, 'ram_util_percent': 48.36666666666667}}\n",
      "{'episode_reward_max': nan, 'episode_reward_min': nan, 'episode_reward_mean': nan, 'episode_len_mean': nan, 'episodes_this_iter': 0, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [], 'episode_lengths': []}, 'sampler_perf': {}, 'off_policy_estimator': {}, 'num_healthy_workers': 0, 'timesteps_total': 6400, 'timers': {'sample_time_ms': 278.923, 'sample_throughput': 1434.088, 'learn_time_ms': 2.933, 'learn_throughput': 136371.304}, 'info': {'learner': {'model': {}}, 'num_steps_sampled': 6400, 'num_steps_trained': 6400}, 'done': False, 'episodes_total': 0, 'training_iteration': 3, 'experiment_id': 'f7f492935452438d80a6ccb66444ea54', 'date': '2020-04-28_00-52-10', 'timestamp': 1588049530, 'time_this_iter_s': 2.270685911178589, 'time_total_s': 4.79064154624939, 'pid': 8897, 'hostname': 'victor-solus', 'node_ip': '192.168.1.236', 'config': {'num_workers': 0, 'num_envs_per_worker': 1, 'rollout_fragment_length': 200, 'sample_batch_size': -1, 'batch_mode': 'truncate_episodes', 'num_gpus': 0, 'train_batch_size': 200, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_action_dist': None, 'custom_options': {}, 'custom_preprocessor': None}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env_config': {}, 'env': 'SimpleEnv', 'normalize_actions': False, 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 0.0004, 'monitor': False, 'log_level': 'WARN', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None, 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'use_pytorch': False, 'eager': False, 'eager_tracing': False, 'no_eager_on_workers': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'use_exec_api': True, 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'memory': 0, 'object_store_memory': 0, 'memory_per_worker': 0, 'object_store_memory_per_worker': 0, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}}, 'time_since_restore': 4.79064154624939, 'timesteps_since_restore': 0, 'iterations_since_restore': 3, 'perf': {'cpu_util_percent': 36.333333333333336, 'ram_util_percent': 48.4}}\n",
      "{'episode_reward_max': nan, 'episode_reward_min': nan, 'episode_reward_mean': nan, 'episode_len_mean': nan, 'episodes_this_iter': 0, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [], 'episode_lengths': []}, 'sampler_perf': {}, 'off_policy_estimator': {}, 'num_healthy_workers': 0, 'timesteps_total': 9600, 'timers': {'sample_time_ms': 273.817, 'sample_throughput': 1460.832, 'learn_time_ms': 2.912, 'learn_throughput': 137357.163}, 'info': {'learner': {'model': {}}, 'num_steps_sampled': 9600, 'num_steps_trained': 9600}, 'done': False, 'episodes_total': 0, 'training_iteration': 4, 'experiment_id': 'f7f492935452438d80a6ccb66444ea54', 'date': '2020-04-28_00-52-13', 'timestamp': 1588049533, 'time_this_iter_s': 2.1984901428222656, 'time_total_s': 6.989131689071655, 'pid': 8897, 'hostname': 'victor-solus', 'node_ip': '192.168.1.236', 'config': {'num_workers': 0, 'num_envs_per_worker': 1, 'rollout_fragment_length': 200, 'sample_batch_size': -1, 'batch_mode': 'truncate_episodes', 'num_gpus': 0, 'train_batch_size': 200, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_action_dist': None, 'custom_options': {}, 'custom_preprocessor': None}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env_config': {}, 'env': 'SimpleEnv', 'normalize_actions': False, 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 0.0004, 'monitor': False, 'log_level': 'WARN', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None, 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'use_pytorch': False, 'eager': False, 'eager_tracing': False, 'no_eager_on_workers': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'use_exec_api': True, 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'memory': 0, 'object_store_memory': 0, 'memory_per_worker': 0, 'object_store_memory_per_worker': 0, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}}, 'time_since_restore': 6.989131689071655, 'timesteps_since_restore': 0, 'iterations_since_restore': 4, 'perf': {'cpu_util_percent': 42.79999999999999, 'ram_util_percent': 48.4}}\n",
      "{'episode_reward_max': nan, 'episode_reward_min': nan, 'episode_reward_mean': nan, 'episode_len_mean': nan, 'episodes_this_iter': 0, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [], 'episode_lengths': []}, 'sampler_perf': {}, 'off_policy_estimator': {}, 'num_healthy_workers': 0, 'timesteps_total': 12800, 'timers': {'sample_time_ms': 273.531, 'sample_throughput': 1462.357, 'learn_time_ms': 3.052, 'learn_throughput': 131068.928}, 'info': {'learner': {'model': {}}, 'num_steps_sampled': 12800, 'num_steps_trained': 12800}, 'done': False, 'episodes_total': 0, 'training_iteration': 5, 'experiment_id': 'f7f492935452438d80a6ccb66444ea54', 'date': '2020-04-28_00-52-15', 'timestamp': 1588049535, 'time_this_iter_s': 2.2032694816589355, 'time_total_s': 9.19240117073059, 'pid': 8897, 'hostname': 'victor-solus', 'node_ip': '192.168.1.236', 'config': {'num_workers': 0, 'num_envs_per_worker': 1, 'rollout_fragment_length': 200, 'sample_batch_size': -1, 'batch_mode': 'truncate_episodes', 'num_gpus': 0, 'train_batch_size': 200, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_action_dist': None, 'custom_options': {}, 'custom_preprocessor': None}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env_config': {}, 'env': 'SimpleEnv', 'normalize_actions': False, 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 0.0004, 'monitor': False, 'log_level': 'WARN', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': None, 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'use_pytorch': False, 'eager': False, 'eager_tracing': False, 'no_eager_on_workers': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'use_exec_api': True, 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'memory': 0, 'object_store_memory': 0, 'memory_per_worker': 0, 'object_store_memory_per_worker': 0, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}}, 'time_since_restore': 9.19240117073059, 'timesteps_since_restore': 0, 'iterations_since_restore': 5, 'perf': {'cpu_util_percent': 35.699999999999996, 'ram_util_percent': 48.5}}\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents import ppo, pg\n",
    "trainer = pg.PGTrainer(env=SimpleEnv)\n",
    "#trainer = ppo.PPOTrainer(env=SimpleEnv)\n",
    "while True:\n",
    "    print(trainer.train())  # distributed training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
