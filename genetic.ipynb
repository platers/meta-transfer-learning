{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "genetic_with_comments_tuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Sdi2UTA8vWEJ"
      },
      "source": [
        "A proof of concept showing using a genetic algorithm with our environment.\n",
        "It is similar to https://github.com/DEAP/deap/blob/a0b78956e28387785e3bb6e2b4b1f1b32c2b3883/examples/ga/onemax_short.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qv1kWykfqRLl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5cc79b96-10b1-4b98-ab65-f8e9191b6789"
      },
      "source": [
        "# Run this cell if you're using colab. Otherwise, skip it.\n",
        "\n",
        "!git clone https://github.com/platers/meta-transfer-learning.git\n",
        "\n",
        "import os\n",
        "os.chdir('meta-transfer-learning')\n",
        "\n",
        "!pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp36-cp36m-manylinux1_x86_64.whl\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "!pip install ran"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'meta-transfer-learning' already exists and is not an empty directory.\n",
            "Collecting ray==0.9.0.dev0\n",
            "  Using cached https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied, skipping upgrade: aiohttp in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (3.6.2)\n",
            "Requirement already satisfied, skipping upgrade: grpcio in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (1.28.1)\n",
            "Requirement already satisfied, skipping upgrade: msgpack<1.0.0,>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (0.6.2)\n",
            "Requirement already satisfied, skipping upgrade: redis<3.5.0,>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: google in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (2.0.3)\n",
            "Requirement already satisfied, skipping upgrade: colorama in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (0.4.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.16 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (1.18.3)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: jsonschema in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: py-spy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (19.3.0)\n",
            "Requirement already satisfied, skipping upgrade: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (3.0.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (3.6.6)\n",
            "Requirement already satisfied, skipping upgrade: multidict<5.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (4.7.5)\n",
            "Requirement already satisfied, skipping upgrade: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (1.4.2)\n",
            "Requirement already satisfied, skipping upgrade: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from grpcio->ray==0.9.0.dev0) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from google->ray==0.9.0.dev0) (4.6.3)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->ray==0.9.0.dev0) (46.1.3)\n",
            "Requirement already satisfied, skipping upgrade: idna>=2.0 in /usr/local/lib/python3.6/dist-packages (from yarl<2.0,>=1.0->aiohttp->ray==0.9.0.dev0) (2.9)\n",
            "Installing collected packages: ray\n",
            "  Found existing installation: ray 0.9.0.dev0\n",
            "    Uninstalling ray-0.9.0.dev0:\n",
            "      Successfully uninstalled ray-0.9.0.dev0\n",
            "Successfully installed ray-0.9.0.dev0\n",
            "Requirement already satisfied: cloudpickle==1.2.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.2.1)\n",
            "Requirement already satisfied: future==0.17.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.17.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.17.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.18.3)\n",
            "Requirement already satisfied: Pillow==6.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (6.2.0)\n",
            "Requirement already satisfied: pyglet==1.3.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: scipy==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: six==1.12.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (1.12.0)\n",
            "Requirement already satisfied: pytest_cases in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (1.13.1)\n",
            "Requirement already satisfied: deap in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (1.3.1)\n",
            "Requirement already satisfied: ray[rllib] in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (0.9.0.dev0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (2.2.0rc4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from pytest_cases->-r requirements.txt (line 9)) (1.12.1)\n",
            "Requirement already satisfied: makefun>=1.7 in /usr/local/lib/python3.6/dist-packages (from pytest_cases->-r requirements.txt (line 9)) (1.9.2)\n",
            "Requirement already satisfied: decopatch in /usr/local/lib/python3.6/dist-packages (from pytest_cases->-r requirements.txt (line 9)) (1.4.8)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (3.6.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (2.6.0)\n",
            "Requirement already satisfied: google in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (2.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (3.0.12)\n",
            "Requirement already satisfied: msgpack<1.0.0,>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (0.6.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (3.10.0)\n",
            "Requirement already satisfied: redis<3.5.0,>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (3.4.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (0.4.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (7.1.2)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (1.28.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (3.13)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (0.3.3)\n",
            "Requirement already satisfied: opencv-python-headless; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (4.2.0.34)\n",
            "Requirement already satisfied: atari-py; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (0.2.6)\n",
            "Requirement already satisfied: dm-tree; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (0.1.5)\n",
            "Requirement already satisfied: tabulate; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (0.8.7)\n",
            "Requirement already satisfied: lz4; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (3.0.2)\n",
            "Requirement already satisfied: tensorboardX; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (2.0)\n",
            "Requirement already satisfied: pandas; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (1.0.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (1.1.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (1.6.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (0.3.3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (0.34.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (0.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (3.2.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (2.2.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (2.10.0)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (2.2.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]->-r requirements.txt (line 11)) (19.3.0)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]->-r requirements.txt (line 11)) (3.0.4)\n",
            "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]->-r requirements.txt (line 11)) (1.1.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]->-r requirements.txt (line 11)) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]->-r requirements.txt (line 11)) (3.6.6)\n",
            "Requirement already satisfied: multidict<5.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]->-r requirements.txt (line 11)) (4.7.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]->-r requirements.txt (line 11)) (1.4.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from google->ray[rllib]->-r requirements.txt (line 11)) (4.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->ray[rllib]->-r requirements.txt (line 11)) (46.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas; extra == \"rllib\"->ray[rllib]->-r requirements.txt (line 11)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas; extra == \"rllib\"->ray[rllib]->-r requirements.txt (line 11)) (2018.9)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (3.2.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (1.7.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (1.6.0.post3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (1.0.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.6/dist-packages (from idna-ssl>=1.0; python_version < \"3.7\"->aiohttp->ray[rllib]->-r requirements.txt (line 11)) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (0.2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (0.4.8)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement ran (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for ran\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ojEPVGZmvTAe",
        "colab": {}
      },
      "source": [
        "import array\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "from typing import Dict\n",
        "\n",
        "from deap import algorithms\n",
        "from deap import base\n",
        "from deap import creator\n",
        "from deap import tools\n",
        "import gym\n",
        "\n",
        "from environments import SimpleEnv\n",
        "import importlib\n",
        "importlib.reload(SimpleEnv)\n",
        "from environments.SimpleEnv import SimpleEnv #, TODO: add more environments\n",
        "\n",
        "from ray.rllib.agents import ppo\n",
        "from ray import tune\n",
        "from ray.rllib.policy.policy import Policy\n",
        "from ray.rllib.env import BaseEnv\n",
        "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
        "from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker\n",
        "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
        "import ray\n",
        "\n",
        "import time\n",
        "start = time.process_time()\n",
        "\n",
        "from ray import tune\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvaDdyNmEQ-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# some settings that we can tweak:\n",
        "config_evolution={\n",
        "        \"MAX_STEP_COUNT\": 1, # number of steps in each round of DEAP evolution \n",
        "        \"POPULATION_SIZE\": 1, # population in each round of DEAP evolution \n",
        "        \"N_GEN\": 1 # number of rounds of DEAP evolution This has the most effect\n",
        "        }\n",
        "N_RL_TRANING = 1 # number of rounds of RL training\n",
        "TRAIN_BATCH_SIZE = 1 #Batch size for RL training\n",
        "LEARNING_RATE = 0.001 #learning rate for RL training\n",
        "EVAL_METHOD = \"OPTIMAL\" # RL or OPTIMAL\n",
        "TUNING_EVOLUTION = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cnG-FTpEQ-1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "394bb062-8a35-422f-9a50-b244aebf6c49"
      },
      "source": [
        "ray.shutdown()\n",
        "ray.init()\n",
        "n_agents = 3\n",
        "n_var = 2\n",
        "training_envs = [\n",
        "(SimpleEnv, {\n",
        "    'n_agents': n_agents,\n",
        "    'n_vars': n_var,\n",
        "    'true_reward_weights': [1, 0],\n",
        "    'max_step_count': config_evolution[\"MAX_STEP_COUNT\"],\n",
        "}),\n",
        "(SimpleEnv, {\n",
        "    'n_agents': n_agents,\n",
        "    'n_vars': n_var,\n",
        "    'true_reward_weights': [0, 1],\n",
        "    'max_step_count': config_evolution[\"MAX_STEP_COUNT\"],\n",
        "})]\n",
        "test_env = (SimpleEnv, {\n",
        "    'n_agents': n_agents,\n",
        "    'n_vars': n_var,\n",
        "    'true_reward_weights': [1, 1],\n",
        "    'max_step_count': config_evolution[\"MAX_STEP_COUNT\"],\n",
        "})\n",
        "\n",
        "# creator.create('FitnessMax', base.Fitness, weights=(1.0, ))\n",
        "# creator.create('Individual', array.array, typecode='d',\n",
        "#                fitness=creator.FitnessMax)\n",
        "\n",
        "# toolbox = base.Toolbox()\n",
        "\n",
        "# toolbox.register('attr', random.uniform, -1, 1)\n",
        "# toolbox.register('individual', tools.initRepeat, creator.Individual,\n",
        "#                  toolbox.attr, n_agents * n_var)\n",
        "# toolbox.register('population', tools.initRepeat, list,\n",
        "#                  toolbox.individual)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-08 00:56:49,577\tINFO resource_spec.py:212 -- Starting Ray with 7.18 GiB memory available for workers and up to 3.59 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
            "2020-05-08 00:56:49,990\tINFO services.py:1170 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn9RzymlEQ_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directly generate \"optimal action\" suggested by the reward function created by Evolution\n",
        "# reward_weights is created from env(config=env_config)\n",
        "def get_optimal_action(reward_weights, env_config):\n",
        "    n_vars = env_config[\"n_vars\"]\n",
        "    max_act = 5\n",
        "    \n",
        "    reward_scale_factor = np.array([1]*n_vars + [2]*(len(reward_weights)-n_vars))\n",
        "    scaled_reward_weights = reward_weights * reward_scale_factor\n",
        "    best_act = np.argmax(scaled_reward_weights)\n",
        "    action = np.eye(len(reward_weights))[best_act] * max_act\n",
        "    action = np.reshape(action, [n_agents, n_vars])\n",
        "    return action \n",
        "\n",
        "# If we do not want to optimize RL but just want to optimize reward function using Evolution function\n",
        "# then here we just do the \"optimal action\" suggested by the generated reward function\n",
        "def evaluate_individual_env_optimal_act(individual, environment_fn, env_config):\n",
        "    env_config['reward_weights'] = np.array([individual for i in range(n_agents)])\n",
        "    env = environment_fn(config=env_config)\n",
        "    \n",
        "    ave_true_rewards = 0\n",
        "    obs = env.reset()\n",
        "    ave_reward = 0\n",
        "    for _ in range(env_config[\"max_step_count\"]):\n",
        "        actions = {i: get_optimal_action(env.reward_weights[i], env_config) for i in range(n_agents)}\n",
        "        obs, reward, _, _ = env.step(actions)\n",
        "        reward = np.array([reward[i] for i in range(len(reward))])\n",
        "        ave_reward += reward\n",
        "        ave_true_rewards += env.last_true_reward\n",
        "        \n",
        "    ave_true_rewards /= env_config[\"max_step_count\"]\n",
        "    ave_reward /= env_config[\"max_step_count\"]\n",
        "    \n",
        "    return np.mean(ave_true_rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oP1G3iuuolJi",
        "colab": {}
      },
      "source": [
        "# train RL agent with given setup\n",
        "def evaluate_individual_env_rl(individual, environment, env_config):\n",
        "    \"\"\"Runs the environment. All agents have the same policy.\n",
        "  It returns the total true reward as the fitness.\n",
        "  \"\"\"\n",
        "    #Select random individuals from pop and create the reward weights\n",
        "    pop = np.array([individual for i in range(n_agents)])\n",
        "    reward_weights = pop\n",
        "    env_config['reward_weights'] = reward_weights\n",
        "    #env is only to get action space and observation space\n",
        "    env = environment(config=env_config)\n",
        "    class MyCallbacks(DefaultCallbacks):\n",
        "        #Callback functions to keep track of true reward while training\n",
        "        def on_episode_start(self, worker: RolloutWorker, base_env: BaseEnv,\n",
        "                         policies: Dict[str, Policy],\n",
        "                         episode: MultiAgentEpisode, **kwargs):\n",
        "            episode.user_data[\"true_rewards\"] = np.zeros(n_agents)\n",
        "\n",
        "        def on_episode_step(self, worker: RolloutWorker, base_env: BaseEnv,\n",
        "                        episode: MultiAgentEpisode, **kwargs):\n",
        "            env = base_env\n",
        "            true_reward = env.env_states[0].env.last_true_reward\n",
        "            episode.user_data[\"true_rewards\"] += true_reward\n",
        "\n",
        "        def on_episode_end(self, worker: RolloutWorker, base_env: BaseEnv,\n",
        "                       policies: Dict[str, Policy], episode: MultiAgentEpisode,\n",
        "                       **kwargs):\n",
        "            true_reward = episode.user_data[\"true_rewards\"]\n",
        "            for i, r in enumerate(true_reward):\n",
        "                episode.custom_metrics[\"true_reward_agent_\" + str(i)] = r\n",
        "    \n",
        "    # settings for the RL agent trainer     \n",
        "    config={\n",
        "        \"train_batch_size\": TRAIN_BATCH_SIZE,\n",
        "        \"lr\": LEARNING_RATE,\n",
        "        \"sgd_minibatch_size\": TRAIN_BATCH_SIZE,\n",
        "        \"multiagent\": {\n",
        "            \"policies\": {\n",
        "            },\n",
        "            \"policy_mapping_fn\":  #all agents share a policy\n",
        "                lambda agent_id:\n",
        "                    'agent'\n",
        "        },\n",
        "        \"model\": {\"fcnet_hiddens\": []},\n",
        "        'env_config': env_config,\n",
        "        \"callbacks\": MyCallbacks,\n",
        "    }\n",
        "    config['multiagent']['policies']['agent'] = (None, env.observation_space, env.action_space, {})\n",
        "    metrics = None\n",
        "    while True:\n",
        "        trainer = ppo.PPOTrainer(env=environment, config=config)\n",
        "        true_reward_mean = 0\n",
        "        for i in range(N_RL_TRANING):\n",
        "            true_reward_mean = 0\n",
        "            #Train the RL agent\n",
        "            metrics = trainer.train()  # distributed training step\n",
        "            print(\"episode_reward_mean\", metrics[\"episode_reward_mean\"])\n",
        "            if metrics[\"episode_reward_mean\"] < 0 and i > 2:\n",
        "                break\n",
        "        #Train agent until it does well\n",
        "        if metrics[\"episode_reward_mean\"] > 0:\n",
        "            break\n",
        "    print(\"episode_reward_mean\", metrics[\"episode_reward_mean\"])\n",
        "    for i in range(n_agents):\n",
        "        true_reward_mean += metrics['custom_metrics']['true_reward_agent_' + str(i) + '_mean']\n",
        "    true_reward_mean /= n_agents\n",
        "    print('Evaluated', individual, 'Fitness', true_reward_mean)\n",
        "    return true_reward_mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy4idwUWEQ_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if EVAL_METHOD == \"RL\":\n",
        "    evaluate_individual_env = evaluate_individual_env_rl\n",
        "else:\n",
        "    evaluate_individual_env = evaluate_individual_env_optimal_act\n",
        "        \n",
        "def evaluate_individual(individual):\n",
        "    \"\"\"Runs all environments. \n",
        "  returns the average true reward over all environments as the fitness.\n",
        "  \"\"\"\n",
        "    \n",
        "    fitness = 0\n",
        "    for env, config in training_envs:\n",
        "        fitness += evaluate_individual_env(individual, env, config)\n",
        "    fitness /= len(training_envs)\n",
        "    return (fitness, )\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k1cp_IzAwg5n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d68ffcd2-a9db-41d4-b698-3b7d8790da9d"
      },
      "source": [
        "\n",
        "\n",
        "#function usin tuning\n",
        "def train_reward_function(config):\n",
        "  creator.create('FitnessMax', base.Fitness, weights=(1.0, ))\n",
        "  creator.create('Individual', array.array, typecode='d',\n",
        "                fitness=creator.FitnessMax)\n",
        "\n",
        "  toolbox = base.Toolbox()\n",
        "\n",
        "  toolbox.register('attr', random.uniform, -1, 1)\n",
        "  toolbox.register('individual', tools.initRepeat, creator.Individual,\n",
        "                  toolbox.attr, n_agents * n_var)\n",
        "  toolbox.register('population', tools.initRepeat, list,\n",
        "                  toolbox.individual)\n",
        "    # some setup for Deap\n",
        "  toolbox.register('evaluate', evaluate_individual)\n",
        "  toolbox.register('mate', tools.cxTwoPoint)\n",
        "  toolbox.register('mutate', tools.mutFlipBit, indpb=0.05) # add more noise\n",
        "  toolbox.register('select', tools.selTournament, tournsize=3)\n",
        "\n",
        "  # pop is a list individual reward function's weights \n",
        "  pop = toolbox.population(n=config[\"POPULATION_SIZE\"])\n",
        "  # the 10 best individual reward function (could even befround the first round)\n",
        "  hof = tools.HallOfFame(10)\n",
        "\n",
        "  stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "  stats.register('avg', np.mean)\n",
        "  stats.register('std', np.std)\n",
        "  stats.register('min', np.min)\n",
        "  stats.register('max', np.max)\n",
        "\n",
        "  \n",
        "  pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=config[\"N_GEN\"], \n",
        "                                    stats=stats, halloffame=hof, verbose=True)\n",
        "  best_individual = hof[0]\n",
        "  test_reward = evaluate_individual_env(best_individual, test_env[0], test_env[1])\n",
        "  print(\"---test_reward:\", test_reward)\n",
        "  tune.track.log(mean_accuracy=test_reward)\n",
        "\n",
        "if TUNING_EVOLUTION:\n",
        "  #configurations\n",
        "  analysis = tune.run(\n",
        "      train_reward_function, config={\n",
        "          \"N_GEN\": tune.grid_search([1, 4]), \n",
        "          \"POPULATION_SIZE\": tune.grid_search([1])\n",
        "          })\n",
        "  #print best config\n",
        "  print(\"Best config: \", analysis.get_best_config(metric=\"mean_accuracy\"))\n",
        "else:\n",
        "  train_reward_function(config_evolution)\n",
        "\n",
        "\n",
        "# print ('pop', pop)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-08 00:56:50,766\tWARNING tune.py:316 -- Tune detects GPUs, but no trials are using GPUs. To enable trials to use GPUs, set tune.run(resources_per_trial={'gpu': 1}...) which allows Tune to expose 1 GPU to each trial. You can also override `Trainable.default_resource_request` if using the Trainable API.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 1.2/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/2 CPUs, 0/1 GPUs, 0.0/7.18 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /root/ray_results/train_reward_function<br>Number of trials: 4 (3 PENDING, 1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  N_GEN</th><th style=\"text-align: right;\">  POPULATION_SIZE</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_reward_function_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">                1</td></tr>\n",
              "<tr><td>train_reward_function_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">                1</td></tr>\n",
              "<tr><td>train_reward_function_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">                3</td></tr>\n",
              "<tr><td>train_reward_function_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">                3</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2020-05-08 00:56:53,381\tWARNING worker.py:1094 -- The dashboard on node 5516454f8e45 failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1062, in create_server\n",
            "    sock.bind(sa)\n",
            "OSError: [Errno 99] Cannot assign requested address\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ray/dashboard/dashboard.py\", line 1221, in <module>\n",
            "    dashboard.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ray/dashboard/dashboard.py\", line 595, in run\n",
            "    aiohttp.web.run_app(self.app, host=self.host, port=self.port)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/aiohttp/web.py\", line 433, in run_app\n",
            "    reuse_port=reuse_port))\n",
            "  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 484, in run_until_complete\n",
            "    return future.result()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/aiohttp/web.py\", line 359, in _run_app\n",
            "    await site.start()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/aiohttp/web_runner.py\", line 104, in start\n",
            "    reuse_port=self._reuse_port)\n",
            "  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1066, in create_server\n",
            "    % (sa, err.strerror.lower()))\n",
            "OSError: [Errno 99] error while attempting to bind on address ('::1', 8265, 0, 0): cannot assign requested address\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2987)\u001b[0m 2020-05-08 00:56:57,224\tINFO trainable.py:217 -- Getting current IP.\n",
            "Result for train_reward_function_00000:\n",
            "  date: 2020-05-08_00-56-57\n",
            "  done: false\n",
            "  experiment_id: 3c99477c79ab40cd9bab8a80d3f4eb0e\n",
            "  experiment_tag: 0_N_GEN=1,POPULATION_SIZE=1\n",
            "  hostname: 5516454f8e45\n",
            "  iterations_since_restore: 1\n",
            "  mean_accuracy: -3.333333333333334\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2987\n",
            "  time_since_restore: 0.022668838500976562\n",
            "  time_this_iter_s: 0.022668838500976562\n",
            "  time_total_s: 0.022668838500976562\n",
            "  timestamp: 1588899417\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 0\n",
            "  trial_id: '00000'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 1.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/2 CPUs, 0/1 GPUs, 0.0/7.18 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /root/ray_results/train_reward_function<br>Number of trials: 4 (2 PENDING, 2 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  N_GEN</th><th style=\"text-align: right;\">  POPULATION_SIZE</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_reward_function_00000</td><td>RUNNING </td><td>172.28.0.2:2987</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">-3.33333</td><td style=\"text-align: right;\">     0</td><td style=\"text-align: right;\">       0.0226688</td></tr>\n",
              "<tr><td>train_reward_function_00001</td><td>RUNNING </td><td>               </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
              "<tr><td>train_reward_function_00002</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
              "<tr><td>train_reward_function_00003</td><td>PENDING </td><td>               </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=2987)\u001b[0m gen\tnevals\tavg     \tstd\tmin     \tmax     \n",
            "\u001b[2m\u001b[36m(pid=2987)\u001b[0m 0  \t1     \t-1.66667\t0  \t-1.66667\t-1.66667\n",
            "\u001b[2m\u001b[36m(pid=2987)\u001b[0m 1  \t0     \t-1.66667\t0  \t-1.66667\t-1.66667\n",
            "\u001b[2m\u001b[36m(pid=2987)\u001b[0m ----------test_reward -3.333333333333334\n",
            "\u001b[2m\u001b[36m(pid=2986)\u001b[0m 2020-05-08 00:56:57,302\tINFO trainable.py:217 -- Getting current IP.\n",
            "\u001b[2m\u001b[36m(pid=2986)\u001b[0m gen\tnevals\tavg     \tstd\tmin     \tmax     \n",
            "\u001b[2m\u001b[36m(pid=2986)\u001b[0m 0  \t1     \t0.833333\t0  \t0.833333\t0.833333\n",
            "\u001b[2m\u001b[36m(pid=2986)\u001b[0m 1  \t0     \t0.833333\t0  \t0.833333\t0.833333\n",
            "\u001b[2m\u001b[36m(pid=2986)\u001b[0m 2  \t0     \t0.833333\t0  \t0.833333\t0.833333\n",
            "\u001b[2m\u001b[36m(pid=2986)\u001b[0m 3  \t0     \t0.833333\t0  \t0.833333\t0.833333\n",
            "\u001b[2m\u001b[36m(pid=2986)\u001b[0m 4  \t1     \t0.833333\t0  \t0.833333\t0.833333\n",
            "Result for train_reward_function_00001:\n",
            "  date: 2020-05-08_00-56-57\n",
            "  done: false\n",
            "  experiment_id: 9ee68ae8b7084efc8bc24526ef36a8af\n",
            "  experiment_tag: 1_N_GEN=4,POPULATION_SIZE=1\n",
            "  hostname: 5516454f8e45\n",
            "  iterations_since_restore: 1\n",
            "  mean_accuracy: 1.6666666666666672\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2986\n",
            "  time_since_restore: 0.03384089469909668\n",
            "  time_this_iter_s: 0.03384089469909668\n",
            "  time_total_s: 0.03384089469909668\n",
            "  timestamp: 1588899417\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 0\n",
            "  trial_id: '00001'\n",
            "  \n",
            "\u001b[2m\u001b[36m(pid=2986)\u001b[0m ----------test_reward 1.6666666666666672\n",
            "\u001b[2m\u001b[36m(pid=3018)\u001b[0m 2020-05-08 00:57:02,080\tINFO trainable.py:217 -- Getting current IP.\n",
            "Result for train_reward_function_00002:\n",
            "  date: 2020-05-08_00-57-02\n",
            "  done: false\n",
            "  experiment_id: 98a21f62562d47dc968e93080da21744\n",
            "  experiment_tag: 2_N_GEN=1,POPULATION_SIZE=3\n",
            "  hostname: 5516454f8e45\n",
            "  iterations_since_restore: 1\n",
            "  mean_accuracy: 1.6666666666666672\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3018\n",
            "  time_since_restore: 0.05411696434020996\n",
            "  time_this_iter_s: 0.05411696434020996\n",
            "  time_total_s: 0.05411696434020996\n",
            "  timestamp: 1588899422\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 0\n",
            "  trial_id: '00002'\n",
            "  \n",
            "\u001b[2m\u001b[36m(pid=3018)\u001b[0m gen\tnevals\tavg     \tstd        \tmin     \tmax     \n",
            "\u001b[2m\u001b[36m(pid=3018)\u001b[0m 0  \t3     \t0.833333\t4.20324e-16\t0.833333\t0.833333\n",
            "\u001b[2m\u001b[36m(pid=3018)\u001b[0m 1  \t0     \t0.833333\t0          \t0.833333\t0.833333\n",
            "\u001b[2m\u001b[36m(pid=3018)\u001b[0m ----------test_reward 1.6666666666666672\n",
            "\u001b[2m\u001b[36m(pid=3067)\u001b[0m gen\tnevals\tavg         \tstd    \tmin     \tmax     \n",
            "\u001b[2m\u001b[36m(pid=3067)\u001b[0m 0  \t3     \t-2.22045e-16\t1.17851\t-1.66667\t0.833333\n",
            "\u001b[2m\u001b[36m(pid=3067)\u001b[0m 1  \t1     \t0.833333    \t4.20324e-16\t0.833333\t0.833333\n",
            "\u001b[2m\u001b[36m(pid=3067)\u001b[0m 2020-05-08 00:57:02,868\tINFO trainable.py:217 -- Getting current IP.\n",
            "Result for train_reward_function_00003:\n",
            "  date: 2020-05-08_00-57-02\n",
            "  done: false\n",
            "  experiment_id: 3af6de84bf4348598eda0ed413f8b715\n",
            "  experiment_tag: 3_N_GEN=4,POPULATION_SIZE=3\n",
            "  hostname: 5516454f8e45\n",
            "  iterations_since_restore: 1\n",
            "  mean_accuracy: 1.6666666666666672\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3067\n",
            "  time_since_restore: 0.0750892162322998\n",
            "  time_this_iter_s: 0.0750892162322998\n",
            "  time_total_s: 0.0750892162322998\n",
            "  timestamp: 1588899422\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 0\n",
            "  trial_id: '00003'\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 1.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/2 CPUs, 0/1 GPUs, 0.0/7.18 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /root/ray_results/train_reward_function<br>Number of trials: 4 (1 RUNNING, 3 TERMINATED)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  N_GEN</th><th style=\"text-align: right;\">  POPULATION_SIZE</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_reward_function_00000</td><td>TERMINATED</td><td>               </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">-3.33333</td><td style=\"text-align: right;\">     0</td><td style=\"text-align: right;\">       0.0226688</td></tr>\n",
              "<tr><td>train_reward_function_00001</td><td>TERMINATED</td><td>               </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\"> 1.66667</td><td style=\"text-align: right;\">     0</td><td style=\"text-align: right;\">       0.0338409</td></tr>\n",
              "<tr><td>train_reward_function_00002</td><td>TERMINATED</td><td>               </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\"> 1.66667</td><td style=\"text-align: right;\">     0</td><td style=\"text-align: right;\">       0.054117 </td></tr>\n",
              "<tr><td>train_reward_function_00003</td><td>RUNNING   </td><td>172.28.0.2:3067</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\"> 1.66667</td><td style=\"text-align: right;\">     0</td><td style=\"text-align: right;\">       0.0750892</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=3067)\u001b[0m 2  \t2     \t0.833333    \t0          \t0.833333\t0.833333\n",
            "\u001b[2m\u001b[36m(pid=3067)\u001b[0m 3  \t2     \t0.833333    \t0          \t0.833333\t0.833333\n",
            "\u001b[2m\u001b[36m(pid=3067)\u001b[0m 4  \t2     \t0.833333    \t0          \t0.833333\t0.833333\n",
            "\u001b[2m\u001b[36m(pid=3067)\u001b[0m ----------test_reward 1.6666666666666672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 1.5/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.18 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /root/ray_results/train_reward_function<br>Number of trials: 4 (4 TERMINATED)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  N_GEN</th><th style=\"text-align: right;\">  POPULATION_SIZE</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_reward_function_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">-3.33333</td><td style=\"text-align: right;\">     0</td><td style=\"text-align: right;\">       0.0226688</td></tr>\n",
              "<tr><td>train_reward_function_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\"> 1.66667</td><td style=\"text-align: right;\">     0</td><td style=\"text-align: right;\">       0.0338409</td></tr>\n",
              "<tr><td>train_reward_function_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\"> 1.66667</td><td style=\"text-align: right;\">     0</td><td style=\"text-align: right;\">       0.054117 </td></tr>\n",
              "<tr><td>train_reward_function_00003</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\"> 1.66667</td><td style=\"text-align: right;\">     0</td><td style=\"text-align: right;\">       0.0750892</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Best config:  {'N_GEN': 4, 'POPULATION_SIZE': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ztw5G6VWERA6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42f17725-a601-4769-ddd7-7867f69b2f6f"
      },
      "source": [
        "print(evaluate_individual([0.9298461960519508, 0.7449587149229808, 0.4628576259710946, -0.282921307700329, -0.7019321146761455, 0.618488821337605])) #Ideal reward, altruistic agent"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0.833333333333333,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS2i0F_qERBb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68bb9f3a-ff56-4fc2-d85a-7ab190d7e6fb"
      },
      "source": [
        "print(evaluate_individual([1, 0, -1, 0, -1, 0])) #Worst reward, selfish agent"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(-1.6666666666666667,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiWsv5EGERCQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e702ce6a-5b53-480d-994a-44a51843d24f"
      },
      "source": [
        "print(\"Time Spent = \", (time.process_time() - start)/60, \" minutes\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time Spent =  0.014906510616666665  minutes\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}