{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "2020-05-03 12:44:39,761\tINFO resource_spec.py:212 -- Starting Ray with 2.54 GiB memory available for workers and up to 1.29 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-05-03 12:44:40,107\tINFO services.py:1170 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "# A proof of concept showing using a genetic algorithm with our environment.\n",
    "# It is similar to https://github.com/DEAP/deap/blob/a0b78956e28387785e3bb6e2b4b1f1b32c2b3883/examples/ga/onemax_short.py\n",
    "\n",
    "import array\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "\n",
    "from deap import algorithms\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "import gym\n",
    "from environments.SimpleEnv import SimpleEnv\n",
    "\n",
    "from ray.rllib.agents import ppo\n",
    "from ray import tune\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "import ray\n",
    "\n",
    "ray.init()\n",
    "\n",
    "\n",
    "creator.create('FitnessMax', base.Fitness, weights=(1.0, ))\n",
    "creator.create('Individual', array.array, typecode='d',\n",
    "               fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "toolbox.register('attr', random.uniform, -1, 1)\n",
    "toolbox.register('individual', tools.initRepeat, creator.Individual,\n",
    "                 toolbox.attr, 2)\n",
    "toolbox.register('population', tools.initRepeat, list,\n",
    "                 toolbox.individual)\n",
    "\n",
    "n_agents = 3\n",
    "\n",
    "def evaluate_population(pop):\n",
    "    \"\"\"Runs the environment. Selects random agents from pop.\n",
    "  It returns the total true reward as the fitness.\n",
    "  \"\"\"\n",
    "    \n",
    "    #Select random individuals from pop and create the reward weights\n",
    "    pop = np.array(pop)\n",
    "    reward_weights = pop\n",
    "    #print(individual, reward_weights)\n",
    "    \n",
    "    #env is only to get action space and observation space\n",
    "    env = SimpleEnv(config={\n",
    "        'n_agents': n_agents,\n",
    "        'n_vars': 2,\n",
    "        'reward_weights': reward_weights,\n",
    "        'max_step_count': 20,\n",
    "    })\n",
    "    class MyCallbacks(DefaultCallbacks):\n",
    "        #Callback functions to keep track of true reward while training\n",
    "        def on_episode_start(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                         policies: Dict[str, Policy],\n",
    "                         episode: MultiAgentEpisode, **kwargs):\n",
    "            episode.user_data[\"true_rewards\"] = np.zeros(n_agents)\n",
    "\n",
    "        def on_episode_step(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                        episode: MultiAgentEpisode, **kwargs):\n",
    "            env = base_env\n",
    "            #print(env.env_states[0].env.last_true_reward)\n",
    "            true_reward = env.env_states[0].env.last_true_reward\n",
    "            episode.user_data[\"true_rewards\"] += true_reward\n",
    "\n",
    "        def on_episode_end(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                       policies: Dict[str, Policy], episode: MultiAgentEpisode,\n",
    "                       **kwargs):\n",
    "            true_reward = episode.user_data[\"true_rewards\"]\n",
    "            for i, r in enumerate(true_reward):\n",
    "                episode.custom_metrics[\"true_reward_agent_\" + str(i)] = r\n",
    "            \n",
    "    config={\n",
    "        \"multiagent\": {\n",
    "            \"policies\": {\n",
    "            },\n",
    "            \"policy_mapping_fn\":\n",
    "                lambda agent_id:\n",
    "                    agent_id\n",
    "        },\n",
    "        'env_config': {\n",
    "            'n_agents': n_agents,\n",
    "            'n_vars': 2,\n",
    "            'reward_weights': reward_weights,\n",
    "            'max_step_count': 20,\n",
    "        },\n",
    "        \"callbacks\": MyCallbacks,\n",
    "    }\n",
    "    for i in range(n_agents):\n",
    "        config['multiagent']['policies']['agent_' + str(i)] = (None, env.observation_space, env.action_space, {})\n",
    "    trainer = ppo.PPOTrainer(env=SimpleEnv, config=config)\n",
    "    \n",
    "    true_reward_mean = []\n",
    "    for i in range(10):\n",
    "        #print('TRAINING', i)\n",
    "        true_reward_mean = []\n",
    "        custom_metrics = trainer.train()['custom_metrics']  # distributed training step\n",
    "        print(custom_metrics)\n",
    "        for i in range(n_agents):\n",
    "            true_reward_mean.append((custom_metrics['true_reward_agent_' + str(i) + '_mean'], ))\n",
    "        \n",
    "    #print('true reward', trainer.collect_metrics()['custom_metrics']['true_reward_mean'])\n",
    "    return true_reward_mean\n",
    "\n",
    "def evaluate_individual(individual):\n",
    "    \"\"\"Runs the environment. All agents have the same policy.\n",
    "  It returns the total true reward as the fitness.\n",
    "  \"\"\"\n",
    "    \n",
    "    #Select random individuals from pop and create the reward weights\n",
    "    pop = np.array([individual for i in range(n_agents)])\n",
    "    reward_weights = pop\n",
    "    #print(individual, reward_weights)\n",
    "    \n",
    "    #env is only to get action space and observation space\n",
    "    env = SimpleEnv(config={\n",
    "        'n_agents': n_agents,\n",
    "        'n_vars': 2,\n",
    "        'reward_weights': reward_weights,\n",
    "        'max_step_count': 20,\n",
    "    })\n",
    "    class MyCallbacks(DefaultCallbacks):\n",
    "        #Callback functions to keep track of true reward while training\n",
    "        def on_episode_start(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                         policies: Dict[str, Policy],\n",
    "                         episode: MultiAgentEpisode, **kwargs):\n",
    "            episode.user_data[\"true_rewards\"] = np.zeros(n_agents)\n",
    "\n",
    "        def on_episode_step(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                        episode: MultiAgentEpisode, **kwargs):\n",
    "            env = base_env\n",
    "            #print(env.env_states[0].env.last_true_reward)\n",
    "            true_reward = env.env_states[0].env.last_true_reward\n",
    "            episode.user_data[\"true_rewards\"] += true_reward\n",
    "\n",
    "        def on_episode_end(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                       policies: Dict[str, Policy], episode: MultiAgentEpisode,\n",
    "                       **kwargs):\n",
    "            true_reward = episode.user_data[\"true_rewards\"]\n",
    "            for i, r in enumerate(true_reward):\n",
    "                episode.custom_metrics[\"true_reward_agent_\" + str(i)] = r\n",
    "            \n",
    "    config={\n",
    "        \"multiagent\": {\n",
    "            \"policies\": {\n",
    "            },\n",
    "            \"policy_mapping_fn\":  #all agents share a policy\n",
    "                lambda agent_id:\n",
    "                    'agent'\n",
    "        },\n",
    "        'env_config': {\n",
    "            'n_agents': n_agents,\n",
    "            'n_vars': 2,\n",
    "            'reward_weights': reward_weights,\n",
    "            'max_step_count': 20,\n",
    "        },\n",
    "        \"callbacks\": MyCallbacks,\n",
    "    }\n",
    "    config['multiagent']['policies']['agent'] = (None, env.observation_space, env.action_space, {})\n",
    "    trainer = ppo.PPOTrainer(env=SimpleEnv, config=config)\n",
    "    \n",
    "    true_reward_mean = 0\n",
    "    for i in range(10):\n",
    "        #print('TRAINING', i)\n",
    "        true_reward_mean = 0\n",
    "        custom_metrics = trainer.train()['custom_metrics']  # distributed training step\n",
    "        print(custom_metrics)\n",
    "        for i in range(n_agents):\n",
    "            true_reward_mean += custom_metrics['true_reward_agent_' + str(i) + '_mean']\n",
    "    true_reward_mean /= n_agents\n",
    "    #print('true reward', trainer.collect_metrics()['custom_metrics']['true_reward_mean'])\n",
    "    return (true_reward_mean, )\n",
    "\n",
    "\n",
    "#toolbox.register('evaluate', evaluate_population)\n",
    "toolbox.register('mate', tools.cxTwoPoint)\n",
    "toolbox.register('mutate', tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register('select', tools.selTournament, tournsize=3)\n",
    "\n",
    "def evolve(population, toolbox, cxpb, mutpb, ngen, stats=None, \n",
    "           halloffame=None, verbose=__debug__):\n",
    "    \"\"\"\n",
    "    Almost identical to deap.algorithms.eaSimple. \n",
    "    \"\"\"\n",
    "    \n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population]\n",
    "    #print('population1', population)\n",
    "    fitnesses = evaluate_population(invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    #print('population2', population)\n",
    "    if halloffame is not None:\n",
    "        halloffame.update(population)\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "        # Select the next generation individuals\n",
    "        print('population', population)\n",
    "        offspring = toolbox.select(population, len(population))\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring]\n",
    "        fitnesses = evaluate_population(invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        if halloffame is not None:\n",
    "            halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-03 12:44:40,806\tINFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "2020-05-03 12:44:40,900\tERROR syncer.py:39 -- Log sync requires rsync to be installed.\n",
      "2020-05-03 12:44:40,909\tINFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=9114)\u001b[0m /home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\u001b[2m\u001b[36m(pid=9114)\u001b[0m   from ._conv import register_converters as _register_converters\n",
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m /home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m   from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-03 12:44:47,106\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-05-03 12:44:47,107\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_reward_agent_0_mean': 0.18978884825733985, 'true_reward_agent_0_min': -11.674890041351318, 'true_reward_agent_0_max': 18.933333314955235, 'true_reward_agent_1_mean': -0.4147306205322457, 'true_reward_agent_1_min': -12.045711737126112, 'true_reward_agent_1_max': 12.723820120096207, 'true_reward_agent_2_mean': 0.36191094439689553, 'true_reward_agent_2_min': -14.41079644113779, 'true_reward_agent_2_max': 19.053350493311882}\n",
      "{'true_reward_agent_0_mean': -0.1986860764940502, 'true_reward_agent_0_min': -15.024060606956482, 'true_reward_agent_0_max': 13.53712423890829, 'true_reward_agent_1_mean': -0.7003104172301573, 'true_reward_agent_1_min': -16.76941379904747, 'true_reward_agent_1_max': 19.54326067864895, 'true_reward_agent_2_mean': -2.0313096019920884, 'true_reward_agent_2_min': -15.644575461745262, 'true_reward_agent_2_max': 15.58729462325573}\n",
      "{'true_reward_agent_0_mean': -2.0014530976825338, 'true_reward_agent_0_min': -13.836990661919117, 'true_reward_agent_0_max': 9.113343521952629, 'true_reward_agent_1_mean': -4.565071710296106, 'true_reward_agent_1_min': -18.1258151717484, 'true_reward_agent_1_max': 23.095557399094105, 'true_reward_agent_2_mean': -4.6421545576366405, 'true_reward_agent_2_min': -17.145101375877857, 'true_reward_agent_2_max': 13.19861214607954}\n",
      "{'true_reward_agent_0_mean': -4.843317463127769, 'true_reward_agent_0_min': -14.063366569578648, 'true_reward_agent_0_max': 8.386767064454034, 'true_reward_agent_1_mean': -9.112256282873913, 'true_reward_agent_1_min': -22.618077628314495, 'true_reward_agent_1_max': 24.879021048545837, 'true_reward_agent_2_mean': -5.750882074175197, 'true_reward_agent_2_min': -20.995436757802963, 'true_reward_agent_2_max': 10.266372561454773}\n",
      "{'true_reward_agent_0_mean': -5.8040670276033595, 'true_reward_agent_0_min': -20.51050772331655, 'true_reward_agent_0_max': 8.07440656912513, 'true_reward_agent_1_mean': -13.448587471578376, 'true_reward_agent_1_min': -25.614070320501924, 'true_reward_agent_1_max': 19.004715954884887, 'true_reward_agent_2_mean': -8.270932176340212, 'true_reward_agent_2_min': -21.67985625565052, 'true_reward_agent_2_max': 9.114592000842094}\n",
      "{'true_reward_agent_0_mean': -5.850617965510619, 'true_reward_agent_0_min': -23.166613548994064, 'true_reward_agent_0_max': 9.865668877959251, 'true_reward_agent_1_mean': -15.895013615752395, 'true_reward_agent_1_min': -26.80279740691185, 'true_reward_agent_1_max': 10.178974390029907, 'true_reward_agent_2_mean': -11.136543475163139, 'true_reward_agent_2_min': -21.406951293349266, 'true_reward_agent_2_max': 6.708460807800293}\n",
      "{'true_reward_agent_0_mean': -6.121701906540002, 'true_reward_agent_0_min': -19.879235055297613, 'true_reward_agent_0_max': 8.388846088200808, 'true_reward_agent_1_mean': -15.53876966263102, 'true_reward_agent_1_min': -26.67538522183895, 'true_reward_agent_1_max': 13.273068621754646, 'true_reward_agent_2_mean': -11.277210952823443, 'true_reward_agent_2_min': -21.911073707044125, 'true_reward_agent_2_max': 3.572448881343007}\n",
      "{'true_reward_agent_0_mean': -6.432686536000456, 'true_reward_agent_0_min': -19.897998958826065, 'true_reward_agent_0_max': 9.108286026865244, 'true_reward_agent_1_mean': -16.92648147097585, 'true_reward_agent_1_min': -26.25775020569563, 'true_reward_agent_1_max': 25.744277343153954, 'true_reward_agent_2_mean': -11.60701709173616, 'true_reward_agent_2_min': -21.82810190320015, 'true_reward_agent_2_max': 0.32363777235150337}\n",
      "{'true_reward_agent_0_mean': -6.421275293998697, 'true_reward_agent_0_min': -17.243138724938035, 'true_reward_agent_0_max': 3.986427091062069, 'true_reward_agent_1_mean': -16.857299094474694, 'true_reward_agent_1_min': -26.85077130049467, 'true_reward_agent_1_max': -8.035631272941828, 'true_reward_agent_2_mean': -11.365006806099482, 'true_reward_agent_2_min': -21.353443411178887, 'true_reward_agent_2_max': 6.157055401708931}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-03 12:47:17,951\tERROR syncer.py:39 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_reward_agent_0_mean': -5.922253688414294, 'true_reward_agent_0_min': -16.37138692039298, 'true_reward_agent_0_max': 4.950532461516559, 'true_reward_agent_1_mean': -16.681630438000376, 'true_reward_agent_1_min': -27.253180027008057, 'true_reward_agent_1_max': -7.57680525816977, 'true_reward_agent_2_mean': -11.39771924143829, 'true_reward_agent_2_min': -20.634342340752482, 'true_reward_agent_2_max': -2.723449006676674}\n",
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m /home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m   from ._conv import register_converters as _register_converters\n",
      "\u001b[2m\u001b[36m(pid=9116)\u001b[0m /home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\u001b[2m\u001b[36m(pid=9116)\u001b[0m   from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-03 12:47:24,101\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-05-03 12:47:24,102\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_reward_agent_0_mean': -0.21029326899414172, 'true_reward_agent_0_min': -12.285366874188185, 'true_reward_agent_0_max': 16.6010437682271, 'true_reward_agent_1_mean': 0.19122196278718093, 'true_reward_agent_1_min': -13.748688347637653, 'true_reward_agent_1_max': 14.761405177414417, 'true_reward_agent_2_mean': 0.03409754443103111, 'true_reward_agent_2_min': -13.871654592454433, 'true_reward_agent_2_max': 16.323897045105696}\n",
      "{'true_reward_agent_0_mean': 18.110378507060233, 'true_reward_agent_0_min': -28.501447150483727, 'true_reward_agent_0_max': 84.43289574980736, 'true_reward_agent_1_mean': -5.331898848536512, 'true_reward_agent_1_min': -23.44284749031067, 'true_reward_agent_1_max': 23.108754217624664, 'true_reward_agent_2_mean': 11.427101130800146, 'true_reward_agent_2_min': -25.257414028048515, 'true_reward_agent_2_max': 60.03092351555824}\n",
      "{'true_reward_agent_0_mean': 28.477860109504253, 'true_reward_agent_0_min': -29.607445895671844, 'true_reward_agent_0_max': 98.73897376656532, 'true_reward_agent_1_mean': -2.5095987709025214, 'true_reward_agent_1_min': -24.884568613022566, 'true_reward_agent_1_max': 25.97830106317997, 'true_reward_agent_2_mean': 23.55683625216363, 'true_reward_agent_2_min': -25.980544172227383, 'true_reward_agent_2_max': 72.83322632312775}\n",
      "{'true_reward_agent_0_mean': 25.34440749351692, 'true_reward_agent_0_min': -29.59327758383006, 'true_reward_agent_0_max': 104.40105425193906, 'true_reward_agent_1_mean': -1.3701070283169974, 'true_reward_agent_1_min': -29.059191117994487, 'true_reward_agent_1_max': 30.10069876909256, 'true_reward_agent_2_mean': 23.30572634261829, 'true_reward_agent_2_min': -24.502028435468674, 'true_reward_agent_2_max': 76.31277188658714}\n",
      "{'true_reward_agent_0_mean': 26.72180764302946, 'true_reward_agent_0_min': -29.02172040194273, 'true_reward_agent_0_max': 98.87841776758432, 'true_reward_agent_1_mean': 0.14203721279278397, 'true_reward_agent_1_min': -22.805259704589844, 'true_reward_agent_1_max': 30.518079966306686, 'true_reward_agent_2_mean': 24.76417000601374, 'true_reward_agent_2_min': -24.794508308172226, 'true_reward_agent_2_max': 76.38146223127842}\n",
      "{'true_reward_agent_0_mean': 28.540376344731595, 'true_reward_agent_0_min': -27.66177001595497, 'true_reward_agent_0_max': 96.38919991254807, 'true_reward_agent_1_mean': 1.1633762935724916, 'true_reward_agent_1_min': -24.722291938960552, 'true_reward_agent_1_max': 28.71039581298828, 'true_reward_agent_2_mean': 26.691144061187806, 'true_reward_agent_2_min': -25.01844210922718, 'true_reward_agent_2_max': 79.09223306179047}\n",
      "{'true_reward_agent_0_mean': 31.647652642879983, 'true_reward_agent_0_min': -28.18840917944908, 'true_reward_agent_0_max': 87.6803053021431, 'true_reward_agent_1_mean': 1.0987630430761783, 'true_reward_agent_1_min': -24.41560297459364, 'true_reward_agent_1_max': 29.534144766628742, 'true_reward_agent_2_mean': 26.639282953901713, 'true_reward_agent_2_min': -24.529947087168694, 'true_reward_agent_2_max': 68.9640543460846}\n",
      "{'true_reward_agent_0_mean': 35.2730544003041, 'true_reward_agent_0_min': -28.18771967291832, 'true_reward_agent_0_max': 100.58473563194275, 'true_reward_agent_1_mean': 2.0424045324124744, 'true_reward_agent_1_min': -29.98400980234146, 'true_reward_agent_1_max': 32.38024267554283, 'true_reward_agent_2_mean': 28.269425530108563, 'true_reward_agent_2_min': -21.86341592669487, 'true_reward_agent_2_max': 72.78860267996788}\n",
      "{'true_reward_agent_0_mean': 36.79511904987914, 'true_reward_agent_0_min': -28.93946149945259, 'true_reward_agent_0_max': 101.50989866256714, 'true_reward_agent_1_mean': 2.4619747443500817, 'true_reward_agent_1_min': -24.413949321955442, 'true_reward_agent_1_max': 28.50090403854847, 'true_reward_agent_2_mean': 29.139779920069415, 'true_reward_agent_2_min': -21.779248669743538, 'true_reward_agent_2_max': 66.54216495156288}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-03 12:49:49,910\tERROR syncer.py:39 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_reward_agent_0_mean': 37.88226355888299, 'true_reward_agent_0_min': -26.087430112063885, 'true_reward_agent_0_max': 94.49110712110996, 'true_reward_agent_1_mean': 3.1386909396797273, 'true_reward_agent_1_min': -19.79840850830078, 'true_reward_agent_1_max': 34.79483203962445, 'true_reward_agent_2_mean': 31.419096953207845, 'true_reward_agent_2_min': -22.12456312775612, 'true_reward_agent_2_max': 78.5134951993823}\n",
      "\u001b[2m\u001b[36m(pid=9351)\u001b[0m /home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\u001b[2m\u001b[36m(pid=9351)\u001b[0m   from ._conv import register_converters as _register_converters\n",
      "\u001b[2m\u001b[36m(pid=9350)\u001b[0m /home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\u001b[2m\u001b[36m(pid=9350)\u001b[0m   from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-03 12:49:55,961\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-05-03 12:49:55,965\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_reward_agent_0_mean': -0.25357442714099304, 'true_reward_agent_0_min': -13.96676990389824, 'true_reward_agent_0_max': 14.782632521470077, 'true_reward_agent_1_mean': 0.2829695787985111, 'true_reward_agent_1_min': -14.117055165581405, 'true_reward_agent_1_max': 16.691753648221493, 'true_reward_agent_2_mean': -0.04163038854116166, 'true_reward_agent_2_min': -12.15682478249073, 'true_reward_agent_2_max': 15.184949710965157}\n",
      "{'true_reward_agent_0_mean': 4.634362882149726, 'true_reward_agent_0_min': -16.960823990404606, 'true_reward_agent_0_max': 35.359932268678676, 'true_reward_agent_1_mean': 0.8706147514446638, 'true_reward_agent_1_min': -18.46067025512457, 'true_reward_agent_1_max': 34.83531603962183, 'true_reward_agent_2_mean': -0.9164166976656998, 'true_reward_agent_2_min': -22.04696974158287, 'true_reward_agent_2_max': 20.290313452482224}\n",
      "{'true_reward_agent_0_mean': 3.7019351542956427, 'true_reward_agent_0_min': -19.36236497014761, 'true_reward_agent_0_max': 48.53447723388672, 'true_reward_agent_1_mean': -6.8290003798199175, 'true_reward_agent_1_min': -22.807962588965893, 'true_reward_agent_1_max': 40.732861150056124, 'true_reward_agent_2_mean': 0.30059288087126335, 'true_reward_agent_2_min': -27.95312586426735, 'true_reward_agent_2_max': 17.94203145802021}\n",
      "{'true_reward_agent_0_mean': 4.625265565699865, 'true_reward_agent_0_min': -22.13405279442668, 'true_reward_agent_0_max': 53.622347466647625, 'true_reward_agent_1_mean': -14.776858764548543, 'true_reward_agent_1_min': -26.973070114850998, 'true_reward_agent_1_max': 50.46455578505993, 'true_reward_agent_2_mean': 3.44123295309596, 'true_reward_agent_2_min': -20.19050806760788, 'true_reward_agent_2_max': 22.164825446903706}\n",
      "{'true_reward_agent_0_mean': 1.8552323545250693, 'true_reward_agent_0_min': -21.463724315166473, 'true_reward_agent_0_max': 54.147663064301014, 'true_reward_agent_1_mean': -17.258447854988482, 'true_reward_agent_1_min': -31.127351120114326, 'true_reward_agent_1_max': 32.74653282761574, 'true_reward_agent_2_mean': 4.254876137500105, 'true_reward_agent_2_min': -11.298929805867374, 'true_reward_agent_2_max': 21.753280207514763}\n",
      "{'true_reward_agent_0_mean': 1.8623651597401112, 'true_reward_agent_0_min': -22.806917153298855, 'true_reward_agent_0_max': 42.210509933531284, 'true_reward_agent_1_mean': -17.563909305440685, 'true_reward_agent_1_min': -30.264129742980003, 'true_reward_agent_1_max': 29.79503385350108, 'true_reward_agent_2_mean': 4.072675118443731, 'true_reward_agent_2_min': -15.64255565032363, 'true_reward_agent_2_max': 20.727470164652914}\n",
      "{'true_reward_agent_0_mean': 2.6723764004618715, 'true_reward_agent_0_min': -20.67434048652649, 'true_reward_agent_0_max': 43.6684062294662, 'true_reward_agent_1_mean': -17.429962020104032, 'true_reward_agent_1_min': -30.188986465334892, 'true_reward_agent_1_max': 34.663903914391994, 'true_reward_agent_2_mean': 3.970827364572979, 'true_reward_agent_2_min': -14.922017231583595, 'true_reward_agent_2_max': 25.856216222047806}\n",
      "{'true_reward_agent_0_mean': 2.0187548486572156, 'true_reward_agent_0_min': -21.173121735453606, 'true_reward_agent_0_max': 59.00931335729547, 'true_reward_agent_1_mean': -17.521526560202958, 'true_reward_agent_1_min': -28.41706409305334, 'true_reward_agent_1_max': 40.095670534297824, 'true_reward_agent_2_mean': 4.456858717406334, 'true_reward_agent_2_min': -14.931966731324792, 'true_reward_agent_2_max': 26.243077060207725}\n",
      "{'true_reward_agent_0_mean': 1.1325458023379724, 'true_reward_agent_0_min': -22.818359605968, 'true_reward_agent_0_max': 34.11674082279205, 'true_reward_agent_1_mean': -17.925268114916534, 'true_reward_agent_1_min': -30.071966816671193, 'true_reward_agent_1_max': 40.24449598649517, 'true_reward_agent_2_mean': 4.146318521574703, 'true_reward_agent_2_min': -14.568495720624924, 'true_reward_agent_2_max': 19.475861057639122}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-03 12:52:25,576\tERROR syncer.py:39 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_reward_agent_0_mean': 1.9738986582279359, 'true_reward_agent_0_min': -20.832955986261368, 'true_reward_agent_0_max': 54.807140827178955, 'true_reward_agent_1_mean': -16.880362026375188, 'true_reward_agent_1_min': -29.505124360322952, 'true_reward_agent_1_max': 45.63651278987527, 'true_reward_agent_2_mean': 4.232172342714803, 'true_reward_agent_2_min': -13.792873591184616, 'true_reward_agent_2_max': 24.29930578172207}\n",
      "gen\tnevals\tavg    \tstd    \tmin     \tmax    \n",
      "0  \t3     \t3.08491\t15.2275\t-11.3339\t24.1467\n",
      "\u001b[2m\u001b[36m(pid=9363)\u001b[0m /home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\u001b[2m\u001b[36m(pid=9363)\u001b[0m   from ._conv import register_converters as _register_converters\n",
      "\u001b[2m\u001b[36m(pid=9462)\u001b[0m /home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\u001b[2m\u001b[36m(pid=9462)\u001b[0m   from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-03 12:52:31,686\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-05-03 12:52:31,687\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_reward_agent_0_mean': -0.30241972306524983, 'true_reward_agent_0_min': -13.896786071360111, 'true_reward_agent_0_max': 13.630845069885254, 'true_reward_agent_1_mean': 0.6428976068622433, 'true_reward_agent_1_min': -19.827843889594078, 'true_reward_agent_1_max': 14.509237967431545, 'true_reward_agent_2_mean': -0.1772894032009208, 'true_reward_agent_2_min': -12.189038844779134, 'true_reward_agent_2_max': 23.95267380774021}\n",
      "{'true_reward_agent_0_mean': 3.1929477456660242, 'true_reward_agent_0_min': -24.834673024713993, 'true_reward_agent_0_max': 81.02191178500652, 'true_reward_agent_1_mean': -1.2830997437402403, 'true_reward_agent_1_min': -24.38797217607498, 'true_reward_agent_1_max': 55.90010266751051, 'true_reward_agent_2_mean': 5.672376704898852, 'true_reward_agent_2_min': -51.32901482284069, 'true_reward_agent_2_max': 42.20939818024635}\n",
      "{'true_reward_agent_0_mean': -12.879729356165917, 'true_reward_agent_0_min': -25.448423206806183, 'true_reward_agent_0_max': 84.24664422869682, 'true_reward_agent_1_mean': -12.436536711653753, 'true_reward_agent_1_min': -25.942617312073708, 'true_reward_agent_1_max': 51.6459963619709, 'true_reward_agent_2_mean': 19.91403545827401, 'true_reward_agent_2_min': -40.18830871582031, 'true_reward_agent_2_max': 41.903313130140305}\n",
      "{'true_reward_agent_0_mean': -15.746329503519245, 'true_reward_agent_0_min': -30.757892414927483, 'true_reward_agent_0_max': 71.4273567199707, 'true_reward_agent_1_mean': -13.701067937439001, 'true_reward_agent_1_min': -30.368420153856277, 'true_reward_agent_1_max': 60.933473601937294, 'true_reward_agent_2_mean': 22.654090917937683, 'true_reward_agent_2_min': -33.91857214272022, 'true_reward_agent_2_max': 46.35506974160671}\n",
      "{'true_reward_agent_0_mean': -16.944100410381214, 'true_reward_agent_0_min': -29.74461232125759, 'true_reward_agent_0_max': 70.97450368478894, 'true_reward_agent_1_mean': -15.002788463593024, 'true_reward_agent_1_min': -29.15182998776436, 'true_reward_agent_1_max': 56.37201191484928, 'true_reward_agent_2_mean': 23.462725189788106, 'true_reward_agent_2_min': -29.550480969250202, 'true_reward_agent_2_max': 44.02991634607315}\n",
      "{'true_reward_agent_0_mean': -16.984196555279958, 'true_reward_agent_0_min': -37.814037054777145, 'true_reward_agent_0_max': 72.3826703876257, 'true_reward_agent_1_mean': -14.341859881554047, 'true_reward_agent_1_min': -38.38390824198723, 'true_reward_agent_1_max': 63.328523720265366, 'true_reward_agent_2_mean': 23.67632656964619, 'true_reward_agent_2_min': -28.5847774669528, 'true_reward_agent_2_max': 48.43858081847429}\n",
      "{'true_reward_agent_0_mean': -16.391625784507852, 'true_reward_agent_0_min': -30.336560933850706, 'true_reward_agent_0_max': 75.5854062512517, 'true_reward_agent_1_mean': -14.20416560308673, 'true_reward_agent_1_min': -28.61526794731617, 'true_reward_agent_1_max': 59.25054829195142, 'true_reward_agent_2_mean': 22.840802085475296, 'true_reward_agent_2_min': -38.19053538143635, 'true_reward_agent_2_max': 41.21356585621834}\n",
      "{'true_reward_agent_0_mean': -18.086165420520892, 'true_reward_agent_0_min': -32.12701229378581, 'true_reward_agent_0_max': 50.49292759411037, 'true_reward_agent_1_mean': -15.477111632269166, 'true_reward_agent_1_min': -33.99784064292908, 'true_reward_agent_1_max': 56.56507820938714, 'true_reward_agent_2_mean': 24.41594670391055, 'true_reward_agent_2_min': -30.954838917590678, 'true_reward_agent_2_max': 45.414690267294645}\n",
      "{'true_reward_agent_0_mean': -17.3347568970644, 'true_reward_agent_0_min': -31.712037429213524, 'true_reward_agent_0_max': 53.00868450850248, 'true_reward_agent_1_mean': -14.562732340033545, 'true_reward_agent_1_min': -29.384892404079437, 'true_reward_agent_1_max': 56.557318020612, 'true_reward_agent_2_mean': 23.65561136195018, 'true_reward_agent_2_min': -31.433030262589455, 'true_reward_agent_2_max': 47.29110278189182}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-03 12:54:56,287\tERROR syncer.py:39 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_reward_agent_0_mean': -17.19477107252318, 'true_reward_agent_0_min': -35.03441867232323, 'true_reward_agent_0_max': 61.716988891363144, 'true_reward_agent_1_mean': -14.642290729974848, 'true_reward_agent_1_min': -34.25021241605282, 'true_reward_agent_1_max': 47.69436630606651, 'true_reward_agent_2_mean': 23.85136944721271, 'true_reward_agent_2_min': -30.557699874043465, 'true_reward_agent_2_max': 45.52760075032711}\n",
      "\u001b[2m\u001b[36m(pid=9541)\u001b[0m /home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\u001b[2m\u001b[36m(pid=9541)\u001b[0m   from ._conv import register_converters as _register_converters\n",
      "\u001b[2m\u001b[36m(pid=9542)\u001b[0m /home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\u001b[2m\u001b[36m(pid=9542)\u001b[0m   from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-03 12:55:02,614\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-05-03 12:55:02,615\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_reward_agent_0_mean': -0.1633284401099081, 'true_reward_agent_0_min': -17.062376998364925, 'true_reward_agent_0_max': 16.88639849051833, 'true_reward_agent_1_mean': 0.2473616630930337, 'true_reward_agent_1_min': -12.864770494401455, 'true_reward_agent_1_max': 18.931149289011955, 'true_reward_agent_2_mean': 0.1776803478381771, 'true_reward_agent_2_min': -13.266850480809808, 'true_reward_agent_2_max': 16.609469570219517}\n",
      "{'true_reward_agent_0_mean': -1.0906223294431039, 'true_reward_agent_0_min': -22.010109370574355, 'true_reward_agent_0_max': 43.09563681483269, 'true_reward_agent_1_mean': 3.2763092634048006, 'true_reward_agent_1_min': -17.95288346707821, 'true_reward_agent_1_max': 24.856494599953294, 'true_reward_agent_2_mean': 2.144435331440054, 'true_reward_agent_2_min': -18.326523691415787, 'true_reward_agent_2_max': 22.407754676416516}\n",
      "{'true_reward_agent_0_mean': -4.4395671349557775, 'true_reward_agent_0_min': -27.562517369784473, 'true_reward_agent_0_max': 44.62791334465146, 'true_reward_agent_1_mean': 4.7058672240713255, 'true_reward_agent_1_min': -14.633674383163452, 'true_reward_agent_1_max': 23.934570772573352, 'true_reward_agent_2_mean': 4.359301635842112, 'true_reward_agent_2_min': -18.536962255835533, 'true_reward_agent_2_max': 31.553494887892157}\n",
      "{'true_reward_agent_0_mean': -4.537916022459504, 'true_reward_agent_0_min': -28.847951740957797, 'true_reward_agent_0_max': 46.09633198752999, 'true_reward_agent_1_mean': 5.2309533715515135, 'true_reward_agent_1_min': -17.584875436499715, 'true_reward_agent_1_max': 25.495254062116146, 'true_reward_agent_2_mean': 4.603548844615143, 'true_reward_agent_2_min': -19.29067164286971, 'true_reward_agent_2_max': 29.321081293281168}\n",
      "{'true_reward_agent_0_mean': -4.533287913258555, 'true_reward_agent_0_min': -26.530489034950733, 'true_reward_agent_0_max': 44.142557464540005, 'true_reward_agent_1_mean': 5.217383919602699, 'true_reward_agent_1_min': -21.923762552440166, 'true_reward_agent_1_max': 29.10458872653544, 'true_reward_agent_2_mean': 4.689833047092589, 'true_reward_agent_2_min': -18.035253026522696, 'true_reward_agent_2_max': 32.131896674633026}\n",
      "{'true_reward_agent_0_mean': -4.2097446285345, 'true_reward_agent_0_min': -28.75200266391039, 'true_reward_agent_0_max': 37.75913656502962, 'true_reward_agent_1_mean': 4.808776594727606, 'true_reward_agent_1_min': -16.900596164166927, 'true_reward_agent_1_max': 24.234593637287617, 'true_reward_agent_2_mean': 4.071395511203591, 'true_reward_agent_2_min': -20.28930938988924, 'true_reward_agent_2_max': 26.12110177706927}\n",
      "{'true_reward_agent_0_mean': -5.417720636084608, 'true_reward_agent_0_min': -28.479080580174923, 'true_reward_agent_0_max': 42.290738109499216, 'true_reward_agent_1_mean': 4.838475015582613, 'true_reward_agent_1_min': -13.963660821318626, 'true_reward_agent_1_max': 23.867157869040966, 'true_reward_agent_2_mean': 5.281216280500605, 'true_reward_agent_2_min': -17.964398879557848, 'true_reward_agent_2_max': 26.832158640027046}\n",
      "{'true_reward_agent_0_mean': -5.896603269988845, 'true_reward_agent_0_min': -27.71065260330215, 'true_reward_agent_0_max': 37.599005833268166, 'true_reward_agent_1_mean': 4.484849212905392, 'true_reward_agent_1_min': -17.04858480580151, 'true_reward_agent_1_max': 25.275460228323936, 'true_reward_agent_2_mean': 5.13432697905635, 'true_reward_agent_2_min': -14.953168818727136, 'true_reward_agent_2_max': 27.403431236743927}\n",
      "{'true_reward_agent_0_mean': -6.421535933518299, 'true_reward_agent_0_min': -32.725397285073996, 'true_reward_agent_0_max': 43.53967219684273, 'true_reward_agent_1_mean': 5.043371250046639, 'true_reward_agent_1_min': -16.36422799527645, 'true_reward_agent_1_max': 24.042165830731392, 'true_reward_agent_2_mean': 5.341027074516415, 'true_reward_agent_2_min': -17.658869979903102, 'true_reward_agent_2_max': 39.8676343858242}\n",
      "{'true_reward_agent_0_mean': -7.2945789822389635, 'true_reward_agent_0_min': -28.789411135017872, 'true_reward_agent_0_max': 40.80625952407718, 'true_reward_agent_1_mean': 4.190896687395943, 'true_reward_agent_1_min': -15.570284500718117, 'true_reward_agent_1_max': 24.461688861250877, 'true_reward_agent_2_mean': 5.806241856638881, 'true_reward_agent_2_min': -17.23169770464301, 'true_reward_agent_2_max': 26.65442870184779}\n",
      "1  \t2     \t7.46188\t11.8873\t-2.6619 \t24.1467\n",
      "2  \t0     \t-0.28673\t1.6795 \t-2.6619 \t0.900853\n"
     ]
    }
   ],
   "source": [
    "#Use evaluate individual - select for group selection\n",
    "pop = toolbox.population(n=n_agents)\n",
    "toolbox.register('evaluate', evaluate_individual)\n",
    "hof = tools.HallOfFame(10)\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register('avg', np.mean)\n",
    "stats.register('std', np.std)\n",
    "stats.register('min', np.min)\n",
    "stats.register('max', np.max)\n",
    "\n",
    "pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=2, \n",
    "                                   stats=stats, halloffame=hof, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 19:44:09,694\tINFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "2020-05-02 19:44:09,789\tERROR syncer.py:39 -- Log sync requires rsync to be installed.\n",
      "2020-05-02 19:44:09,798\tINFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17780)\u001b[0m /home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\u001b[2m\u001b[36m(pid=17780)\u001b[0m   from ._conv import register_converters as _register_converters\n",
      "\u001b[2m\u001b[36m(pid=17781)\u001b[0m /home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\u001b[2m\u001b[36m(pid=17781)\u001b[0m   from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 19:44:29,038\tINFO trainable.py:180 -- _setup took 19.245 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2020-05-02 19:44:29,039\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-05-02 19:44:29,040\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_reward_agent_0_mean': -0.03672808718911256, 'true_reward_agent_0_min': -14.902736930991523, 'true_reward_agent_0_max': 14.684467680752277, 'true_reward_agent_1_mean': 0.10257448901487805, 'true_reward_agent_1_min': -13.657341606914997, 'true_reward_agent_1_max': 21.018747905269265, 'true_reward_agent_2_mean': 0.01720135783081787, 'true_reward_agent_2_min': -13.558223649859428, 'true_reward_agent_2_max': 16.660553514957428}\n",
      "{'true_reward_agent_0_mean': -3.3104821921887524, 'true_reward_agent_0_min': -22.918064691126347, 'true_reward_agent_0_max': 37.737172320485115, 'true_reward_agent_1_mean': -0.05132808728485998, 'true_reward_agent_1_min': -16.546868479810655, 'true_reward_agent_1_max': 19.297310791909695, 'true_reward_agent_2_mean': -3.0512137598750524, 'true_reward_agent_2_min': -15.538922376930714, 'true_reward_agent_2_max': 16.56323118507862}\n",
      "{'true_reward_agent_0_mean': -7.628187831962751, 'true_reward_agent_0_min': -26.3912685662508, 'true_reward_agent_0_max': 46.75625977292657, 'true_reward_agent_1_mean': -6.2513778362189445, 'true_reward_agent_1_min': -21.841339863836765, 'true_reward_agent_1_max': 16.536014534533024, 'true_reward_agent_2_mean': -4.777791737255647, 'true_reward_agent_2_min': -19.019059523940086, 'true_reward_agent_2_max': 17.9944216334261}\n",
      "{'true_reward_agent_0_mean': -11.81705640147371, 'true_reward_agent_0_min': -30.95806133747101, 'true_reward_agent_0_max': 40.73220468591899, 'true_reward_agent_1_mean': -7.632902071597892, 'true_reward_agent_1_min': -26.005508709698915, 'true_reward_agent_1_max': 16.173384511843324, 'true_reward_agent_2_mean': -4.313491586352848, 'true_reward_agent_2_min': -18.797007963061333, 'true_reward_agent_2_max': 16.282567020505667}\n",
      "{'true_reward_agent_0_mean': -11.446422078782852, 'true_reward_agent_0_min': -29.692390859127045, 'true_reward_agent_0_max': 56.43646205961704, 'true_reward_agent_1_mean': -8.347879388162564, 'true_reward_agent_1_min': -28.03753486368805, 'true_reward_agent_1_max': 16.80947821214795, 'true_reward_agent_2_mean': -3.5313831934626068, 'true_reward_agent_2_min': -18.588718593120575, 'true_reward_agent_2_max': 18.44470762554556}\n",
      "{'true_reward_agent_0_mean': -12.593146148504239, 'true_reward_agent_0_min': -33.0538624972105, 'true_reward_agent_0_max': 49.97914254060015, 'true_reward_agent_1_mean': -7.630799582872933, 'true_reward_agent_1_min': -23.92444932460785, 'true_reward_agent_1_max': 17.439100235700607, 'true_reward_agent_2_mean': -4.047448905590536, 'true_reward_agent_2_min': -17.212732411921024, 'true_reward_agent_2_max': 20.833138436079025}\n",
      "{'true_reward_agent_0_mean': -14.444285797515931, 'true_reward_agent_0_min': -30.65702611207962, 'true_reward_agent_0_max': 44.47339901700616, 'true_reward_agent_1_mean': -8.446655741722688, 'true_reward_agent_1_min': -23.234982691705227, 'true_reward_agent_1_max': 24.6982351411134, 'true_reward_agent_2_mean': -3.9498978847274158, 'true_reward_agent_2_min': -20.91677536070347, 'true_reward_agent_2_max': 16.46358253993094}\n",
      "{'true_reward_agent_0_mean': -13.63007016496449, 'true_reward_agent_0_min': -30.026272982358932, 'true_reward_agent_0_max': 48.87273438088596, 'true_reward_agent_1_mean': -8.445832217308416, 'true_reward_agent_1_min': -25.78761000931263, 'true_reward_agent_1_max': 20.00469825323671, 'true_reward_agent_2_mean': -3.7699619691315456, 'true_reward_agent_2_min': -20.220649018883705, 'true_reward_agent_2_max': 13.293759435415268}\n",
      "{'true_reward_agent_0_mean': -14.116505951230975, 'true_reward_agent_0_min': -30.564459389075637, 'true_reward_agent_0_max': 44.633245321922004, 'true_reward_agent_1_mean': -8.518284954375522, 'true_reward_agent_1_min': -25.341087389737368, 'true_reward_agent_1_max': 20.618558697402477, 'true_reward_agent_2_mean': -3.5764269963048356, 'true_reward_agent_2_min': -18.5827876329422, 'true_reward_agent_2_max': 22.980668995529413}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 19:46:50,013\tERROR syncer.py:39 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_reward_agent_0_mean': -13.463509728138398, 'true_reward_agent_0_min': -31.726001374423504, 'true_reward_agent_0_max': 44.61888311430812, 'true_reward_agent_1_mean': -8.511883776946451, 'true_reward_agent_1_min': -28.87863167375326, 'true_reward_agent_1_max': 21.5758962277323, 'true_reward_agent_2_mean': -4.653170590403096, 'true_reward_agent_2_min': -22.34093351662159, 'true_reward_agent_2_max': 24.591818057000637}\n",
      "gen\tnevals\tavg     \tstd    \tmin     \tmax     \n",
      "0  \t3     \t-8.87619\t3.60602\t-13.4635\t-4.65317\n",
      "population [array('d', [-0.8179451046903832, 0.8381507555515655]), array('d', [-0.8255587441221259, 0.6138589995044808]), array('d', [0.5042140939425694, -0.3126186444144592])]\n",
      "\u001b[2m\u001b[36m(pid=17782)\u001b[0m /home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\u001b[2m\u001b[36m(pid=17782)\u001b[0m   from ._conv import register_converters as _register_converters\n",
      "\u001b[2m\u001b[36m(pid=17783)\u001b[0m /home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\u001b[2m\u001b[36m(pid=17783)\u001b[0m   from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 19:47:08,882\tINFO trainable.py:180 -- _setup took 18.868 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2020-05-02 19:47:08,882\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-05-02 19:47:08,883\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_reward_agent_0_mean': -0.11330947479291353, 'true_reward_agent_0_min': -13.030011162161827, 'true_reward_agent_0_max': 17.891326010227203, 'true_reward_agent_1_mean': -0.27129522192903094, 'true_reward_agent_1_min': -12.189794339239597, 'true_reward_agent_1_max': 18.658127814531326, 'true_reward_agent_2_mean': 0.17577213245342135, 'true_reward_agent_2_min': -17.081376053392887, 'true_reward_agent_2_max': 20.274739645421505}\n",
      "{'true_reward_agent_0_mean': -1.0008624063792013, 'true_reward_agent_0_min': -19.85705430060625, 'true_reward_agent_0_max': 19.455532629042864, 'true_reward_agent_1_mean': 1.0167946666359784, 'true_reward_agent_1_min': -24.610318889841437, 'true_reward_agent_1_max': 41.40404460579157, 'true_reward_agent_2_mean': 1.8600263651762907, 'true_reward_agent_2_min': -18.970519348978996, 'true_reward_agent_2_max': 21.800836976617575}\n",
      "{'true_reward_agent_0_mean': -2.055206073766649, 'true_reward_agent_0_min': -25.561499568633735, 'true_reward_agent_0_max': 23.711720287799835, 'true_reward_agent_1_mean': -2.126263764558262, 'true_reward_agent_1_min': -31.988144993782043, 'true_reward_agent_1_max': 48.04317408800125, 'true_reward_agent_2_mean': 3.2478824945092217, 'true_reward_agent_2_min': -22.300235932692885, 'true_reward_agent_2_max': 33.53246796131134}\n",
      "{'true_reward_agent_0_mean': -3.5236033338327912, 'true_reward_agent_0_min': -24.99370827525854, 'true_reward_agent_0_max': 32.89553124457598, 'true_reward_agent_1_mean': -1.1131190524243926, 'true_reward_agent_1_min': -35.43251636624336, 'true_reward_agent_1_max': 48.47759521752596, 'true_reward_agent_2_mean': 2.9969276213485445, 'true_reward_agent_2_min': -26.195096261799335, 'true_reward_agent_2_max': 37.940769992768764}\n",
      "{'true_reward_agent_0_mean': -5.864387046272495, 'true_reward_agent_0_min': -26.492556024342775, 'true_reward_agent_0_max': 31.17487743496895, 'true_reward_agent_1_mean': 2.9942425821146026, 'true_reward_agent_1_min': -33.17791932821274, 'true_reward_agent_1_max': 57.795996529981494, 'true_reward_agent_2_mean': 0.11419622190142036, 'true_reward_agent_2_min': -24.471731612458825, 'true_reward_agent_2_max': 40.8862193133682}\n",
      "{'true_reward_agent_0_mean': -7.419829330457251, 'true_reward_agent_0_min': -27.556127287447453, 'true_reward_agent_0_max': 29.089948958950117, 'true_reward_agent_1_mean': 4.977927700008913, 'true_reward_agent_1_min': -35.58124648779631, 'true_reward_agent_1_max': 51.490603506565094, 'true_reward_agent_2_mean': -0.9951950821844184, 'true_reward_agent_2_min': -24.230784937739372, 'true_reward_agent_2_max': 34.61636509746313}\n",
      "{'true_reward_agent_0_mean': -6.682443121582674, 'true_reward_agent_0_min': -30.011382550001144, 'true_reward_agent_0_max': 28.07389822602272, 'true_reward_agent_1_mean': 3.0780904778370313, 'true_reward_agent_1_min': -32.977633625268936, 'true_reward_agent_1_max': 50.97582824528217, 'true_reward_agent_2_mean': 0.4987498731128744, 'true_reward_agent_2_min': -26.091557033360004, 'true_reward_agent_2_max': 37.5913196913898}\n",
      "{'true_reward_agent_0_mean': -6.590894534485815, 'true_reward_agent_0_min': -28.198469638824463, 'true_reward_agent_0_max': 28.549073299393058, 'true_reward_agent_1_mean': 2.2754094083193195, 'true_reward_agent_1_min': -41.4278227686882, 'true_reward_agent_1_max': 60.51441229879856, 'true_reward_agent_2_mean': 1.0936467457005348, 'true_reward_agent_2_min': -27.287001881748438, 'true_reward_agent_2_max': 39.9975166246295}\n",
      "{'true_reward_agent_0_mean': -6.513871994787187, 'true_reward_agent_0_min': -26.981948320753872, 'true_reward_agent_0_max': 26.39957869052887, 'true_reward_agent_1_mean': 1.4498214184727112, 'true_reward_agent_1_min': -35.26923468708992, 'true_reward_agent_1_max': 52.86235756240785, 'true_reward_agent_2_mean': 0.7955958483270661, 'true_reward_agent_2_min': -23.00878332182765, 'true_reward_agent_2_max': 36.38455294072628}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 19:49:28,985\tERROR syncer.py:39 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_reward_agent_0_mean': -7.197712974591486, 'true_reward_agent_0_min': -30.207335256040096, 'true_reward_agent_0_max': 29.585197642445564, 'true_reward_agent_1_mean': 2.6962196839671377, 'true_reward_agent_1_min': -34.793314695358276, 'true_reward_agent_1_max': 52.56577454507351, 'true_reward_agent_2_mean': 0.3625911633353644, 'true_reward_agent_2_min': -27.642026364803314, 'true_reward_agent_2_max': 35.475639291107655}\n",
      "1  \t3     \t-1.37963\t4.22287\t-7.19771\t2.69622 \n",
      "population [array('d', [-0.8255587441221259, -0.3126186444144592]), array('d', [0.5042140939425694, 0.6138589995044808]), array('d', [0.5042140939425694, -0.3126186444144592])]\n",
      "\u001b[2m\u001b[36m(pid=18047)\u001b[0m /home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\u001b[2m\u001b[36m(pid=18047)\u001b[0m   from ._conv import register_converters as _register_converters\n",
      "\u001b[2m\u001b[36m(pid=18048)\u001b[0m /home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\u001b[2m\u001b[36m(pid=18048)\u001b[0m   from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 19:49:48,195\tINFO trainable.py:180 -- _setup took 19.209 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2020-05-02 19:49:48,195\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-05-02 19:49:48,196\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_reward_agent_0_mean': -0.10516482778166392, 'true_reward_agent_0_min': -13.142949096858501, 'true_reward_agent_0_max': 18.579245075583458, 'true_reward_agent_1_mean': 0.3342458797002291, 'true_reward_agent_1_min': -13.988721085712314, 'true_reward_agent_1_max': 17.159135138615966, 'true_reward_agent_2_mean': -0.14160609966904303, 'true_reward_agent_2_min': -14.808785647153854, 'true_reward_agent_2_max': 17.225516138598323}\n",
      "{'true_reward_agent_0_mean': 0.3255056980250447, 'true_reward_agent_0_min': -16.705914959311485, 'true_reward_agent_0_max': 21.934297181665897, 'true_reward_agent_1_mean': -0.30527540663162656, 'true_reward_agent_1_min': -12.476791235327255, 'true_reward_agent_1_max': 14.820636610034853, 'true_reward_agent_2_mean': -0.3281357921641029, 'true_reward_agent_2_min': -11.425935875624418, 'true_reward_agent_2_max': 19.170060358941555}\n",
      "{'true_reward_agent_0_mean': 5.803816202782018, 'true_reward_agent_0_min': -22.7904114946723, 'true_reward_agent_0_max': 37.74996955692768, 'true_reward_agent_1_mean': -1.1144678408413164, 'true_reward_agent_1_min': -15.104023940861225, 'true_reward_agent_1_max': 18.77951266989112, 'true_reward_agent_2_mean': -1.8676766419691193, 'true_reward_agent_2_min': -16.05406984174624, 'true_reward_agent_2_max': 16.53550236672163}\n",
      "{'true_reward_agent_0_mean': 8.763962840264066, 'true_reward_agent_0_min': -31.802974238991737, 'true_reward_agent_0_max': 47.94937861338258, 'true_reward_agent_1_mean': -2.1094487142030993, 'true_reward_agent_1_min': -13.945253256708384, 'true_reward_agent_1_max': 12.184481407166459, 'true_reward_agent_2_mean': -3.0401106741854163, 'true_reward_agent_2_min': -15.45381630398333, 'true_reward_agent_2_max': 12.582988485693932}\n",
      "{'true_reward_agent_0_mean': 9.60682804371023, 'true_reward_agent_0_min': -28.84562267921865, 'true_reward_agent_0_max': 42.08197709172964, 'true_reward_agent_1_mean': -1.8701973413363886, 'true_reward_agent_1_min': -16.248604025691748, 'true_reward_agent_1_max': 11.713785661384463, 'true_reward_agent_2_mean': -4.644828688397356, 'true_reward_agent_2_min': -14.32776191085577, 'true_reward_agent_2_max': 17.50426142476499}\n",
      "{'true_reward_agent_0_mean': 8.504649035497168, 'true_reward_agent_0_min': -23.2819102704525, 'true_reward_agent_0_max': 42.92136690788902, 'true_reward_agent_1_mean': -2.1526363646247044, 'true_reward_agent_1_min': -13.202705051749945, 'true_reward_agent_1_max': 14.030800975859165, 'true_reward_agent_2_mean': -3.6279859160425985, 'true_reward_agent_2_min': -13.330938782542944, 'true_reward_agent_2_max': 13.808869934175164}\n",
      "{'true_reward_agent_0_mean': 9.716919519706535, 'true_reward_agent_0_min': -30.602187544107437, 'true_reward_agent_0_max': 54.570881351828575, 'true_reward_agent_1_mean': -2.303701535819041, 'true_reward_agent_1_min': -12.623765803873539, 'true_reward_agent_1_max': 12.289117030799389, 'true_reward_agent_2_mean': -3.4855422119262673, 'true_reward_agent_2_min': -16.603885794989765, 'true_reward_agent_2_max': 20.204202122986317}\n",
      "{'true_reward_agent_0_mean': 9.90295066273975, 'true_reward_agent_0_min': -25.217221554368734, 'true_reward_agent_0_max': 45.52400432433933, 'true_reward_agent_1_mean': -2.548503179991858, 'true_reward_agent_1_min': -14.816020055441186, 'true_reward_agent_1_max': 12.71896693110466, 'true_reward_agent_2_mean': -3.9797903063709965, 'true_reward_agent_2_min': -15.335011647548527, 'true_reward_agent_2_max': 15.343822404742241}\n",
      "{'true_reward_agent_0_mean': 10.231881518462723, 'true_reward_agent_0_min': -25.853299286216497, 'true_reward_agent_0_max': 46.338922143913805, 'true_reward_agent_1_mean': -2.361598526644502, 'true_reward_agent_1_min': -15.572812370955944, 'true_reward_agent_1_max': 15.785362251102924, 'true_reward_agent_2_mean': -3.189533386033572, 'true_reward_agent_2_min': -13.958712175488472, 'true_reward_agent_2_max': 15.94292552722618}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 19:52:07,525\tERROR syncer.py:39 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_reward_agent_0_mean': 8.915034366304807, 'true_reward_agent_0_min': -27.971521943807602, 'true_reward_agent_0_max': 50.16226092353463, 'true_reward_agent_1_mean': -2.7409492544754177, 'true_reward_agent_1_min': -17.782050577225164, 'true_reward_agent_1_max': 14.182712577283382, 'true_reward_agent_2_mean': -2.691558982191964, 'true_reward_agent_2_min': -15.104011187329888, 'true_reward_agent_2_max': 16.981375370174646}\n",
      "2  \t3     \t1.16084 \t5.48308\t-2.74095\t8.91503 \n",
      "population [array('d', [0.5042140939425694, -0.3126186444144592]), array('d', [-0.8255587441221259, 0.6138589995044808]), array('d', [0.5042140939425694, 0.6138589995044808])]\n",
      "\u001b[2m\u001b[36m(pid=18058)\u001b[0m /home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\u001b[2m\u001b[36m(pid=18058)\u001b[0m   from ._conv import register_converters as _register_converters\n",
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m /home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m   from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 19:52:26,598\tINFO trainable.py:180 -- _setup took 19.072 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2020-05-02 19:52:26,599\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-05-02 19:52:26,599\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_reward_agent_0_mean': 0.09126990698758164, 'true_reward_agent_0_min': -14.828576222062111, 'true_reward_agent_0_max': 18.506440553814173, 'true_reward_agent_1_mean': 0.24993146612294367, 'true_reward_agent_1_min': -14.371537514030933, 'true_reward_agent_1_max': 20.74822475016117, 'true_reward_agent_2_mean': -0.1708565825290134, 'true_reward_agent_2_min': -11.774140860885382, 'true_reward_agent_2_max': 15.316454976797104}\n",
      "{'true_reward_agent_0_mean': -0.8091277288935089, 'true_reward_agent_0_min': -17.74844066798687, 'true_reward_agent_0_max': 16.605887584388256, 'true_reward_agent_1_mean': 6.517130101349685, 'true_reward_agent_1_min': -13.541281402111053, 'true_reward_agent_1_max': 38.81933831423521, 'true_reward_agent_2_mean': -1.4573186728635483, 'true_reward_agent_2_min': -16.77055600658059, 'true_reward_agent_2_max': 17.80556874105241}\n",
      "{'true_reward_agent_0_mean': -2.595669973913573, 'true_reward_agent_0_min': -15.56043853238225, 'true_reward_agent_0_max': 16.878474798053503, 'true_reward_agent_1_mean': 16.578369949348343, 'true_reward_agent_1_min': -18.04410034418106, 'true_reward_agent_1_max': 52.91061198711395, 'true_reward_agent_2_mean': -3.745252856133666, 'true_reward_agent_2_min': -17.260256990790367, 'true_reward_agent_2_max': 16.532135143876076}\n",
      "{'true_reward_agent_0_mean': -2.4964871600888365, 'true_reward_agent_0_min': -16.876857487484813, 'true_reward_agent_0_max': 22.653169497847557, 'true_reward_agent_1_mean': 19.591815084485134, 'true_reward_agent_1_min': -22.950962215662003, 'true_reward_agent_1_max': 64.88325601443648, 'true_reward_agent_2_mean': -4.158435937802133, 'true_reward_agent_2_min': -18.761934192851186, 'true_reward_agent_2_max': 17.128163397312164}\n",
      "{'true_reward_agent_0_mean': -2.542204903551792, 'true_reward_agent_0_min': -14.258322671055794, 'true_reward_agent_0_max': 20.799386844038963, 'true_reward_agent_1_mean': 20.859142545972063, 'true_reward_agent_1_min': -19.458875606535003, 'true_reward_agent_1_max': 56.21264886390418, 'true_reward_agent_2_mean': -3.9065725436997853, 'true_reward_agent_2_min': -19.240024057216942, 'true_reward_agent_2_max': 19.09205700457096}\n",
      "{'true_reward_agent_0_mean': -1.9876697116616198, 'true_reward_agent_0_min': -17.644758515059948, 'true_reward_agent_0_max': 23.422188192605972, 'true_reward_agent_1_mean': 21.090753439358412, 'true_reward_agent_1_min': -22.982469879090786, 'true_reward_agent_1_max': 60.25638959556818, 'true_reward_agent_2_mean': -4.004761728194881, 'true_reward_agent_2_min': -17.572322234511375, 'true_reward_agent_2_max': 19.429334019310772}\n",
      "{'true_reward_agent_0_mean': -2.4417531188856083, 'true_reward_agent_0_min': -19.203957423567772, 'true_reward_agent_0_max': 19.27889544516802, 'true_reward_agent_1_mean': 20.34396237206587, 'true_reward_agent_1_min': -29.54587882384658, 'true_reward_agent_1_max': 57.10758494772017, 'true_reward_agent_2_mean': -3.4279512780141888, 'true_reward_agent_2_min': -16.61257917433977, 'true_reward_agent_2_max': 19.684551939368248}\n",
      "{'true_reward_agent_0_mean': -1.9972148993187875, 'true_reward_agent_0_min': -18.51774312276393, 'true_reward_agent_0_max': 26.639377146959305, 'true_reward_agent_1_mean': 20.206871546537876, 'true_reward_agent_1_min': -22.51560727506876, 'true_reward_agent_1_max': 64.73841696884483, 'true_reward_agent_2_mean': -3.5596970011393934, 'true_reward_agent_2_min': -17.20693608187139, 'true_reward_agent_2_max': 20.629517074674368}\n",
      "{'true_reward_agent_0_mean': -2.5289494864959123, 'true_reward_agent_0_min': -15.035623237490654, 'true_reward_agent_0_max': 17.334802985191345, 'true_reward_agent_1_mean': 21.536558869811923, 'true_reward_agent_1_min': -24.992673948407173, 'true_reward_agent_1_max': 58.843290627002716, 'true_reward_agent_2_mean': -3.8743476049593664, 'true_reward_agent_2_min': -18.958148254547268, 'true_reward_agent_2_max': 15.426741721108556}\n",
      "{'true_reward_agent_0_mean': -2.5729126606321007, 'true_reward_agent_0_min': -15.662712438032031, 'true_reward_agent_0_max': 17.677172169089317, 'true_reward_agent_1_mean': 22.663667153344687, 'true_reward_agent_1_min': -20.66631717979908, 'true_reward_agent_1_max': 56.629332575947046, 'true_reward_agent_2_mean': -3.8253864159235125, 'true_reward_agent_2_min': -17.209767814725637, 'true_reward_agent_2_max': 18.104524480178952}\n",
      "3  \t3     \t5.42179 \t12.2026\t-3.82539\t22.6637 \n",
      "pop [array('d', [0.5042140939425694, -0.3126186444144592]), array('d', [0.5042140939425694, -0.3126186444144592]), array('d', [0.5042140939425694, -0.3126186444144592])]\n"
     ]
    }
   ],
   "source": [
    "#Use evaluate population\n",
    "pop = toolbox.population(n=n_agents)\n",
    "hof = tools.HallOfFame(10)\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register('avg', np.mean)\n",
    "stats.register('std', np.std)\n",
    "stats.register('min', np.min)\n",
    "stats.register('max', np.max)\n",
    "\n",
    "(pop, log) = evolve(\n",
    "    pop,\n",
    "    toolbox,\n",
    "    cxpb=0.5,\n",
    "    mutpb=0.2,\n",
    "    ngen=3,\n",
    "    stats=stats,\n",
    "    halloffame=hof,\n",
    "    verbose=True,\n",
    "    )\n",
    "\n",
    "print ('pop', pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 15:02:15,714\tINFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "2020-05-02 15:02:15,788\tERROR syncer.py:39 -- Log sync requires rsync to be installed.\n",
      "2020-05-02 15:02:15,798\tINFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=10479)\u001b[0m /home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\u001b[2m\u001b[36m(pid=10479)\u001b[0m   from ._conv import register_converters as _register_converters\n",
      "\u001b[2m\u001b[36m(pid=10478)\u001b[0m /home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\u001b[2m\u001b[36m(pid=10478)\u001b[0m   from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 15:02:34,721\tINFO trainable.py:180 -- _setup took 18.933 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2020-05-02 15:02:34,722\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-05-02 15:02:34,723\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_reward_agent_0_mean': 0.06309596741950373, 'true_reward_agent_0_min': -15.793003261089325, 'true_reward_agent_0_max': 20.82217174768448, 'true_reward_agent_1_mean': -0.07088532369147288, 'true_reward_agent_1_min': -13.382614257046953, 'true_reward_agent_1_max': 17.442511551082134, 'true_reward_agent_2_mean': -0.10051303304877365, 'true_reward_agent_2_min': -13.546605411916971, 'true_reward_agent_2_max': 14.234586730599403}\n",
      "{'true_reward_agent_0_mean': -1.4887058582130954, 'true_reward_agent_0_min': -17.87443380570039, 'true_reward_agent_0_max': 16.0120142782107, 'true_reward_agent_1_mean': 0.18680497107047814, 'true_reward_agent_1_min': -22.44005953706801, 'true_reward_agent_1_max': 22.56702247262001, 'true_reward_agent_2_mean': 0.589440964944854, 'true_reward_agent_2_min': -14.150779940187931, 'true_reward_agent_2_max': 23.312319873366505}\n",
      "{'true_reward_agent_0_mean': -2.0609699610471353, 'true_reward_agent_0_min': -21.483412104658782, 'true_reward_agent_0_max': 18.646432980895042, 'true_reward_agent_1_mean': -1.3159576987744641, 'true_reward_agent_1_min': -26.310951497405767, 'true_reward_agent_1_max': 35.93740080296993, 'true_reward_agent_2_mean': 1.4816883622391834, 'true_reward_agent_2_min': -14.796920895576477, 'true_reward_agent_2_max': 25.22360012680292}\n",
      "{'true_reward_agent_0_mean': 2.143811938447343, 'true_reward_agent_0_min': -18.032710023224354, 'true_reward_agent_0_max': 30.40882419794798, 'true_reward_agent_1_mean': -0.33930142717432316, 'true_reward_agent_1_min': -28.435117281973362, 'true_reward_agent_1_max': 34.42295905947685, 'true_reward_agent_2_mean': 0.24093250811314648, 'true_reward_agent_2_min': -19.711087211966515, 'true_reward_agent_2_max': 29.792298644781113}\n",
      "{'true_reward_agent_0_mean': 5.80017540700171, 'true_reward_agent_0_min': -19.203442361205816, 'true_reward_agent_0_max': 42.25494859693572, 'true_reward_agent_1_mean': -0.8153153751427936, 'true_reward_agent_1_min': -26.491929046809673, 'true_reward_agent_1_max': 33.34745179489255, 'true_reward_agent_2_mean': 1.079653420841593, 'true_reward_agent_2_min': -20.76729342713952, 'true_reward_agent_2_max': 34.86712507158518}\n",
      "{'true_reward_agent_0_mean': 6.8511486717317895, 'true_reward_agent_0_min': -20.143270292319357, 'true_reward_agent_0_max': 40.9011589512229, 'true_reward_agent_1_mean': -0.8708096477256185, 'true_reward_agent_1_min': -29.122826451435685, 'true_reward_agent_1_max': 35.456314735114574, 'true_reward_agent_2_mean': 0.5658338572678258, 'true_reward_agent_2_min': -24.815683523193, 'true_reward_agent_2_max': 36.93499129766133}\n",
      "{'true_reward_agent_0_mean': 8.341465614607696, 'true_reward_agent_0_min': -23.246480422094464, 'true_reward_agent_0_max': 42.67835248634219, 'true_reward_agent_1_mean': -0.9090179757078477, 'true_reward_agent_1_min': -27.13604463636875, 'true_reward_agent_1_max': 34.97517130151391, 'true_reward_agent_2_mean': 1.0533894928409062, 'true_reward_agent_2_min': -19.367104310542345, 'true_reward_agent_2_max': 39.13245551288128}\n",
      "{'true_reward_agent_0_mean': 7.010386774050107, 'true_reward_agent_0_min': -21.405896589159966, 'true_reward_agent_0_max': 45.49092057347298, 'true_reward_agent_1_mean': -0.7880518068656, 'true_reward_agent_1_min': -28.81185609102249, 'true_reward_agent_1_max': 30.498509235680103, 'true_reward_agent_2_mean': 0.6861925426350228, 'true_reward_agent_2_min': -24.78351168334484, 'true_reward_agent_2_max': 33.45574054867029}\n",
      "{'true_reward_agent_0_mean': 7.58961858546827, 'true_reward_agent_0_min': -21.176493518054485, 'true_reward_agent_0_max': 47.37336626648903, 'true_reward_agent_1_mean': -1.969429697619562, 'true_reward_agent_1_min': -31.00334045290947, 'true_reward_agent_1_max': 31.227329439483583, 'true_reward_agent_2_mean': 1.238268805062362, 'true_reward_agent_2_min': -20.164164051413536, 'true_reward_agent_2_max': 33.523706678301096}\n",
      "{'true_reward_agent_0_mean': 7.2253328971187605, 'true_reward_agent_0_min': -22.725696597248316, 'true_reward_agent_0_max': 42.75110822916031, 'true_reward_agent_1_mean': -1.024685053479725, 'true_reward_agent_1_min': -28.857210583984852, 'true_reward_agent_1_max': 32.37803337350488, 'true_reward_agent_2_mean': 1.2604474292862733, 'true_reward_agent_2_min': -19.21334145963192, 'true_reward_agent_2_max': 31.089661672711372}\n",
      "[(7.2253328971187605,), (-1.024685053479725,), (1.2604474292862733,)]\n"
     ]
    }
   ],
   "source": [
    "#Sanity check, this pop should be very close to the true reward function. Expect high fitness\n",
    "pop = [[1, -1], [1, -1], [1, -1]]\n",
    "print(evaluate(pop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 15:04:51,773\tERROR syncer.py:39 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=10476)\u001b[0m /home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\u001b[2m\u001b[36m(pid=10476)\u001b[0m   from ._conv import register_converters as _register_converters\n",
      "\u001b[2m\u001b[36m(pid=10477)\u001b[0m /home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\u001b[2m\u001b[36m(pid=10477)\u001b[0m   from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 15:05:10,393\tINFO trainable.py:180 -- _setup took 18.617 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2020-05-02 15:05:10,394\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-05-02 15:05:10,394\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_reward_agent_0_mean': -0.11662918505806374, 'true_reward_agent_0_min': -15.453765712678432, 'true_reward_agent_0_max': 16.0421684384346, 'true_reward_agent_1_mean': 0.5692989542410942, 'true_reward_agent_1_min': -12.483187314122915, 'true_reward_agent_1_max': 15.38494049012661, 'true_reward_agent_2_mean': 0.14381846922395197, 'true_reward_agent_2_min': -14.652116991579533, 'true_reward_agent_2_max': 19.261050276458263}\n",
      "{'true_reward_agent_0_mean': -0.3484838804654737, 'true_reward_agent_0_min': -13.308767020702362, 'true_reward_agent_0_max': 13.805523544549942, 'true_reward_agent_1_mean': -0.2733594095386252, 'true_reward_agent_1_min': -16.898636043071747, 'true_reward_agent_1_max': 19.101291661150753, 'true_reward_agent_2_mean': 0.3308985035362821, 'true_reward_agent_2_min': -12.60540136974305, 'true_reward_agent_2_max': 16.61903390288353}\n",
      "{'true_reward_agent_0_mean': -1.3687531748475157, 'true_reward_agent_0_min': -15.67809831816703, 'true_reward_agent_0_max': 25.746386766433716, 'true_reward_agent_1_mean': 2.0039341607883396, 'true_reward_agent_1_min': -24.346748907119036, 'true_reward_agent_1_max': 30.889570467174053, 'true_reward_agent_2_mean': 0.818768589401443, 'true_reward_agent_2_min': -15.723249293863773, 'true_reward_agent_2_max': 25.797153562307358}\n",
      "{'true_reward_agent_0_mean': -1.8456471015571878, 'true_reward_agent_0_min': -17.421552017331123, 'true_reward_agent_0_max': 18.943795171711827, 'true_reward_agent_1_mean': 2.8228569095606146, 'true_reward_agent_1_min': -25.561049669981003, 'true_reward_agent_1_max': 29.513853907585144, 'true_reward_agent_2_mean': 2.5237047474394787, 'true_reward_agent_2_min': -19.003349006175995, 'true_reward_agent_2_max': 27.55145718343556}\n",
      "{'true_reward_agent_0_mean': -3.2685979417994986, 'true_reward_agent_0_min': -19.95617637038231, 'true_reward_agent_0_max': 14.745900731999427, 'true_reward_agent_1_mean': 0.28692365868220804, 'true_reward_agent_1_min': -28.236389461904764, 'true_reward_agent_1_max': 36.92440488189459, 'true_reward_agent_2_mean': 4.723847527975576, 'true_reward_agent_2_min': -20.30185583420098, 'true_reward_agent_2_max': 35.23554190248251}\n",
      "{'true_reward_agent_0_mean': -2.9351816295785103, 'true_reward_agent_0_min': -17.194507509469986, 'true_reward_agent_0_max': 13.492220051586628, 'true_reward_agent_1_mean': 0.22542511644627666, 'true_reward_agent_1_min': -29.635378807783127, 'true_reward_agent_1_max': 35.95967482030392, 'true_reward_agent_2_mean': 5.884837075943178, 'true_reward_agent_2_min': -19.532210821285844, 'true_reward_agent_2_max': 32.89941053465009}\n",
      "{'true_reward_agent_0_mean': -3.0187082229939914, 'true_reward_agent_0_min': -19.554207295179367, 'true_reward_agent_0_max': 17.018610760569572, 'true_reward_agent_1_mean': 0.444237019372722, 'true_reward_agent_1_min': -23.47276345267892, 'true_reward_agent_1_max': 34.977957567200065, 'true_reward_agent_2_mean': 5.8311615799385255, 'true_reward_agent_2_min': -18.201723594218493, 'true_reward_agent_2_max': 36.2324134465307}\n",
      "{'true_reward_agent_0_mean': -3.2707290175871004, 'true_reward_agent_0_min': -20.07835692795925, 'true_reward_agent_0_max': 15.9715905636549, 'true_reward_agent_1_mean': 1.2147747273241567, 'true_reward_agent_1_min': -24.862088909372687, 'true_reward_agent_1_max': 31.810226678848267, 'true_reward_agent_2_mean': 5.845506239774209, 'true_reward_agent_2_min': -18.70405089110136, 'true_reward_agent_2_max': 37.93229369819164}\n",
      "{'true_reward_agent_0_mean': -2.9795537176413927, 'true_reward_agent_0_min': -21.056115709245205, 'true_reward_agent_0_max': 16.510937590152025, 'true_reward_agent_1_mean': 0.4051878807070534, 'true_reward_agent_1_min': -27.94858453795314, 'true_reward_agent_1_max': 31.740137808024883, 'true_reward_agent_2_mean': 5.609414333542482, 'true_reward_agent_2_min': -21.43626143410802, 'true_reward_agent_2_max': 30.316789062693715}\n",
      "{'true_reward_agent_0_mean': -3.2071857077605457, 'true_reward_agent_0_min': -16.39958156645298, 'true_reward_agent_0_max': 18.120598673820496, 'true_reward_agent_1_mean': 0.4618732006516075, 'true_reward_agent_1_min': -22.732380136847496, 'true_reward_agent_1_max': 36.47753982618451, 'true_reward_agent_2_mean': 6.271934122993826, 'true_reward_agent_2_min': -14.179044363321736, 'true_reward_agent_2_max': 33.85079237399623}\n",
      "[(-3.2071857077605457,), (0.4618732006516075,), (6.271934122993826,)]\n"
     ]
    }
   ],
   "source": [
    "pop = [[-1, 1], [-1, 1], [-1, 1]]\n",
    "print(evaluate(pop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
