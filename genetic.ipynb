{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sdi2UTA8vWEJ"
   },
   "source": [
    "A proof of concept showing using a genetic algorithm with our environment.\n",
    "It is similar to https://github.com/DEAP/deap/blob/a0b78956e28387785e3bb6e2b4b1f1b32c2b3883/examples/ga/onemax_short.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Qv1kWykfqRLl",
    "outputId": "b9839811-2c7d-4132-f332-990247867c58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'meta-transfer-learning' already exists and is not an empty directory.\n",
      "Collecting ray==0.9.0.dev0\n",
      "  Using cached https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied, skipping upgrade: jsonschema in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: colorama in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (0.4.3)\n",
      "Requirement already satisfied, skipping upgrade: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: msgpack<1.0.0,>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (0.6.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.16 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (1.18.3)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (3.10.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (3.13)\n",
      "Requirement already satisfied, skipping upgrade: redis<3.5.0,>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (3.4.1)\n",
      "Requirement already satisfied, skipping upgrade: grpcio in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (1.28.1)\n",
      "Requirement already satisfied, skipping upgrade: google in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (2.0.3)\n",
      "Requirement already satisfied, skipping upgrade: aiohttp in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (3.6.2)\n",
      "Requirement already satisfied, skipping upgrade: py-spy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->ray==0.9.0.dev0) (46.1.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->ray==0.9.0.dev0) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from google->ray==0.9.0.dev0) (4.6.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (3.6.6)\n",
      "Requirement already satisfied, skipping upgrade: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (1.4.2)\n",
      "Requirement already satisfied, skipping upgrade: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: multidict<5.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (4.7.5)\n",
      "Requirement already satisfied, skipping upgrade: idna>=2.0 in /usr/local/lib/python3.6/dist-packages (from yarl<2.0,>=1.0->aiohttp->ray==0.9.0.dev0) (2.9)\n",
      "Installing collected packages: ray\n",
      "  Found existing installation: ray 0.9.0.dev0\n",
      "    Uninstalling ray-0.9.0.dev0:\n",
      "      Successfully uninstalled ray-0.9.0.dev0\n",
      "Successfully installed ray-0.9.0.dev0\n",
      "Requirement already satisfied: cloudpickle==1.2.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.2.1)\n",
      "Requirement already satisfied: future==0.17.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.17.1)\n",
      "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.17.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.18.3)\n",
      "Requirement already satisfied: Pillow==6.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (6.2.0)\n",
      "Requirement already satisfied: pyglet==1.3.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (1.3.2)\n",
      "Requirement already satisfied: scipy==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: six==1.12.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (1.12.0)\n",
      "Requirement already satisfied: pytest_cases in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (1.13.1)\n",
      "Requirement already satisfied: deap in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: ray[rllib] in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (0.9.0.dev0)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (2.2.0rc4)\n",
      "Requirement already satisfied: decopatch in /usr/local/lib/python3.6/dist-packages (from pytest_cases->-r requirements.txt (line 9)) (1.4.8)\n",
      "Requirement already satisfied: makefun>=1.7 in /usr/local/lib/python3.6/dist-packages (from pytest_cases->-r requirements.txt (line 9)) (1.9.2)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from pytest_cases->-r requirements.txt (line 9)) (1.12.1)\n",
      "Requirement already satisfied: google in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (2.0.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (3.13)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (2.6.0)\n",
      "Requirement already satisfied: msgpack<1.0.0,>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (0.6.2)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (3.10.0)\n",
      "Requirement already satisfied: redis<3.5.0,>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (3.4.1)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (0.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (3.0.12)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (3.6.2)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (7.1.2)\n",
      "Requirement already satisfied: grpcio in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (1.28.1)\n",
      "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (0.3.3)\n",
      "Requirement already satisfied: pandas; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (1.0.3)\n",
      "Requirement already satisfied: tabulate; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (0.8.7)\n",
      "Requirement already satisfied: tensorboardX; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (2.0)\n",
      "Requirement already satisfied: atari-py; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (0.2.6)\n",
      "Requirement already satisfied: dm-tree; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (0.1.5)\n",
      "Requirement already satisfied: opencv-python-headless; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (4.2.0.34)\n",
      "Requirement already satisfied: lz4; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (3.0.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (0.9.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (1.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (0.2.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (2.10.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (0.34.2)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (1.6.3)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (0.3.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (3.2.1)\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (2.2.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (2.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from google->ray[rllib]->-r requirements.txt (line 11)) (4.6.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->ray[rllib]->-r requirements.txt (line 11)) (46.1.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]->-r requirements.txt (line 11)) (3.6.6)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]->-r requirements.txt (line 11)) (3.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]->-r requirements.txt (line 11)) (1.4.2)\n",
      "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]->-r requirements.txt (line 11)) (1.1.0)\n",
      "Requirement already satisfied: multidict<5.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]->-r requirements.txt (line 11)) (4.7.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]->-r requirements.txt (line 11)) (19.3.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]->-r requirements.txt (line 11)) (3.0.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas; extra == \"rllib\"->ray[rllib]->-r requirements.txt (line 11)) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas; extra == \"rllib\"->ray[rllib]->-r requirements.txt (line 11)) (2.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (3.2.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (2.23.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (1.6.0.post3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (1.7.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (0.4.1)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.6/dist-packages (from yarl<2.0,>=1.0->aiohttp->ray[rllib]->-r requirements.txt (line 11)) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (2020.4.5.1)\n",
      "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (3.1.1)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (3.1.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement ran (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for ran\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run this cell if you're using colab. Otherwise, skip it.\n",
    "\n",
    "!git clone https://github.com/platers/meta-transfer-learning.git\n",
    "\n",
    "import os\n",
    "os.chdir('meta-transfer-learning')\n",
    "\n",
    "!pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp36-cp36m-manylinux1_x86_64.whl\n",
    "\n",
    "!pip install -r requirements.txt\n",
    "!pip install ran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ojEPVGZmvTAe"
   },
   "outputs": [],
   "source": [
    "import array\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "\n",
    "from deap import algorithms\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "import gym\n",
    "\n",
    "from environments import SimpleEnv\n",
    "import importlib\n",
    "importlib.reload(SimpleEnv)\n",
    "from environments.SimpleEnv import SimpleEnv #, TODO: add more environments\n",
    "\n",
    "from ray.rllib.agents import ppo\n",
    "from ray import tune\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "import ray\n",
    "\n",
    "import time\n",
    "start = time.process_time()\n",
    "\n",
    "from ray import tune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wvaDdyNmEQ-o"
   },
   "outputs": [],
   "source": [
    "# some settings that we can tweak:\n",
    "config_evolution={\n",
    "        \"MAX_STEP_COUNT\": 1, # number of steps in each round of DEAP evolution \n",
    "        \"POPULATION_SIZE\": 6, # population in each round of DEAP evolution \n",
    "        \"N_GEN\": 6 # number of rounds of DEAP evolution This has the most effect\n",
    "        }\n",
    "N_RL_TRANING = 1 # number of rounds of RL training\n",
    "TRAIN_BATCH_SIZE = 1 #Batch size for RL training\n",
    "LEARNING_RATE = 0.001 #learning rate for RL training\n",
    "EVAL_METHOD = \"OPTIMAL\" # RL or OPTIMAL\n",
    "TUNING_EVOLUTION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "_cnG-FTpEQ-1",
    "outputId": "b67b0b3c-5dc8-4d98-b6e4-b7c801acb818"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-08 12:33:34,868\tINFO resource_spec.py:212 -- Starting Ray with 30.37 GiB memory available for workers and up to 15.19 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-05-08 12:33:35,235\tINFO services.py:1170 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init()\n",
    "n_agents = 3\n",
    "n_var = 2\n",
    "training_envs = [\n",
    "(SimpleEnv, {\n",
    "    'n_agents': n_agents,\n",
    "    'n_vars': n_var,\n",
    "    'true_reward_weights': [1, 0],\n",
    "    'max_step_count': config_evolution[\"MAX_STEP_COUNT\"],\n",
    "}),\n",
    "(SimpleEnv, {\n",
    "    'n_agents': n_agents,\n",
    "    'n_vars': n_var,\n",
    "    'true_reward_weights': [0, 1],\n",
    "    'max_step_count': config_evolution[\"MAX_STEP_COUNT\"],\n",
    "})]\n",
    "test_env = (SimpleEnv, {\n",
    "    'n_agents': n_agents,\n",
    "    'n_vars': n_var,\n",
    "    'true_reward_weights': [1, 1],\n",
    "    'max_step_count': config_evolution[\"MAX_STEP_COUNT\"],\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yn9RzymlEQ_G"
   },
   "outputs": [],
   "source": [
    "# Directly generate \"optimal action\" suggested by the reward function created by Evolution\n",
    "# reward_weights is created from env(config=env_config)\n",
    "def get_optimal_action(reward_weights, env_config):\n",
    "    n_vars = env_config[\"n_vars\"]\n",
    "    max_act = 5\n",
    "    \n",
    "    reward_scale_factor = np.array([1]*n_vars + [2]*(len(reward_weights)-n_vars))\n",
    "    scaled_reward_weights = reward_weights * reward_scale_factor\n",
    "    best_act = np.argmax(scaled_reward_weights)\n",
    "    action = np.eye(len(reward_weights))[best_act] * max_act\n",
    "    action = np.reshape(action, [n_agents, n_vars])\n",
    "    return action \n",
    "\n",
    "# If we do not want to optimize RL but just want to optimize reward function using Evolution function\n",
    "# then here we just do the \"optimal action\" suggested by the generated reward function\n",
    "def evaluate_individual_env_optimal_act(individual, environment_fn, env_config):\n",
    "    env_config['reward_weights'] = np.array([individual for i in range(n_agents)])\n",
    "    env = environment_fn(config=env_config)\n",
    "    \n",
    "    ave_true_rewards = 0\n",
    "    obs = env.reset()\n",
    "    ave_reward = 0\n",
    "    for _ in range(env_config[\"max_step_count\"]):\n",
    "        actions = {i: get_optimal_action(env.reward_weights[i], env_config) for i in range(n_agents)}\n",
    "        obs, reward, _, _ = env.step(actions)\n",
    "        reward = np.array([reward[i] for i in range(len(reward))])\n",
    "        ave_reward += reward\n",
    "        ave_true_rewards += env.last_true_reward\n",
    "        \n",
    "    ave_true_rewards /= env_config[\"max_step_count\"]\n",
    "    ave_reward /= env_config[\"max_step_count\"]\n",
    "    \n",
    "    return np.mean(ave_true_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oP1G3iuuolJi"
   },
   "outputs": [],
   "source": [
    "# train RL agent with given setup\n",
    "def evaluate_individual_env_rl(individual, environment, env_config):\n",
    "    \"\"\"Runs the environment. All agents have the same policy.\n",
    "  It returns the total true reward as the fitness.\n",
    "  \"\"\"\n",
    "    #Select random individuals from pop and create the reward weights\n",
    "    pop = np.array([individual for i in range(n_agents)])\n",
    "    reward_weights = pop\n",
    "    env_config['reward_weights'] = reward_weights\n",
    "    #env is only to get action space and observation space\n",
    "    env = environment(config=env_config)\n",
    "    class MyCallbacks(DefaultCallbacks):\n",
    "        #Callback functions to keep track of true reward while training\n",
    "        def on_episode_start(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                         policies: Dict[str, Policy],\n",
    "                         episode: MultiAgentEpisode, **kwargs):\n",
    "            episode.user_data[\"true_rewards\"] = np.zeros(n_agents)\n",
    "\n",
    "        def on_episode_step(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                        episode: MultiAgentEpisode, **kwargs):\n",
    "            env = base_env\n",
    "            true_reward = env.env_states[0].env.last_true_reward\n",
    "            episode.user_data[\"true_rewards\"] += true_reward\n",
    "\n",
    "        def on_episode_end(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                       policies: Dict[str, Policy], episode: MultiAgentEpisode,\n",
    "                       **kwargs):\n",
    "            true_reward = episode.user_data[\"true_rewards\"]\n",
    "            for i, r in enumerate(true_reward):\n",
    "                episode.custom_metrics[\"true_reward_agent_\" + str(i)] = r\n",
    "    \n",
    "    # settings for the RL agent trainer     \n",
    "    config={\n",
    "        \"train_batch_size\": TRAIN_BATCH_SIZE,\n",
    "        \"lr\": LEARNING_RATE,\n",
    "        \"sgd_minibatch_size\": TRAIN_BATCH_SIZE,\n",
    "        \"multiagent\": {\n",
    "            \"policies\": {\n",
    "            },\n",
    "            \"policy_mapping_fn\":  #all agents share a policy\n",
    "                lambda agent_id:\n",
    "                    'agent'\n",
    "        },\n",
    "        \"model\": {\"fcnet_hiddens\": []},\n",
    "        'env_config': env_config,\n",
    "        \"callbacks\": MyCallbacks,\n",
    "    }\n",
    "    config['multiagent']['policies']['agent'] = (None, env.observation_space, env.action_space, {})\n",
    "    metrics = None\n",
    "    while True:\n",
    "        trainer = ppo.PPOTrainer(env=environment, config=config)\n",
    "        true_reward_mean = 0\n",
    "        for i in range(N_RL_TRANING):\n",
    "            true_reward_mean = 0\n",
    "            #Train the RL agent\n",
    "            metrics = trainer.train()  # distributed training step\n",
    "            print(\"episode_reward_mean\", metrics[\"episode_reward_mean\"])\n",
    "            if metrics[\"episode_reward_mean\"] < 0 and i > 2:\n",
    "                break\n",
    "        #Train agent until it does well\n",
    "        if metrics[\"episode_reward_mean\"] > 0:\n",
    "            break\n",
    "    print(\"episode_reward_mean\", metrics[\"episode_reward_mean\"])\n",
    "    for i in range(n_agents):\n",
    "        true_reward_mean += metrics['custom_metrics']['true_reward_agent_' + str(i) + '_mean']\n",
    "    true_reward_mean /= n_agents\n",
    "    print('Evaluated', individual, 'Fitness', true_reward_mean)\n",
    "    return true_reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dy4idwUWEQ_a"
   },
   "outputs": [],
   "source": [
    "if EVAL_METHOD == \"RL\":\n",
    "    evaluate_individual_env = evaluate_individual_env_rl\n",
    "else:\n",
    "    evaluate_individual_env = evaluate_individual_env_optimal_act\n",
    "        \n",
    "def evaluate_individual(individual):\n",
    "    \"\"\"Runs all environments. \n",
    "  returns the average true reward over all environments as the fitness.\n",
    "  \"\"\"\n",
    "    \n",
    "    all_fitness = []\n",
    "    for env, config in training_envs:\n",
    "        all_fitness.append(evaluate_individual_env(individual, env, config))\n",
    "    fitness_comb = np.min(all_fitness)\n",
    "    return (fitness_comb, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "k1cp_IzAwg5n",
    "outputId": "a1bfe319-afbf-462f-f027-cce5e18d1906"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tavg     \tstd\tmin     \tmax     \n",
      "0  \t6     \t-4.16667\t0  \t-4.16667\t-4.16667\n",
      "1  \t2     \t-4.16667\t0  \t-4.16667\t-4.16667\n",
      "2  \t6     \t-4.16667\t0  \t-4.16667\t-4.16667\n",
      "3  \t2     \t-4.16667\t0  \t-4.16667\t-4.16667\n",
      "4  \t5     \t-4.16667\t0  \t-4.16667\t-4.16667\n",
      "5  \t2     \t-4.16667\t0  \t-4.16667\t-4.16667\n",
      "6  \t2     \t-4.16667\t0  \t-4.16667\t-4.16667\n",
      "Individual('d', [0.9272513701309804, -0.20305490987092356, -0.5952878100209511, -0.5024775839990516, 0.35937025320684635, -0.13233733724895558])\n",
      "---test_reward: -3.333333333333334\n"
     ]
    }
   ],
   "source": [
    "#function that tunes the reward function\n",
    "def train_reward_function(config):\n",
    "  creator.create('FitnessMax', base.Fitness, weights=(1.0, ))\n",
    "  creator.create('Individual', array.array, typecode='d',\n",
    "                fitness=creator.FitnessMax)\n",
    "\n",
    "  toolbox = base.Toolbox()\n",
    "\n",
    "  toolbox.register('attr', random.uniform, -1, 1)\n",
    "  toolbox.register('individual', tools.initRepeat, creator.Individual,\n",
    "                  toolbox.attr, n_agents * n_var)\n",
    "  toolbox.register('population', tools.initRepeat, list,\n",
    "                  toolbox.individual)\n",
    "    # some setup for Deap\n",
    "  toolbox.register('evaluate', evaluate_individual)\n",
    "  toolbox.register('mate', tools.cxTwoPoint)\n",
    "  toolbox.register('mutate', tools.mutFlipBit, indpb=0.05) # add more noise\n",
    "  toolbox.register('select', tools.selTournament, tournsize=3)\n",
    "\n",
    "  # pop is a list individual reward function's weights \n",
    "  pop = toolbox.population(n=config[\"POPULATION_SIZE\"])\n",
    "  # the 10 best individual reward function (could even befround the first round)\n",
    "  hof = tools.HallOfFame(10)\n",
    "\n",
    "  stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "  stats.register('avg', np.mean)\n",
    "  stats.register('std', np.std)\n",
    "  stats.register('min', np.min)\n",
    "  stats.register('max', np.max)\n",
    "\n",
    "  \n",
    "  pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=config[\"N_GEN\"], \n",
    "                                    stats=stats, halloffame=hof, verbose=True)\n",
    "  best_individual = hof[0]\n",
    "  print(best_individual)\n",
    "  test_reward = evaluate_individual_env(best_individual, test_env[0], test_env[1])\n",
    "  print(\"---test_reward:\", test_reward)\n",
    "  if TUNING_EVOLUTION:\n",
    "    tune.track.log(mean_accuracy=test_reward)\n",
    "\n",
    "if TUNING_EVOLUTION:\n",
    "  #configurations\n",
    "  analysis = tune.run(\n",
    "      train_reward_function, config={\n",
    "          \"N_GEN\": tune.grid_search(list(range(1, 20, 5))), \n",
    "          \"POPULATION_SIZE\": tune.grid_search(list(range(1, 20, 5)))\n",
    "          })\n",
    "  #print best config\n",
    "  print(\"Best config: \", analysis.get_best_config(metric=\"mean_accuracy\"))\n",
    "else:\n",
    "  train_reward_function(config_evolution)\n",
    "\n",
    "\n",
    "# print ('pop', pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ztw5G6VWERA6",
    "outputId": "0a24bf3d-97c5-4f03-ef19-f585b3b60ad5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-4.166666666666667,)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_individual([0.9298461960519508, 0.7449587149229808, 0.4628576259710946, -0.282921307700329, -0.7019321146761455, 0.618488821337605]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GS2i0F_qERBb",
    "outputId": "8962cd7e-aa9f-4d33-ce24-115e0d3e8a28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1.6666666666666667,)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_individual([1, 0, -1, 0, -1, 0])) #Worst reward, selfish agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PiWsv5EGERCQ",
    "outputId": "f86e17d0-f612-4e3a-fd87-b9bb60d646b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Spent =  0.006995936300000002  minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Time Spent = \", (time.process_time() - start)/60, \" minutes\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "genetic_with_comments (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
