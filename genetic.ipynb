{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sdi2UTA8vWEJ"
   },
   "source": [
    "A proof of concept showing using a genetic algorithm with our environment.\n",
    "It is similar to https://github.com/DEAP/deap/blob/a0b78956e28387785e3bb6e2b4b1f1b32c2b3883/examples/ga/onemax_short.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Qv1kWykfqRLl",
    "outputId": "eda4b75a-48ff-4852-a019-b40577db74e0"
   },
   "outputs": [],
   "source": [
    "# Run this cell if you're using colab. Otherwise, skip it.\n",
    "\n",
    "!git clone https://github.com/platers/meta-transfer-learning.git\n",
    "\n",
    "import os\n",
    "os.chdir('meta-transfer-learning')\n",
    "\n",
    "!pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp36-cp36m-manylinux1_x86_64.whl\n",
    "\n",
    "!pip install -r requirements.txt\n",
    "!pip install ran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some settings that we can tweak:\n",
    "MAX_STEP_COUNT = 20 # number of steps in each round of DEAP evolution \n",
    "POPULATION_SIZE = 3 # population in each round of DEAP evolution \n",
    "N_GEN = 3 # number of rounds of DEAP evolution\n",
    "N_RL_TRANING = 10 # number of rounds of training for RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ojEPVGZmvTAe"
   },
   "outputs": [],
   "source": [
    "import array\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "\n",
    "from deap import algorithms\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "import gym\n",
    "\n",
    "from environments import SimpleEnv\n",
    "import importlib\n",
    "importlib.reload(SimpleEnv)\n",
    "from environments.SimpleEnv import SimpleEnv #, TODO: add more environments\n",
    "\n",
    "from ray.rllib.agents import ppo\n",
    "from ray import tune\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "import ray\n",
    "\n",
    "import time\n",
    "start = time.process_time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 17:31:08,431\tINFO resource_spec.py:212 -- Starting Ray with 15.28 GiB memory available for workers and up to 7.66 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-05-05 17:31:08,800\tINFO services.py:1170 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "n_agents = 3\n",
    "n_var = 2\n",
    "training_envs = [\n",
    "(SimpleEnv, {\n",
    "    'n_agents': n_agents,\n",
    "    'n_vars': n_var,\n",
    "    'true_reward_weights': [1, 0],\n",
    "    'max_step_count': MAX_STEP_COUNT,\n",
    "}),\n",
    "(SimpleEnv, {\n",
    "    'n_agents': n_agents,\n",
    "    'n_vars': n_var,\n",
    "    'true_reward_weights': [0, 1],\n",
    "    'max_step_count': MAX_STEP_COUNT,\n",
    "})]\n",
    "test_env = (SimpleEnv, {\n",
    "    'n_agents': n_agents,\n",
    "    'n_vars': n_var,\n",
    "    'true_reward_weights': [1, 1],\n",
    "    'max_step_count': MAX_STEP_COUNT,\n",
    "})\n",
    "\n",
    "creator.create('FitnessMax', base.Fitness, weights=(1.0, ))\n",
    "creator.create('Individual', array.array, typecode='d',\n",
    "               fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "toolbox.register('attr', random.uniform, -1, 1)\n",
    "toolbox.register('individual', tools.initRepeat, creator.Individual,\n",
    "                 toolbox.attr, n_agents * n_var)\n",
    "toolbox.register('population', tools.initRepeat, list,\n",
    "                 toolbox.individual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "oP1G3iuuolJi",
    "outputId": "d957997c-cf69-42eb-c90c-91a90fdaa8be"
   },
   "outputs": [],
   "source": [
    "def evaluate_individual_env(individual, environment, env_config):\n",
    "    \"\"\"Runs the environment. All agents have the same policy.\n",
    "  It returns the total true reward as the fitness.\n",
    "  \"\"\"\n",
    "    #Select random individuals from pop and create the reward weights\n",
    "    pop = np.array([individual for i in range(n_agents)])\n",
    "    reward_weights = pop\n",
    "    env_config['reward_weights'] = reward_weights\n",
    "    #env is only to get action space and observation space\n",
    "    env = environment(config=env_config)\n",
    "    class MyCallbacks(DefaultCallbacks):\n",
    "        #Callback functions to keep track of true reward while training\n",
    "        def on_episode_start(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                         policies: Dict[str, Policy],\n",
    "                         episode: MultiAgentEpisode, **kwargs):\n",
    "            episode.user_data[\"true_rewards\"] = np.zeros(n_agents)\n",
    "\n",
    "        def on_episode_step(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                        episode: MultiAgentEpisode, **kwargs):\n",
    "            env = base_env\n",
    "            true_reward = env.env_states[0].env.last_true_reward\n",
    "            episode.user_data[\"true_rewards\"] += true_reward\n",
    "\n",
    "        def on_episode_end(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                       policies: Dict[str, Policy], episode: MultiAgentEpisode,\n",
    "                       **kwargs):\n",
    "            true_reward = episode.user_data[\"true_rewards\"]\n",
    "            for i, r in enumerate(true_reward):\n",
    "                episode.custom_metrics[\"true_reward_agent_\" + str(i)] = r\n",
    "    \n",
    "    # settings for the RL agent trainer     \n",
    "    config={\n",
    "        \"multiagent\": {\n",
    "            \"policies\": {\n",
    "            },\n",
    "            \"policy_mapping_fn\":  #all agents share a policy\n",
    "                lambda agent_id:\n",
    "                    'agent'\n",
    "        },\n",
    "        \"model\": {\"fcnet_hiddens\": []},\n",
    "        'env_config': env_config,\n",
    "        \"callbacks\": MyCallbacks,\n",
    "    }\n",
    "    config['multiagent']['policies']['agent'] = (None, env.observation_space, env.action_space, {})\n",
    "    trainer = ppo.PPOTrainer(env=environment, config=config)\n",
    "    \n",
    "    true_reward_mean = 0\n",
    "    for i in range(10):\n",
    "        #print('TRAINING', i)\n",
    "        true_reward_mean = 0\n",
    "\n",
    "        #Train the RL agent\n",
    "        metrics = trainer.train()  # distributed training step\n",
    "    for i in range(n_agents):\n",
    "        true_reward_mean += metrics['custom_metrics']['true_reward_agent_' + str(i) + '_mean']\n",
    "    true_reward_mean /= n_agents\n",
    "    print('Evaluated', individual, 'Fitness', true_reward_mean)\n",
    "    return true_reward_mean\n",
    "\n",
    "\n",
    "def evaluate_individual(individual):\n",
    "    \"\"\"Runs all environments. \n",
    "  returns the average true reward over all environments as the fitness.\n",
    "  \"\"\"\n",
    "    fitness = 0\n",
    "    for env, config in training_envs:\n",
    "        fitness += evaluate_individual_env(individual, env, config)\n",
    "    return (fitness, )\n",
    "\n",
    "\n",
    "# some setup for Deap\n",
    "toolbox.register('evaluate', evaluate_individual)\n",
    "toolbox.register('mate', tools.cxTwoPoint)\n",
    "toolbox.register('mutate', tools.mutFlipBit, indpb=0.05) # add more noise\n",
    "toolbox.register('select', tools.selTournament, tournsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "k1cp_IzAwg5n",
    "outputId": "fc500615-efde-4668-e94a-685f9ef3df07"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 17:31:09,255\tINFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "2020-05-05 17:31:09,321\tINFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2020-05-05 17:31:13,891\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-05-05 17:31:13,892\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated Individual('d', [-0.5736182553637941, 0.5892023998030322, -0.9903685261991824, 0.5557522206169334, -0.3083585820811703, 0.837132354157242]) Fitness -1.2179618445745535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 17:32:42,523\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-05-05 17:32:42,524\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated Individual('d', [-0.5736182553637941, 0.5892023998030322, -0.9903685261991824, 0.5557522206169334, -0.3083585820811703, 0.837132354157242]) Fitness 5.071448488455359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 17:34:05,985\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-05-05 17:34:05,986\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated Individual('d', [0.5071654074511669, -0.0073635743876669935, 0.42271438734468214, 0.4066623855661502, -0.27119354982949395, 0.4742112743135678]) Fitness 0.4232872362791749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 17:35:34,995\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-05-05 17:35:34,996\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated Individual('d', [0.5071654074511669, -0.0073635743876669935, 0.42271438734468214, 0.4066623855661502, -0.27119354982949395, 0.4742112743135678]) Fitness 0.4257035476535141\n"
     ]
    }
   ],
   "source": [
    "# pop is a list individual reward function's weights \n",
    "pop = toolbox.population(n=POPULATION_SIZE)\n",
    "# the 10 best individual reward function (could even befround the first round)\n",
    "hof = tools.HallOfFame(10)\n",
    "\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register('avg', np.mean)\n",
    "stats.register('std', np.std)\n",
    "stats.register('min', np.min)\n",
    "stats.register('max', np.max)\n",
    "\n",
    "pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=N_GEN, \n",
    "                                   stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "print ('pop', pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_individual = hof[0]\n",
    "print(best_individual)\n",
    "\n",
    "test_reward = evaluate_individual_env(best_individual, test_env[0], test_env[1])\n",
    "print(test_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate_individual([0, 0, 1, 0, 1, 0])) #Ideal reward, altruistic agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hof[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate_individual([1, 0, -1, 0, -1, 0])) #Worst reward, selfish agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Time Spent = \", (time.process_time() - start)/60, \" minutes\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "genetic.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
