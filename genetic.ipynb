{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sdi2UTA8vWEJ"
   },
   "source": [
    "A proof of concept showing using a genetic algorithm with our environment.\n",
    "It is similar to https://github.com/DEAP/deap/blob/a0b78956e28387785e3bb6e2b4b1f1b32c2b3883/examples/ga/onemax_short.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Qv1kWykfqRLl",
    "outputId": "b9839811-2c7d-4132-f332-990247867c58",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell if you're using colab. Otherwise, skip it.\n",
    "\n",
    "!git clone https://github.com/platers/meta-transfer-learning.git\n",
    "\n",
    "import os\n",
    "os.chdir('meta-transfer-learning')\n",
    "\n",
    "!pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp36-cp36m-manylinux1_x86_64.whl\n",
    "\n",
    "!pip install -r requirements.txt\n",
    "!pip install ran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ojEPVGZmvTAe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import array\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "\n",
    "from deap import algorithms\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "import gym\n",
    "\n",
    "from environments import SimpleEnv\n",
    "import importlib\n",
    "from environments import SimpleEnv\n",
    "importlib.reload(SimpleEnv)\n",
    "\n",
    "from ray.rllib.agents import ppo\n",
    "from ray import tune\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "import ray\n",
    "\n",
    "import time\n",
    "start = time.process_time()\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "SimpleEnv = SimpleEnv.SimpleEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wvaDdyNmEQ-o"
   },
   "outputs": [],
   "source": [
    "# some settings that we can tweak:\n",
    "config_evolution={\n",
    "        \"MAX_STEP_COUNT\": 1, # number of steps in each round of DEAP evolution \n",
    "        \"POPULATION_SIZE\": 6, # population in each round of DEAP evolution \n",
    "        \"N_GEN\": 6 # number of rounds of DEAP evolution This has the most effect\n",
    "        }\n",
    "N_RL_TRANING = 5 # number of rounds of RL training\n",
    "TRAIN_BATCH_SIZE = 10 #Batch size for RL training\n",
    "LEARNING_RATE = 0.01 #learning rate for RL training\n",
    "EVAL_METHOD = \"ARGMAX\" # RL or OPTIMAL or ARGMAX\n",
    "TUNING_EVOLUTION = False\n",
    "N_ARGMAX = 300 # number of actions to try for ARGMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "_cnG-FTpEQ-1",
    "outputId": "b67b0b3c-5dc8-4d98-b6e4-b7c801acb818"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-09 22:15:39,649\tINFO resource_spec.py:212 -- Starting Ray with 1.86 GiB memory available for workers and up to 0.93 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-05-09 22:15:39,918\tINFO services.py:1170 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init()\n",
    "n_agents = 3\n",
    "n_var = 2\n",
    "training_envs = [\n",
    "(SimpleEnv, {\n",
    "    'n_agents': n_agents,\n",
    "    'n_vars': n_var,\n",
    "    'true_reward_weights': [1, 0],\n",
    "    'max_step_count': config_evolution[\"MAX_STEP_COUNT\"],\n",
    "}),\n",
    "(SimpleEnv, {\n",
    "    'n_agents': n_agents,\n",
    "    'n_vars': n_var,\n",
    "    'true_reward_weights': [0, 1],\n",
    "    'max_step_count': config_evolution[\"MAX_STEP_COUNT\"],\n",
    "})]\n",
    "test_env = (SimpleEnv, {\n",
    "    'n_agents': n_agents,\n",
    "    'n_vars': n_var,\n",
    "    'true_reward_weights': [1, 1],\n",
    "    'max_step_count': config_evolution[\"MAX_STEP_COUNT\"],\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yn9RzymlEQ_G"
   },
   "outputs": [],
   "source": [
    "# Directly generate \"optimal action\" suggested by the reward function created by Evolution\n",
    "# reward_weights is created from env(config=env_config)\n",
    "def get_optimal_action(reward_weights, env_config):\n",
    "    n_vars = env_config[\"n_vars\"]\n",
    "    max_act = 5    \n",
    "    \n",
    "    reward_scale_factor = np.array([2]*n_vars + [1]*(len(reward_weights)-n_vars))\n",
    "    action = reward_weights * reward_scale_factor\n",
    "    action = action / np.max(action) * max_act\n",
    "    \n",
    "    action = np.reshape(action, [n_agents, n_vars])\n",
    "    return action\n",
    "\n",
    "# If we do not want to optimize RL but just want to optimize reward function using Evolution function\n",
    "# then here we just do the \"optimal action\" suggested by the generated reward function\n",
    "def evaluate_individual_env_optimal_act(individual, environment_fn, env_config):\n",
    "    env_config['reward_weights'] = np.array([individual for i in range(n_agents)])\n",
    "    env = environment_fn(config=env_config)\n",
    "    \n",
    "    ave_true_rewards = 0\n",
    "    obs = env.reset()\n",
    "    ave_reward = 0\n",
    "    for _ in range(env_config[\"max_step_count\"]):\n",
    "        actions = {i: get_optimal_action(env.reward_weights[i], env_config) for i in range(n_agents)}\n",
    "        obs, reward, _, _ = env.step(actions)\n",
    "        reward = np.array([reward[i] for i in range(len(reward))])\n",
    "        ave_reward += reward\n",
    "        ave_true_rewards += env.last_true_reward\n",
    "        \n",
    "    ave_true_rewards /= env_config[\"max_step_count\"]\n",
    "    ave_reward /= env_config[\"max_step_count\"]\n",
    "    \n",
    "    return np.mean(ave_true_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directly generate \"optimal action\" suggested by the reward function created by Evolution\n",
    "# reward_weights is created from env(config=env_config)\n",
    "def get_argmax_action(reward_weights, env_config):\n",
    "    best_action = None\n",
    "    best_reward = -np.inf\n",
    "    env = SimpleEnv(config=env_config)\n",
    "    for i in range(N_ARGMAX):\n",
    "        env.reset()\n",
    "        action = np.array(env.action_space.sample())\n",
    "        actions = {i: action for i in range(n_agents)}\n",
    "        observation, reward, done, info = env.step(actions)\n",
    "        reward = np.mean(np.array([reward[i] for i in range(len(reward))]))\n",
    "        if reward > best_reward:\n",
    "            best_reward = reward\n",
    "            best_action = action\n",
    "    return best_action\n",
    "\n",
    "# If we do not want to optimize RL but just want to optimize reward function using Evolution function\n",
    "# then here we just do the \"optimal action\" suggested by the generated reward function\n",
    "def evaluate_individual_env_argmax_act(individual, environment_fn, env_config):\n",
    "    env_config['reward_weights'] = np.array([individual for i in range(n_agents)])\n",
    "    env = environment_fn(config=env_config)\n",
    "    \n",
    "    ave_true_rewards = 0\n",
    "    obs = env.reset()\n",
    "    ave_reward = 0\n",
    "    for _ in range(env_config[\"max_step_count\"]):\n",
    "        actions = {i: get_argmax_action(env.reward_weights[i], env_config) for i in range(n_agents)}\n",
    "        obs, reward, _, _ = env.step(actions)\n",
    "        reward = np.array([reward[i] for i in range(len(reward))])\n",
    "        ave_reward += reward\n",
    "        ave_true_rewards += env.last_true_reward\n",
    "        \n",
    "    ave_true_rewards /= env_config[\"max_step_count\"]\n",
    "    ave_reward /= env_config[\"max_step_count\"]\n",
    "    \n",
    "    return np.mean(ave_true_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oP1G3iuuolJi"
   },
   "outputs": [],
   "source": [
    "# train RL agent with given setup\n",
    "def evaluate_individual_env_rl(individual, environment, env_config):\n",
    "    \"\"\"Runs the environment. All agents have the same policy.\n",
    "  It returns the total true reward as the fitness.\n",
    "  \"\"\"\n",
    "    #Select random individuals from pop and create the reward weights\n",
    "    pop = np.array([individual for i in range(n_agents)])\n",
    "    reward_weights = pop\n",
    "    env_config['reward_weights'] = reward_weights\n",
    "    #env is only to get action space and observation space\n",
    "    env = environment(config=env_config)\n",
    "    class MyCallbacks(DefaultCallbacks):\n",
    "        #Callback functions to keep track of true reward while training\n",
    "        def on_episode_start(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                         policies: Dict[str, Policy],\n",
    "                         episode: MultiAgentEpisode, **kwargs):\n",
    "            episode.user_data[\"true_rewards\"] = np.zeros(n_agents)\n",
    "\n",
    "        def on_episode_step(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                        episode: MultiAgentEpisode, **kwargs):\n",
    "            env = base_env\n",
    "            true_reward = env.env_states[0].env.last_true_reward\n",
    "            episode.user_data[\"true_rewards\"] += true_reward\n",
    "\n",
    "        def on_episode_end(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                       policies: Dict[str, Policy], episode: MultiAgentEpisode,\n",
    "                       **kwargs):\n",
    "            true_reward = episode.user_data[\"true_rewards\"]\n",
    "            for i, r in enumerate(true_reward):\n",
    "                episode.custom_metrics[\"true_reward_agent_\" + str(i)] = r\n",
    "    \n",
    "    # settings for the RL agent trainer     \n",
    "    config={\n",
    "        \"train_batch_size\": TRAIN_BATCH_SIZE,\n",
    "        \"lr\": LEARNING_RATE,\n",
    "        \"sgd_minibatch_size\": TRAIN_BATCH_SIZE,\n",
    "        \"multiagent\": {\n",
    "            \"policies\": {\n",
    "            },\n",
    "            \"policy_mapping_fn\":  #all agents share a policy\n",
    "                lambda agent_id:\n",
    "                    'agent'\n",
    "        },\n",
    "        \"model\": {\"fcnet_hiddens\": []},\n",
    "        'env_config': env_config,\n",
    "        \"callbacks\": MyCallbacks,\n",
    "    }\n",
    "    config['multiagent']['policies']['agent'] = (None, env.observation_space, env.action_space, {})\n",
    "    metrics = None\n",
    "    while True:\n",
    "        trainer = ppo.PPOTrainer(env=environment, config=config)\n",
    "        true_reward_mean = 0\n",
    "        for i in range(N_RL_TRANING):\n",
    "            true_reward_mean = 0\n",
    "            #Train the RL agent\n",
    "            metrics = trainer.train()  # distributed training step\n",
    "            print(\"episode_reward_mean\", metrics[\"episode_reward_mean\"])\n",
    "            if metrics[\"episode_reward_mean\"] < 0 and i > 2:\n",
    "                break\n",
    "        #Train agent until it does well\n",
    "        if metrics[\"episode_reward_mean\"] > 0:\n",
    "            break\n",
    "    print(\"episode_reward_mean\", metrics[\"episode_reward_mean\"])\n",
    "    for i in range(n_agents):\n",
    "        true_reward_mean += metrics['custom_metrics']['true_reward_agent_' + str(i) + '_mean']\n",
    "    true_reward_mean /= n_agents\n",
    "    print('Evaluated', individual, 'Fitness', true_reward_mean)\n",
    "    return true_reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dy4idwUWEQ_a"
   },
   "outputs": [],
   "source": [
    "if EVAL_METHOD == \"RL\":\n",
    "    evaluate_individual_env = evaluate_individual_env_rl\n",
    "elif EVAL_METHOD == \"OPTIMAL\":\n",
    "    evaluate_individual_env = evaluate_individual_env_optimal_act\n",
    "elif EVAL_METHOD == \"ARGMAX\":\n",
    "    evaluate_individual_env = evaluate_individual_env_argmax_act\n",
    "        \n",
    "def evaluate_individual(individual):\n",
    "    \"\"\"Runs all environments. \n",
    "  returns the average true reward over all environments as the fitness.\n",
    "  \"\"\"\n",
    "    \n",
    "    all_fitness = []\n",
    "    for env, config in training_envs:\n",
    "        all_fitness.append(evaluate_individual_env(individual, env, config))\n",
    "    fitness_comb = np.min(all_fitness)\n",
    "    return (fitness_comb, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "k1cp_IzAwg5n",
    "outputId": "a1bfe319-afbf-462f-f027-cce5e18d1906"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/anaconda3/lib/python3.6/site-packages/deap/creator.py:141: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  RuntimeWarning)\n",
      "/home/victor/anaconda3/lib/python3.6/site-packages/deap/creator.py:141: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tavg     \tstd    \tmin     \tmax     \n",
      "0  \t6     \t-1.28722\t2.67418\t-7.16842\t0.754622\n",
      "1  \t3     \t0.0438486\t0.511101\t-0.689134\t0.927567\n",
      "2  \t2     \t-1.15788 \t2.25052 \t-5.08846 \t0.927567\n",
      "3  \t1     \t0.783162 \t0.684505\t0.232356 \t2.14677 \n",
      "4  \t4     \t1.49143  \t0.708666\t0.232356 \t2.21521 \n",
      "5  \t4     \t1.39931  \t0.996041\t-0.667267\t2.21521 \n",
      "6  \t4     \t1.50527  \t0.94811 \t0.327877 \t3.16518 \n",
      "array('d', [0.0, 1.0, 1.0, 0.0, 1.0, 1.0])\n",
      "---test_reward: 2.204818546772003\n"
     ]
    }
   ],
   "source": [
    "#function that tunes the reward function\n",
    "def train_reward_function(config):\n",
    "  creator.create('FitnessMax', base.Fitness, weights=(1.0, ))\n",
    "  creator.create('Individual', array.array, typecode='d',\n",
    "                fitness=creator.FitnessMax)\n",
    "\n",
    "  toolbox = base.Toolbox()\n",
    "\n",
    "  toolbox.register('attr', random.randint, 0, 1)\n",
    "  toolbox.register('individual', tools.initRepeat, creator.Individual,\n",
    "                  toolbox.attr, n_agents * n_var)\n",
    "  toolbox.register('population', tools.initRepeat, list,\n",
    "                  toolbox.individual)\n",
    "    # some setup for Deap\n",
    "  toolbox.register('evaluate', evaluate_individual)\n",
    "  toolbox.register('mate', tools.cxTwoPoint)\n",
    "  toolbox.register('mutate', tools.mutFlipBit, indpb=0.05) # add more noise\n",
    "  toolbox.register('select', tools.selTournament, tournsize=3)\n",
    "\n",
    "  # pop is a list individual reward function's weights \n",
    "  pop = toolbox.population(n=config[\"POPULATION_SIZE\"])\n",
    "  # the 10 best individual reward function (could even befround the first round)\n",
    "  hof = tools.HallOfFame(10)\n",
    "\n",
    "  stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "  stats.register('avg', np.mean)\n",
    "  stats.register('std', np.std)\n",
    "  stats.register('min', np.min)\n",
    "  stats.register('max', np.max)\n",
    "\n",
    "  \n",
    "  pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=config[\"N_GEN\"], \n",
    "                                    stats=stats, halloffame=hof, verbose=True)\n",
    "  best_individual = hof[0]\n",
    "  print(best_individual)\n",
    "  test_reward = evaluate_individual_env(best_individual, test_env[0], test_env[1])\n",
    "  print(\"---test_reward:\", test_reward)\n",
    "  if TUNING_EVOLUTION:\n",
    "    tune.track.log(mean_accuracy=test_reward)\n",
    "\n",
    "if TUNING_EVOLUTION:\n",
    "  #configurations\n",
    "  analysis = tune.run(\n",
    "      train_reward_function, config={\n",
    "          \"N_GEN\": tune.grid_search(list(range(1, 20, 5))), \n",
    "          \"POPULATION_SIZE\": tune.grid_search(list(range(1, 20, 5)))\n",
    "          })\n",
    "  #print best config\n",
    "  print(\"Best config: \", analysis.get_best_config(metric=\"mean_accuracy\"))\n",
    "else:\n",
    "  train_reward_function(config_evolution)\n",
    "\n",
    "\n",
    "# print ('pop', pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ztw5G6VWERA6",
    "outputId": "0a24bf3d-97c5-4f03-ef19-f585b3b60ad5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fits = []\n",
    "for i in range(5):\n",
    "    f = evaluate_individual([0, 0, 1, 1, 1, 1]) #Ideal reward, altruistic agent\n",
    "    fits.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GS2i0F_qERBb",
    "outputId": "8962cd7e-aa9f-4d33-ce24-115e0d3e8a28"
   },
   "outputs": [],
   "source": [
    "print(evaluate_individual([1, 0, -1, 0, -1, 0])) #Worst reward, selfish agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PiWsv5EGERCQ",
    "outputId": "f86e17d0-f612-4e3a-fd87-b9bb60d646b4"
   },
   "outputs": [],
   "source": [
    "print(\"Time Spent = \", (time.process_time() - start)/60, \" minutes\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "genetic_with_comments (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
