{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "2020-05-02 11:52:17,696\tINFO resource_spec.py:212 -- Starting Ray with 2.93 GiB memory available for workers and up to 1.48 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-05-02 11:52:18,193\tINFO services.py:1170 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n",
      "2020-05-02 11:52:18,421\tINFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "2020-05-02 11:52:18,506\tERROR syncer.py:39 -- Log sync requires rsync to be installed.\n",
      "2020-05-02 11:52:18,538\tINFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m /home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m   from ._conv import register_converters as _register_converters\n",
      "\u001b[2m\u001b[36m(pid=6489)\u001b[0m /home/victor/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\u001b[2m\u001b[36m(pid=6489)\u001b[0m   from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-02 11:52:51,461\tINFO trainable.py:180 -- _setup took 32.951 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2020-05-02 11:52:51,463\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-05-02 11:52:51,464\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_reward_agent_0_mean': -0.35772826832726423, 'true_reward_agent_0_min': -12.6492231041193, 'true_reward_agent_0_max': 14.459690429270267, 'true_reward_agent_1_mean': -0.572670808238472, 'true_reward_agent_1_min': -13.799482926726341, 'true_reward_agent_1_max': 15.379848877433687, 'true_reward_agent_2_mean': 0.5315363473408797, 'true_reward_agent_2_min': -12.92052212357521, 'true_reward_agent_2_max': 14.712818875908852}\n",
      "{'true_reward_agent_0_mean': 0.45329160315988704, 'true_reward_agent_0_min': -15.438871443271637, 'true_reward_agent_0_max': 18.665814973413944, 'true_reward_agent_1_mean': 2.8635704830906614, 'true_reward_agent_1_min': -16.05509041249752, 'true_reward_agent_1_max': 24.673287319019437, 'true_reward_agent_2_mean': 0.470979436926682, 'true_reward_agent_2_min': -15.158215917646885, 'true_reward_agent_2_max': 16.14712850563228}\n"
     ]
    }
   ],
   "source": [
    "# A proof of concept showing using a genetic algorithm with our environment.\n",
    "# It is similar to https://github.com/DEAP/deap/blob/a0b78956e28387785e3bb6e2b4b1f1b32c2b3883/examples/ga/onemax_short.py\n",
    "\n",
    "import array\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "\n",
    "from deap import algorithms\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "import gym\n",
    "from environments.SimpleEnv import SimpleEnv\n",
    "\n",
    "from ray.rllib.agents import ppo\n",
    "from ray import tune\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "import ray\n",
    "\n",
    "ray.init()\n",
    "\n",
    "\n",
    "creator.create('FitnessMax', base.Fitness, weights=(1.0, ))\n",
    "creator.create('Individual', array.array, typecode='d',\n",
    "               fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "toolbox.register('attr', random.uniform, -1, 1)\n",
    "toolbox.register('individual', tools.initRepeat, creator.Individual,\n",
    "                 toolbox.attr, 2)\n",
    "toolbox.register('population', tools.initRepeat, list,\n",
    "                 toolbox.individual)\n",
    "\n",
    "n_agents = 3\n",
    "\n",
    "def evaluate(pop):\n",
    "    \"\"\"Runs the environment. Selects random agents from pop.\n",
    "  It returns the total true reward as the fitness.\n",
    "  \"\"\"\n",
    "    \n",
    "    #Select random individuals from pop and create the reward weights\n",
    "    pop = np.array(pop)\n",
    "    reward_weights = pop\n",
    "    #print(individual, reward_weights)\n",
    "    \n",
    "    #env is only to get action space and observation space\n",
    "    env = SimpleEnv(config={\n",
    "        'n_agents': n_agents,\n",
    "        'n_vars': 2,\n",
    "        'reward_weights': reward_weights,\n",
    "        'max_step_count': 20,\n",
    "    })\n",
    "    class MyCallbacks(DefaultCallbacks):\n",
    "        #Callback functions to keep track of true reward while training\n",
    "        def on_episode_start(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                         policies: Dict[str, Policy],\n",
    "                         episode: MultiAgentEpisode, **kwargs):\n",
    "            episode.user_data[\"true_rewards\"] = np.zeros(n_agents)\n",
    "\n",
    "        def on_episode_step(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                        episode: MultiAgentEpisode, **kwargs):\n",
    "            env = base_env\n",
    "            #print(env.env_states[0].env.last_true_reward)\n",
    "            true_reward = env.env_states[0].env.last_true_reward\n",
    "            episode.user_data[\"true_rewards\"] += true_reward\n",
    "\n",
    "        def on_episode_end(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                       policies: Dict[str, Policy], episode: MultiAgentEpisode,\n",
    "                       **kwargs):\n",
    "            true_reward = episode.user_data[\"true_rewards\"]\n",
    "            for i, r in enumerate(true_reward):\n",
    "                episode.custom_metrics[\"true_reward_agent_\" + str(i)] = r\n",
    "            \n",
    "    config={\n",
    "        \"multiagent\": {\n",
    "            \"policies\": {\n",
    "            },\n",
    "            \"policy_mapping_fn\":\n",
    "                lambda agent_id:\n",
    "                    agent_id\n",
    "        },\n",
    "        'env_config': {\n",
    "            'n_agents': n_agents,\n",
    "            'n_vars': 2,\n",
    "            'reward_weights': reward_weights,\n",
    "            'max_step_count': 20,\n",
    "        },\n",
    "        \"callbacks\": MyCallbacks,\n",
    "    }\n",
    "    for i in range(n_agents):\n",
    "        config['multiagent']['policies']['agent_' + str(i)] = (None, env.observation_space, env.action_space, {})\n",
    "    trainer = ppo.PPOTrainer(env=SimpleEnv, config=config)\n",
    "    \n",
    "    true_reward_mean = []\n",
    "    for i in range(10):\n",
    "        #print('TRAINING', i)\n",
    "        custom_metrics = trainer.train()['custom_metrics']  # distributed training step\n",
    "        print(custom_metrics)\n",
    "        for i in range(n_agents):\n",
    "            true_reward_mean.append((custom_metrics['true_reward_agent_' + str(i) + '_mean'], ))\n",
    "        \n",
    "    #print('true reward', trainer.collect_metrics()['custom_metrics']['true_reward_mean'])\n",
    "    return true_reward_mean\n",
    "\n",
    "\n",
    "toolbox.register('evaluate', evaluate)\n",
    "toolbox.register('mate', tools.cxTwoPoint)\n",
    "toolbox.register('mutate', tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register('select', tools.selTournament, tournsize=3)\n",
    "\n",
    "def evolve(population, toolbox, cxpb, mutpb, ngen, stats=None, \n",
    "           halloffame=None, verbose=__debug__):\n",
    "    \"\"\"\n",
    "    Almost identical to deap.algorithms.eaSimple. \n",
    "    \n",
    "    TODO: Sometimes all individuals become identical for some reason\n",
    "    Runtime is O(pop size * ngen * RL iterations) with a huge constant. \n",
    "    Easily parralizable, but we can remove the pop size factor by training the\n",
    "    entire population at once. Requires environments to support many agents.\n",
    "    \"\"\"\n",
    "    \n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population]\n",
    "    #print('population1', population)\n",
    "    fitnesses = evaluate(invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    #print('population2', population)\n",
    "    if halloffame is not None:\n",
    "        halloffame.update(population)\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "        # Select the next generation individuals\n",
    "        print('population', population)\n",
    "        offspring = toolbox.select(population, len(population))\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = algorithms.varAnd(offspring, toolbox, cxpb, mutpb)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring]\n",
    "        fitnesses = evaluate(invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        if halloffame is not None:\n",
    "            halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook\n",
    "\n",
    "def main():\n",
    "    pop = toolbox.population(n=n_agents)\n",
    "    hof = tools.HallOfFame(10)\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register('avg', np.mean)\n",
    "    stats.register('std', np.std)\n",
    "    stats.register('min', np.min)\n",
    "    stats.register('max', np.max)\n",
    "\n",
    "    (pop, log) = evolve(\n",
    "        pop,\n",
    "        toolbox,\n",
    "        cxpb=0.5,\n",
    "        mutpb=0.2,\n",
    "        ngen=3,\n",
    "        stats=stats,\n",
    "        halloffame=hof,\n",
    "        verbose=True,\n",
    "        )\n",
    "\n",
    "    print ('pop', pop)\n",
    "\n",
    "    return (pop, log, hof)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I had problems with rllib running out of memory after running many tests. Installing the ray nightly build fixed it\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
