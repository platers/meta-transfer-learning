{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "genetic_newest+RL_tunning+print-whats-inside-tuning+add_all_imports.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Sdi2UTA8vWEJ"
      },
      "source": [
        "A proof of concept showing using a genetic algorithm with our environment.\n",
        "It is similar to https://github.com/DEAP/deap/blob/a0b78956e28387785e3bb6e2b4b1f1b32c2b3883/examples/ga/onemax_short.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qv1kWykfqRLl",
        "outputId": "6feb1d33-78c9-416c-93d4-f3c88e6716b7",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Run this cell if you're using colab. Otherwise, skip it.\n",
        "\n",
        "!git clone https://github.com/platers/meta-transfer-learning.git\n",
        "\n",
        "import os\n",
        "os.chdir('meta-transfer-learning')\n",
        "\n",
        "!pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp36-cp36m-manylinux1_x86_64.whl\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "!pip install ran\n",
        "\n",
        "\n",
        "# Make Colab run faster. make sure that the current GPU memory utilization is 0\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'meta-transfer-learning'...\n",
            "remote: Enumerating objects: 274, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/274)\u001b[K\rremote: Counting objects:   1% (3/274)\u001b[K\rremote: Counting objects:   2% (6/274)\u001b[K\rremote: Counting objects:   3% (9/274)\u001b[K\rremote: Counting objects:   4% (11/274)\u001b[K\rremote: Counting objects:   5% (14/274)\u001b[K\rremote: Counting objects:   6% (17/274)\u001b[K\rremote: Counting objects:   7% (20/274)\u001b[K\rremote: Counting objects:   8% (22/274)\u001b[K\rremote: Counting objects:   9% (25/274)\u001b[K\rremote: Counting objects:  10% (28/274)\u001b[K\rremote: Counting objects:  11% (31/274)\u001b[K\rremote: Counting objects:  12% (33/274)\u001b[K\rremote: Counting objects:  13% (36/274)\u001b[K\rremote: Counting objects:  14% (39/274)\u001b[K\rremote: Counting objects:  15% (42/274)\u001b[K\rremote: Counting objects:  16% (44/274)\u001b[K\rremote: Counting objects:  17% (47/274)\u001b[K\rremote: Counting objects:  18% (50/274)\u001b[K\rremote: Counting objects:  19% (53/274)\u001b[K\rremote: Counting objects:  20% (55/274)\u001b[K\rremote: Counting objects:  21% (58/274)\u001b[K\rremote: Counting objects:  22% (61/274)\u001b[K\rremote: Counting objects:  23% (64/274)\u001b[K\rremote: Counting objects:  24% (66/274)\u001b[K\rremote: Counting objects:  25% (69/274)\u001b[K\rremote: Counting objects:  26% (72/274)\u001b[K\rremote: Counting objects:  27% (74/274)\u001b[K\rremote: Counting objects:  28% (77/274)\u001b[K\rremote: Counting objects:  29% (80/274)\u001b[K\rremote: Counting objects:  30% (83/274)\u001b[K\rremote: Counting objects:  31% (85/274)\u001b[K\rremote: Counting objects:  32% (88/274)\u001b[K\rremote: Counting objects:  33% (91/274)\u001b[K\rremote: Counting objects:  34% (94/274)\u001b[K\rremote: Counting objects:  35% (96/274)\u001b[K\rremote: Counting objects:  36% (99/274)\u001b[K\rremote: Counting objects:  37% (102/274)\u001b[K\rremote: Counting objects:  38% (105/274)\u001b[K\rremote: Counting objects:  39% (107/274)\u001b[K\rremote: Counting objects:  40% (110/274)\u001b[K\rremote: Counting objects:  41% (113/274)\u001b[K\rremote: Counting objects:  42% (116/274)\u001b[K\rremote: Counting objects:  43% (118/274)\u001b[K\rremote: Counting objects:  44% (121/274)\u001b[K\rremote: Counting objects:  45% (124/274)\u001b[K\rremote: Counting objects:  46% (127/274)\u001b[K\rremote: Counting objects:  47% (129/274)\u001b[K\rremote: Counting objects:  48% (132/274)\u001b[K\rremote: Counting objects:  49% (135/274)\u001b[K\rremote: Counting objects:  50% (137/274)\u001b[K\rremote: Counting objects:  51% (140/274)\u001b[K\rremote: Counting objects:  52% (143/274)\u001b[K\rremote: Counting objects:  53% (146/274)\u001b[K\rremote: Counting objects:  54% (148/274)\u001b[K\rremote: Counting objects:  55% (151/274)\u001b[K\rremote: Counting objects:  56% (154/274)\u001b[K\rremote: Counting objects:  57% (157/274)\u001b[K\rremote: Counting objects:  58% (159/274)\u001b[K\rremote: Counting objects:  59% (162/274)\u001b[K\rremote: Counting objects:  60% (165/274)\u001b[K\rremote: Counting objects:  61% (168/274)\u001b[K\rremote: Counting objects:  62% (170/274)\u001b[K\rremote: Counting objects:  63% (173/274)\u001b[K\rremote: Counting objects:  64% (176/274)\u001b[K\rremote: Counting objects:  65% (179/274)\u001b[K\rremote: Counting objects:  66% (181/274)\u001b[K\rremote: Counting objects:  67% (184/274)\u001b[K\rremote: Counting objects:  68% (187/274)\u001b[K\rremote: Counting objects:  69% (190/274)\u001b[K\rremote: Counting objects:  70% (192/274)\u001b[K\rremote: Counting objects:  71% (195/274)\u001b[K\rremote: Counting objects:  72% (198/274)\u001b[K\rremote: Counting objects:  73% (201/274)\u001b[K\rremote: Counting objects:  74% (203/274)\u001b[K\rremote: Counting objects:  75% (206/274)\u001b[K\rremote: Counting objects:  76% (209/274)\u001b[K\rremote: Counting objects:  77% (211/274)\u001b[K\rremote: Counting objects:  78% (214/274)\u001b[K\rremote: Counting objects:  79% (217/274)\u001b[K\rremote: Counting objects:  80% (220/274)\u001b[K\rremote: Counting objects:  81% (222/274)\u001b[K\rremote: Counting objects:  82% (225/274)\u001b[K\rremote: Counting objects:  83% (228/274)\u001b[K\rremote: Counting objects:  84% (231/274)\u001b[K\rremote: Counting objects:  85% (233/274)\u001b[K\rremote: Counting objects:  86% (236/274)\u001b[K\rremote: Counting objects:  87% (239/274)\u001b[K\rremote: Counting objects:  88% (242/274)\u001b[K\rremote: Counting objects:  89% (244/274)\u001b[K\rremote: Counting objects:  90% (247/274)\u001b[K\rremote: Counting objects:  91% (250/274)\u001b[K\rremote: Counting objects:  92% (253/274)\u001b[K\rremote: Counting objects:  93% (255/274)\u001b[K\rremote: Counting objects:  94% (258/274)\u001b[K\rremote: Counting objects:  95% (261/274)\u001b[K\rremote: Counting objects:  96% (264/274)\u001b[K\rremote: Counting objects:  97% (266/274)\u001b[K\rremote: Counting objects:  98% (269/274)\u001b[K\rremote: Counting objects:  99% (272/274)\u001b[K\rremote: Counting objects: 100% (274/274)\u001b[K\rremote: Counting objects: 100% (274/274), done.\u001b[K\n",
            "remote: Compressing objects:   0% (1/179)\u001b[K\rremote: Compressing objects:   1% (2/179)\u001b[K\rremote: Compressing objects:   2% (4/179)\u001b[K\rremote: Compressing objects:   3% (6/179)\u001b[K\rremote: Compressing objects:   4% (8/179)\u001b[K\rremote: Compressing objects:   5% (9/179)\u001b[K\rremote: Compressing objects:   6% (11/179)\u001b[K\rremote: Compressing objects:   7% (13/179)\u001b[K\rremote: Compressing objects:   8% (15/179)\u001b[K\rremote: Compressing objects:   9% (17/179)\u001b[K\rremote: Compressing objects:  10% (18/179)\u001b[K\rremote: Compressing objects:  11% (20/179)\u001b[K\rremote: Compressing objects:  12% (22/179)\u001b[K\rremote: Compressing objects:  13% (24/179)\u001b[K\rremote: Compressing objects:  14% (26/179)\u001b[K\rremote: Compressing objects:  15% (27/179)\u001b[K\rremote: Compressing objects:  16% (29/179)\u001b[K\rremote: Compressing objects:  17% (31/179)\u001b[K\rremote: Compressing objects:  18% (33/179)\u001b[K\rremote: Compressing objects:  19% (35/179)\u001b[K\rremote: Compressing objects:  20% (36/179)\u001b[K\rremote: Compressing objects:  21% (38/179)\u001b[K\rremote: Compressing objects:  22% (40/179)\u001b[K\rremote: Compressing objects:  23% (42/179)\u001b[K\rremote: Compressing objects:  24% (43/179)\u001b[K\rremote: Compressing objects:  25% (45/179)\u001b[K\rremote: Compressing objects:  26% (47/179)\u001b[K\rremote: Compressing objects:  27% (49/179)\u001b[K\rremote: Compressing objects:  28% (51/179)\u001b[K\rremote: Compressing objects:  29% (52/179)\u001b[K\rremote: Compressing objects:  30% (54/179)\u001b[K\rremote: Compressing objects:  31% (56/179)\u001b[K\rremote: Compressing objects:  32% (58/179)\u001b[K\rremote: Compressing objects:  33% (60/179)\u001b[K\rremote: Compressing objects:  34% (61/179)\u001b[K\rremote: Compressing objects:  35% (63/179)\u001b[K\rremote: Compressing objects:  36% (65/179)\u001b[K\rremote: Compressing objects:  37% (67/179)\u001b[K\rremote: Compressing objects:  38% (69/179)\u001b[K\rremote: Compressing objects:  39% (70/179)\u001b[K\rremote: Compressing objects:  40% (72/179)\u001b[K\rremote: Compressing objects:  41% (74/179)\u001b[K\rremote: Compressing objects:  42% (76/179)\u001b[K\rremote: Compressing objects:  43% (77/179)\u001b[K\rremote: Compressing objects:  44% (79/179)\u001b[K\rremote: Compressing objects:  45% (81/179)\u001b[K\rremote: Compressing objects:  46% (83/179)\u001b[K\rremote: Compressing objects:  47% (85/179)\u001b[K\rremote: Compressing objects:  48% (86/179)\u001b[K\rremote: Compressing objects:  49% (88/179)\u001b[K\rremote: Compressing objects:  50% (90/179)\u001b[K\rremote: Compressing objects:  51% (92/179)\u001b[K\rremote: Compressing objects:  52% (94/179)\u001b[K\rremote: Compressing objects:  53% (95/179)\u001b[K\rremote: Compressing objects:  54% (97/179)\u001b[K\rremote: Compressing objects:  55% (99/179)\u001b[K\rremote: Compressing objects:  56% (101/179)\u001b[K\rremote: Compressing objects:  57% (103/179)\u001b[K\rremote: Compressing objects:  58% (104/179)\u001b[K\rremote: Compressing objects:  59% (106/179)\u001b[K\rremote: Compressing objects:  60% (108/179)\u001b[K\rremote: Compressing objects:  61% (110/179)\u001b[K\rremote: Compressing objects:  62% (111/179)\u001b[K\rremote: Compressing objects:  63% (113/179)\u001b[K\rremote: Compressing objects:  64% (115/179)\u001b[K\rremote: Compressing objects:  65% (117/179)\u001b[K\rremote: Compressing objects:  66% (119/179)\u001b[K\rremote: Compressing objects:  67% (120/179)\u001b[K\rremote: Compressing objects:  68% (122/179)\u001b[K\rremote: Compressing objects:  69% (124/179)\u001b[K\rremote: Compressing objects:  70% (126/179)\u001b[K\rremote: Compressing objects:  71% (128/179)\u001b[K\rremote: Compressing objects:  72% (129/179)\u001b[K\rremote: Compressing objects:  73% (131/179)\u001b[K\rremote: Compressing objects:  74% (133/179)\u001b[K\rremote: Compressing objects:  75% (135/179)\u001b[K\rremote: Compressing objects:  76% (137/179)\u001b[K\rremote: Compressing objects:  77% (138/179)\u001b[K\rremote: Compressing objects:  78% (140/179)\u001b[K\rremote: Compressing objects:  79% (142/179)\u001b[K\rremote: Compressing objects:  80% (144/179)\u001b[K\rremote: Compressing objects:  81% (145/179)\u001b[K\rremote: Compressing objects:  82% (147/179)\u001b[K\rremote: Compressing objects:  83% (149/179)\u001b[K\rremote: Compressing objects:  84% (151/179)\u001b[K\rremote: Compressing objects:  85% (153/179)\u001b[K\rremote: Compressing objects:  86% (154/179)\u001b[K\rremote: Compressing objects:  87% (156/179)\u001b[K\rremote: Compressing objects:  88% (158/179)\u001b[K\rremote: Compressing objects:  89% (160/179)\u001b[K\rremote: Compressing objects:  90% (162/179)\u001b[K\rremote: Compressing objects:  91% (163/179)\u001b[K\rremote: Compressing objects:  92% (165/179)\u001b[K\rremote: Compressing objects:  93% (167/179)\u001b[K\rremote: Compressing objects:  94% (169/179)\u001b[K\rremote: Compressing objects:  95% (171/179)\u001b[K\rremote: Compressing objects:  96% (172/179)\u001b[K\rremote: Compressing objects:  97% (174/179)\u001b[K\rremote: Compressing objects:  98% (176/179)\u001b[K\rremote: Compressing objects:  99% (178/179)\u001b[K\rremote: Compressing objects: 100% (179/179)\u001b[K\rremote: Compressing objects: 100% (179/179), done.\u001b[K\n",
            "Receiving objects:   0% (1/274)   \rReceiving objects:   1% (3/274)   \rReceiving objects:   2% (6/274)   \rReceiving objects:   3% (9/274)   \rReceiving objects:   4% (11/274)   \rReceiving objects:   5% (14/274)   \rReceiving objects:   6% (17/274)   \rReceiving objects:   7% (20/274)   \rReceiving objects:   8% (22/274)   \rReceiving objects:   9% (25/274)   \rReceiving objects:  10% (28/274)   \rReceiving objects:  11% (31/274)   \rReceiving objects:  12% (33/274)   \rReceiving objects:  13% (36/274)   \rReceiving objects:  14% (39/274)   \rReceiving objects:  15% (42/274)   \rReceiving objects:  16% (44/274)   \rReceiving objects:  17% (47/274)   \rReceiving objects:  18% (50/274)   \rReceiving objects:  19% (53/274)   \rReceiving objects:  20% (55/274)   \rReceiving objects:  21% (58/274)   \rReceiving objects:  22% (61/274)   \rReceiving objects:  23% (64/274)   \rReceiving objects:  24% (66/274)   \rReceiving objects:  25% (69/274)   \rReceiving objects:  26% (72/274)   \rReceiving objects:  27% (74/274)   \rReceiving objects:  28% (77/274)   \rReceiving objects:  29% (80/274)   \rReceiving objects:  30% (83/274)   \rReceiving objects:  31% (85/274)   \rReceiving objects:  32% (88/274)   \rReceiving objects:  33% (91/274)   \rReceiving objects:  34% (94/274)   \rReceiving objects:  35% (96/274)   \rReceiving objects:  36% (99/274)   \rReceiving objects:  37% (102/274)   \rReceiving objects:  38% (105/274)   \rReceiving objects:  39% (107/274)   \rReceiving objects:  40% (110/274)   \rReceiving objects:  41% (113/274)   \rReceiving objects:  42% (116/274)   \rReceiving objects:  43% (118/274)   \rReceiving objects:  44% (121/274)   \rReceiving objects:  45% (124/274)   \rReceiving objects:  46% (127/274)   \rReceiving objects:  47% (129/274)   \rReceiving objects:  48% (132/274)   \rReceiving objects:  49% (135/274)   \rReceiving objects:  50% (137/274)   \rReceiving objects:  51% (140/274)   \rReceiving objects:  52% (143/274)   \rReceiving objects:  53% (146/274)   \rReceiving objects:  54% (148/274)   \rReceiving objects:  55% (151/274)   \rReceiving objects:  56% (154/274)   \rReceiving objects:  57% (157/274)   \rReceiving objects:  58% (159/274)   \rReceiving objects:  59% (162/274)   \rReceiving objects:  60% (165/274)   \rReceiving objects:  61% (168/274)   \rReceiving objects:  62% (170/274)   \rReceiving objects:  63% (173/274)   \rReceiving objects:  64% (176/274)   \rReceiving objects:  65% (179/274)   \rReceiving objects:  66% (181/274)   \rReceiving objects:  67% (184/274)   \rReceiving objects:  68% (187/274)   \rReceiving objects:  69% (190/274)   \rReceiving objects:  70% (192/274)   \rReceiving objects:  71% (195/274)   \rReceiving objects:  72% (198/274)   \rReceiving objects:  73% (201/274)   \rReceiving objects:  74% (203/274)   \rReceiving objects:  75% (206/274)   \rReceiving objects:  76% (209/274)   \rReceiving objects:  77% (211/274)   \rReceiving objects:  78% (214/274)   \rReceiving objects:  79% (217/274)   \rReceiving objects:  80% (220/274)   \rReceiving objects:  81% (222/274)   \rReceiving objects:  82% (225/274)   \rReceiving objects:  83% (228/274)   \rReceiving objects:  84% (231/274)   \rremote: Total 274 (delta 130), reused 216 (delta 83), pack-reused 0\u001b[K\n",
            "Receiving objects:  85% (233/274)   \rReceiving objects:  86% (236/274)   \rReceiving objects:  87% (239/274)   \rReceiving objects:  88% (242/274)   \rReceiving objects:  89% (244/274)   \rReceiving objects:  90% (247/274)   \rReceiving objects:  91% (250/274)   \rReceiving objects:  92% (253/274)   \rReceiving objects:  93% (255/274)   \rReceiving objects:  94% (258/274)   \rReceiving objects:  95% (261/274)   \rReceiving objects:  96% (264/274)   \rReceiving objects:  97% (266/274)   \rReceiving objects:  98% (269/274)   \rReceiving objects:  99% (272/274)   \rReceiving objects: 100% (274/274)   \rReceiving objects: 100% (274/274), 249.57 KiB | 10.85 MiB/s, done.\n",
            "Resolving deltas:   0% (0/130)   \rResolving deltas:   3% (4/130)   \rResolving deltas:   8% (11/130)   \rResolving deltas:  48% (63/130)   \rResolving deltas:  53% (69/130)   \rResolving deltas:  56% (74/130)   \rResolving deltas:  58% (76/130)   \rResolving deltas:  68% (89/130)   \rResolving deltas:  70% (92/130)   \rResolving deltas:  73% (95/130)   \rResolving deltas:  76% (99/130)   \rResolving deltas:  79% (103/130)   \rResolving deltas:  88% (115/130)   \rResolving deltas:  89% (116/130)   \rResolving deltas:  90% (117/130)   \rResolving deltas:  93% (122/130)   \rResolving deltas:  95% (124/130)   \rResolving deltas:  96% (125/130)   \rResolving deltas: 100% (130/130)   \rResolving deltas: 100% (130/130), done.\n",
            "Collecting ray==0.9.0.dev0\n",
            "  Using cached https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied, skipping upgrade: grpcio in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (1.28.1)\n",
            "Requirement already satisfied, skipping upgrade: msgpack<1.0.0,>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (0.6.2)\n",
            "Requirement already satisfied, skipping upgrade: jsonschema in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: redis<3.5.0,>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.16 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (1.18.4)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: aiohttp in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (3.6.2)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: google in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (2.0.3)\n",
            "Requirement already satisfied, skipping upgrade: py-spy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: colorama in /usr/local/lib/python3.6/dist-packages (from ray==0.9.0.dev0) (0.4.3)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from grpcio->ray==0.9.0.dev0) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->ray==0.9.0.dev0) (46.1.3)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (19.3.0)\n",
            "Requirement already satisfied, skipping upgrade: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (1.4.2)\n",
            "Requirement already satisfied, skipping upgrade: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (3.0.1)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: multidict<5.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (4.7.5)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (3.6.6)\n",
            "Requirement already satisfied, skipping upgrade: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray==0.9.0.dev0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from google->ray==0.9.0.dev0) (4.6.3)\n",
            "Requirement already satisfied, skipping upgrade: idna>=2.0 in /usr/local/lib/python3.6/dist-packages (from yarl<2.0,>=1.0->aiohttp->ray==0.9.0.dev0) (2.9)\n",
            "Installing collected packages: ray\n",
            "  Found existing installation: ray 0.9.0.dev0\n",
            "    Uninstalling ray-0.9.0.dev0:\n",
            "      Successfully uninstalled ray-0.9.0.dev0\n",
            "Successfully installed ray-0.9.0.dev0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ray"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cloudpickle==1.2.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.2.1)\n",
            "Requirement already satisfied: future==0.17.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.17.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.17.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.18.4)\n",
            "Requirement already satisfied: Pillow==6.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (6.2.0)\n",
            "Requirement already satisfied: pyglet==1.3.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: scipy==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: six==1.12.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (1.12.0)\n",
            "Requirement already satisfied: pytest_cases in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (1.13.1)\n",
            "Requirement already satisfied: deap in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (1.3.1)\n",
            "Requirement already satisfied: ray[rllib] in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (0.9.0.dev0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (2.2.0rc4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from pytest_cases->-r requirements.txt (line 9)) (1.12.1)\n",
            "Requirement already satisfied: decopatch in /usr/local/lib/python3.6/dist-packages (from pytest_cases->-r requirements.txt (line 9)) (1.4.8)\n",
            "Requirement already satisfied: makefun>=1.7 in /usr/local/lib/python3.6/dist-packages (from pytest_cases->-r requirements.txt (line 9)) (1.9.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (2.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (3.0.12)\n",
            "Requirement already satisfied: redis<3.5.0,>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (3.4.1)\n",
            "Requirement already satisfied: msgpack<1.0.0,>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (0.6.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (3.6.2)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (0.3.3)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (0.4.3)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (1.28.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (7.1.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (3.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (3.13)\n",
            "Requirement already satisfied: google in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (2.0.3)\n",
            "Requirement already satisfied: opencv-python-headless; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (4.2.0.34)\n",
            "Requirement already satisfied: pandas; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (1.0.3)\n",
            "Requirement already satisfied: tabulate; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (0.8.7)\n",
            "Requirement already satisfied: dm-tree; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (0.1.5)\n",
            "Requirement already satisfied: tensorboardX; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (2.0)\n",
            "Requirement already satisfied: lz4; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (3.0.2)\n",
            "Requirement already satisfied: atari-py; extra == \"rllib\" in /usr/local/lib/python3.6/dist-packages (from ray[rllib]->-r requirements.txt (line 11)) (0.2.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (3.2.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (0.34.2)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (2.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (0.9.0)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (2.2.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (2.10.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 12)) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]->-r requirements.txt (line 11)) (3.6.6)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]->-r requirements.txt (line 11)) (3.0.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]->-r requirements.txt (line 11)) (1.4.2)\n",
            "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]->-r requirements.txt (line 11)) (1.1.0)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]->-r requirements.txt (line 11)) (3.0.4)\n",
            "Requirement already satisfied: multidict<5.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]->-r requirements.txt (line 11)) (4.7.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray[rllib]->-r requirements.txt (line 11)) (19.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->ray[rllib]->-r requirements.txt (line 11)) (46.1.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from google->ray[rllib]->-r requirements.txt (line 11)) (4.6.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas; extra == \"rllib\"->ray[rllib]->-r requirements.txt (line 11)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas; extra == \"rllib\"->ray[rllib]->-r requirements.txt (line 11)) (2.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (1.7.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (1.6.0.post3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (3.2.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (0.4.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.6/dist-packages (from yarl<2.0,>=1.0->aiohttp->ray[rllib]->-r requirements.txt (line 11)) (2.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (3.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 12)) (3.1.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement ran (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for ran\u001b[0m\n",
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 11.5 GB  | Proc size: 1.7 GB\n",
            "GPU RAM Free: 15891MB | Used: 389MB | Util   2% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ojEPVGZmvTAe",
        "colab": {}
      },
      "source": [
        "import array\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "from typing import Dict\n",
        "\n",
        "from deap import algorithms\n",
        "from deap import base\n",
        "from deap import creator\n",
        "from deap import tools\n",
        "import gym\n",
        "\n",
        "from environments import SimpleEnv\n",
        "import importlib\n",
        "from environments import SimpleEnv\n",
        "importlib.reload(SimpleEnv)\n",
        "\n",
        "from ray.rllib.agents import ppo\n",
        "from ray import tune\n",
        "from ray.rllib.policy.policy import Policy\n",
        "from ray.rllib.env import BaseEnv\n",
        "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
        "from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker\n",
        "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
        "import ray\n",
        "\n",
        "import time\n",
        "start = time.process_time()\n",
        "\n",
        "from ray import tune\n",
        "\n",
        "SimpleEnv = SimpleEnv.SimpleEnv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wvaDdyNmEQ-o",
        "colab": {}
      },
      "source": [
        "# some settings that we can tweak:\n",
        "CONFIG_TRAINING={\n",
        "        \"MAX_STEP_COUNT\": 1, # number of steps in each round of DEAP evolution \n",
        "        \"POPULATION_SIZE\": 1, # population in each round of DEAP evolution \n",
        "        \"N_GEN\": 1, # number of rounds of DEAP evolution This has the most effect\n",
        "        \"N_RL_TRANING\" : 1, # number of rounds of RL training\n",
        "        \"TRAIN_BATCH_SIZE\" : 1, #Batch size for RL training\n",
        "        \"LEARNING_RATE\" : 0.001 #learning rate for RL training\n",
        "        }\n",
        "\n",
        "TUNNING = \"RL\" # \"RL\" / \"EV\" / \"OFF\". \n",
        "\n",
        "if TUNNING == \"RL\":\n",
        "  EVAL_METHOD = \"RL\"  # RL (train reward function and agent) or OPTIMAL (only train reward function)\n",
        "  # EVAL_METHOD = \"OPTIMAL\" #-----------need to delete\n",
        "elif TUNNING == \"EV\":\n",
        "  EVAL_METHOD = \"OPTIMAL\"  # RL (train reward function and agent) or OPTIMAL (only train reward function)\n",
        "elif TUNNING == \"OFF\":\n",
        "  EVAL_METHOD = \"RL\"  # RL (train reward function and agent) or OPTIMAL (only train reward function)\n",
        "\n",
        "# List of indivual reward functions used for tunning parameters for evolution\n",
        "INDIVIDUAL = [[0.9298461960519508, 0.7449587149229808, 0.4628576259710946, -0.282921307700329, -0.7019321146761455, 0.618488821337605]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_cnG-FTpEQ-1",
        "outputId": "d1db4ebd-72d8-443d-82c1-7ae45aa99c29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "ray.shutdown()\n",
        "ray.init(webui_host='127.0.0.1') #Use this line to fix the error indicated in: https://github.com/ray-project/ray/issues/7084#issuecomment-584505519\n",
        "n_agents = 3\n",
        "n_var = 2\n",
        "training_envs = [\n",
        "(SimpleEnv, {\n",
        "    'n_agents': n_agents,\n",
        "    'n_vars': n_var,\n",
        "    'true_reward_weights': [1, 0],\n",
        "    'max_step_count': CONFIG_TRAINING[\"MAX_STEP_COUNT\"],\n",
        "}),\n",
        "(SimpleEnv, {\n",
        "    'n_agents': n_agents,\n",
        "    'n_vars': n_var,\n",
        "    'true_reward_weights': [0, 1],\n",
        "    'max_step_count': CONFIG_TRAINING[\"MAX_STEP_COUNT\"],\n",
        "})]\n",
        "test_env = (SimpleEnv, {\n",
        "    'n_agents': n_agents,\n",
        "    'n_vars': n_var,\n",
        "    'true_reward_weights': [1, 1],\n",
        "    'max_step_count': CONFIG_TRAINING[\"MAX_STEP_COUNT\"],\n",
        "})\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-09 22:56:30,601\tINFO resource_spec.py:212 -- Starting Ray with 7.08 GiB memory available for workers and up to 3.56 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
            "2020-05-09 22:56:31,007\tINFO services.py:1170 -- View the Ray dashboard at \u001b[1m\u001b[32m127.0.0.1:8265\u001b[39m\u001b[22m\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yn9RzymlEQ_G",
        "colab": {}
      },
      "source": [
        "# Directly generate \"optimal action\" suggested by the reward function created by Evolution\n",
        "# reward_weights is created from env(config=env_config)\n",
        "def get_optimal_action(reward_weights, env_config):\n",
        "    n_vars = env_config[\"n_vars\"]\n",
        "    max_act = 5    \n",
        "    \n",
        "    reward_scale_factor = np.array([2]*n_vars + [1]*(len(reward_weights)-n_vars))\n",
        "    action = reward_weights * reward_scale_factor\n",
        "    action = action / np.max(action) * max_act\n",
        "    \n",
        "    action = np.reshape(action, [n_agents, n_vars])\n",
        "    return action\n",
        "\n",
        "# If we do not want to optimize RL but just want to optimize reward function using Evolution function\n",
        "# then here we just do the \"optimal action\" suggested by the generated reward function\n",
        "def evaluate_individual_env_optimal_act(individual, environment_fn, env_config, config_trianing):\n",
        "    env_config['reward_weights'] = np.array([individual for i in range(n_agents)])\n",
        "    env = environment_fn(config=env_config)\n",
        "    \n",
        "    ave_true_rewards = 0\n",
        "    obs = env.reset()\n",
        "    ave_reward = 0\n",
        "    for _ in range(env_config[\"max_step_count\"]):\n",
        "        actions = {i: get_optimal_action(env.reward_weights[i], env_config) for i in range(n_agents)}\n",
        "        obs, reward, _, _ = env.step(actions)\n",
        "        reward = np.array([reward[i] for i in range(len(reward))])\n",
        "        ave_reward += reward\n",
        "        ave_true_rewards += env.last_true_reward\n",
        "        \n",
        "    ave_true_rewards /= env_config[\"max_step_count\"]\n",
        "    ave_reward /= env_config[\"max_step_count\"]\n",
        "    \n",
        "    return np.mean(ave_true_rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oP1G3iuuolJi",
        "colab": {}
      },
      "source": [
        "# train RL agent with given setup\n",
        "def evaluate_individual_env_rl(individual, environment, env_config, config_trianing):\n",
        "    \"\"\"Runs the environment. All agents have the same policy.\n",
        "  It returns the total true reward as the fitness.\n",
        "  \"\"\"\n",
        "    # need to import many libaraies again, because otherwise tunning funciton will not find the libaraies. \n",
        "    import array\n",
        "    import random\n",
        "\n",
        "    import numpy as np\n",
        "    from typing import Dict\n",
        "\n",
        "    from deap import algorithms\n",
        "    from deap import base\n",
        "    from deap import creator\n",
        "    from deap import tools\n",
        "    import gym\n",
        "\n",
        "    from environments import SimpleEnv\n",
        "    import importlib\n",
        "    from environments import SimpleEnv\n",
        "    importlib.reload(SimpleEnv)\n",
        "\n",
        "    from ray.rllib.agents import ppo\n",
        "    from ray import tune\n",
        "    from ray.rllib.policy.policy import Policy\n",
        "    from ray.rllib.env import BaseEnv\n",
        "    from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
        "    from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker\n",
        "    from ray.rllib.agents.callbacks import DefaultCallbacks\n",
        "    import ray\n",
        "\n",
        "    import time\n",
        "    start = time.process_time()\n",
        "\n",
        "    from ray import tune\n",
        "\n",
        "    SimpleEnv = SimpleEnv.SimpleEnv\n",
        "    \n",
        "    import time\n",
        "    start = time.process_time()\n",
        "    # your code here    \n",
        "    print(\"Time Spent 1--- = \", (time.process_time() - start)/60, \" minutes\")  \n",
        "    \n",
        "    #Select random individuals from pop and create the reward weights\n",
        "    pop = np.array([individual for i in range(n_agents)])\n",
        "    reward_weights = pop\n",
        "    env_config['reward_weights'] = reward_weights\n",
        "    #env is only to get action space and observation space\n",
        "    env = environment(config=env_config)\n",
        "    class MyCallbacks(DefaultCallbacks):\n",
        "        #Callback functions to keep track of true reward while training\n",
        "        def on_episode_start(self, worker: RolloutWorker, base_env: BaseEnv,\n",
        "                         policies: Dict[str, Policy],\n",
        "                         episode: MultiAgentEpisode, **kwargs):\n",
        "            episode.user_data[\"true_rewards\"] = np.zeros(n_agents)\n",
        "\n",
        "        def on_episode_step(self, worker: RolloutWorker, base_env: BaseEnv,\n",
        "                        episode: MultiAgentEpisode, **kwargs):\n",
        "            env = base_env\n",
        "            true_reward = env.env_states[0].env.last_true_reward\n",
        "            episode.user_data[\"true_rewards\"] += true_reward\n",
        "\n",
        "        def on_episode_end(self, worker: RolloutWorker, base_env: BaseEnv,\n",
        "                       policies: Dict[str, Policy], episode: MultiAgentEpisode,\n",
        "                       **kwargs):\n",
        "            true_reward = episode.user_data[\"true_rewards\"]\n",
        "            for i, r in enumerate(true_reward):\n",
        "                episode.custom_metrics[\"true_reward_agent_\" + str(i)] = r\n",
        "    \n",
        "    print(\"Time Spent 2--- = \", (time.process_time() - start)/60, \" minutes\")  \n",
        "    \n",
        "    \n",
        "    # settings for the RL agent trainer     \n",
        "    config={\n",
        "        \"train_batch_size\": config_trianing[\"TRAIN_BATCH_SIZE\"],\n",
        "        \"lr\": config_trianing[\"LEARNING_RATE\"],\n",
        "        \"sgd_minibatch_size\": config_trianing[\"TRAIN_BATCH_SIZE\"],\n",
        "        \"multiagent\": {\n",
        "            \"policies\": {\n",
        "            },\n",
        "            \"policy_mapping_fn\":  #all agents share a policy\n",
        "                lambda agent_id:\n",
        "                    'agent'\n",
        "        },\n",
        "        \"model\": {\"fcnet_hiddens\": []},\n",
        "        'env_config': env_config,\n",
        "        \"callbacks\": MyCallbacks,\n",
        "    }\n",
        "    config['multiagent']['policies']['agent'] = (None, env.observation_space, env.action_space, {})\n",
        "    metrics = None\n",
        "\n",
        "\n",
        "    print(\"Time Spent 3--- = \", (time.process_time() - start)/60, \" minutes\")  \n",
        "    \n",
        "    counter = 0 # ------ need to take this out later. use counter to break.\n",
        "    while True:\n",
        "        counter += 1 # ------ need to take this out later. use counter to break.\n",
        "        trainer = ppo.PPOTrainer(env=environment, config=config)\n",
        "        true_reward_mean = 0\n",
        "        print(\"Time Spent 4--- = \", (time.process_time() - start)/60, \" minutes, counter = \", counter)  \n",
        "        \n",
        "        for i in range(config_trianing[\"N_RL_TRANING\"]):\n",
        "            true_reward_mean = 0\n",
        "            #Train the RL agent\n",
        "            print(\"Time Spent 5--(right before trainig)-- = \", (time.process_time() - start)/60, \" minutes, counter = \", counter)\n",
        "            # import pdb; pdb.set_trace()\n",
        "            print(\"config_trianing = \", config_trianing, \"individual=\", individual)\n",
        "\n",
        "            metrics = trainer.train()  # distributed training step\n",
        "            print(\"episode_reward_mean\", metrics[\"episode_reward_mean\"])\n",
        "            print(\"Time Spent 6--- = \", (time.process_time() - start)/60, \" minutes, counter = \", counter)  \n",
        "            # ------ need to take this out later. use counter to break.\n",
        "            if counter >= 1:\n",
        "                break\n",
        "\n",
        "            if metrics[\"episode_reward_mean\"] < 0 and i > 2:\n",
        "                break\n",
        "        #Train agent until it does well\n",
        "        if metrics[\"episode_reward_mean\"] > 0:\n",
        "            break\n",
        "        # ------ need to take this out later. use counter to break.\n",
        "        if counter >= 1:\n",
        "            break\n",
        "    \n",
        "    print(\"Time Spent 7--- = \", (time.process_time() - start)/60, \" minutes\")  \n",
        "    \n",
        "    print(\"episode_reward_mean\", metrics[\"episode_reward_mean\"])\n",
        "    for i in range(n_agents):\n",
        "        true_reward_mean += metrics['custom_metrics']['true_reward_agent_' + str(i) + '_mean']\n",
        "    true_reward_mean /= n_agents\n",
        "    print('Evaluated', individual, 'Fitness', true_reward_mean)\n",
        "\n",
        "    print(\"Time Spent 8--- = \", (time.process_time() - start)/60, \" minutes\")  \n",
        "    \n",
        "    return true_reward_mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dy4idwUWEQ_a",
        "colab": {}
      },
      "source": [
        "if EVAL_METHOD == \"RL\":\n",
        "    evaluate_individual_env = evaluate_individual_env_rl\n",
        "else:\n",
        "    evaluate_individual_env = evaluate_individual_env_optimal_act\n",
        "        \n",
        "def evaluate_individual(individual, config_trianing = CONFIG_TRAINING):\n",
        "    \"\"\"Runs all environments. \n",
        "  returns the average true reward over all environments as the fitness.\n",
        "  \"\"\"\n",
        "    \n",
        "    all_fitness = []\n",
        "    for env, config in training_envs:\n",
        "        all_fitness.append(evaluate_individual_env(individual, env, config, config_trianing))\n",
        "    fitness_comb = np.min(all_fitness)\n",
        "    return (fitness_comb, )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k1cp_IzAwg5n",
        "outputId": "02fbacc4-72bb-443e-8c04-fc32dd220ac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        }
      },
      "source": [
        "#function that tunes the reward function\n",
        "def train_reward_function(config_trianing):\n",
        "\n",
        "  creator.create('FitnessMax', base.Fitness, weights=(1.0, ))\n",
        "  creator.create('Individual', array.array, typecode='d',\n",
        "                fitness=creator.FitnessMax)\n",
        "\n",
        "  toolbox = base.Toolbox()\n",
        "\n",
        "  toolbox.register('attr', random.randint, 0, 1)\n",
        "  toolbox.register('individual', tools.initRepeat, creator.Individual,\n",
        "                  toolbox.attr, n_agents * n_var)\n",
        "  toolbox.register('population', tools.initRepeat, list,\n",
        "                  toolbox.individual)\n",
        "    # some setup for Deap\n",
        "  toolbox.register('evaluate', evaluate_individual)\n",
        "  toolbox.register('mate', tools.cxTwoPoint)\n",
        "  toolbox.register('mutate', tools.mutFlipBit, indpb=0.05) # add more noise\n",
        "  toolbox.register('select', tools.selTournament, tournsize=3)\n",
        "\n",
        "  # pop is a list individual reward function's weights \n",
        "  pop = toolbox.population(n=config_trianing[\"POPULATION_SIZE\"])\n",
        "  # the 10 best individual reward function (could even befround the first round)\n",
        "  hof = tools.HallOfFame(10)\n",
        "\n",
        "  stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "  stats.register('avg', np.mean)\n",
        "  stats.register('std', np.std)\n",
        "  stats.register('min', np.min)\n",
        "  stats.register('max', np.max)\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=config_trianing[\"N_GEN\"], \n",
        "                                    stats=stats, halloffame=hof, verbose=True)\n",
        "  best_individual = hof[0]\n",
        "  print(best_individual)\n",
        "  test_reward = evaluate_individual_env(best_individual, test_env[0], test_env[1], config_trianing)\n",
        "  print(\"---test_reward:\", test_reward)\n",
        "  if TUNNING == \"EV\":\n",
        "    tune.track.log(mean_accuracy=test_reward) \n",
        "\n",
        "def tune_rl_training(config_trianing):\n",
        "  print(\"---rl tunning starts\")\n",
        "  all_score = []\n",
        "  for individual in INDIVIDUAL:\n",
        "    all_score.append(evaluate_individual(config_trianing)[0])\n",
        "  tune.track.log(mean_accuracy=sum(all_score)/len(all_score))\n",
        "  \n",
        "\n",
        "if TUNNING == \"EV\":\n",
        "  analysis = tune.run(\n",
        "      train_reward_function, config={\n",
        "          \"N_GEN\": tune.grid_search(list(range(1, 20, 5))), \n",
        "          \"POPULATION_SIZE\": tune.grid_search(list(range(1, 20, 5)))\n",
        "          # \"N_GEN\": tune.grid_search([1]), \n",
        "          # \"POPULATION_SIZE\": tune.grid_search([1])\n",
        "          })\n",
        "  #print best config\n",
        "  print(\"Best config: \", analysis.get_best_config(metric=\"mean_accuracy\"))\n",
        "elif TUNNING == \"RL\":\n",
        "  analysis = tune.run(\n",
        "      tune_rl_training, config={\n",
        "        \"N_RL_TRANING\": tune.grid_search([1]),\n",
        "        \"TRAIN_BATCH_SIZE\": tune.grid_search([1]),\n",
        "        \"LEARNING_RATE\": tune.grid_search([0.001])\n",
        "      })\n",
        "  print(\"Best config: \", analysis.get_best_config(metric=\"mean_accuracy\"))\n",
        "elif TUNNING == \"OFF\":\n",
        "  train_reward_function(CONFIG_TRAINING)\n",
        "\n",
        "\n",
        "\n",
        "# print ('pop', pop)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-09 22:56:33,206\tWARNING tune.py:316 -- Tune detects GPUs, but no trials are using GPUs. To enable trials to use GPUs, set tune.run(resources_per_trial={'gpu': 1}...) which allows Tune to expose 1 GPU to each trial. You can also override `Trainable.default_resource_request` if using the Trainable API.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.0/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/2 CPUs, 0/1 GPUs, 0.0/7.08 GiB heap, 0.0/2.44 GiB objects<br>Result logdir: /root/ray_results/tune_rl_training<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                  </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  LEARNING_RATE</th><th style=\"text-align: right;\">  N_RL_TRANING</th><th style=\"text-align: right;\">  TRAIN_BATCH_SIZE</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>tune_rl_training_53f25_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">          0.001</td><td style=\"text-align: right;\">             1</td><td style=\"text-align: right;\">                 1</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=4241)\u001b[0m ---rl tunning starts\n",
            "\u001b[2m\u001b[36m(pid=4241)\u001b[0m Time Spent 1--- =  1.746666666117373e-08  minutes\n",
            "\u001b[2m\u001b[36m(pid=4241)\u001b[0m Time Spent 2--- =  8.5957449999996e-05  minutes\n",
            "\u001b[2m\u001b[36m(pid=4241)\u001b[0m Time Spent 3--- =  8.676783333333068e-05  minutes\n",
            "\u001b[2m\u001b[36m(pid=4241)\u001b[0m 2020-05-09 22:56:37,114\tINFO trainable.py:217 -- Getting current IP.\n",
            "\u001b[2m\u001b[36m(pid=4241)\u001b[0m 2020-05-09 22:56:37,129\tINFO trainer.py:425 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
            "\u001b[2m\u001b[36m(pid=4241)\u001b[0m 2020-05-09 22:56:37,158\tINFO trainer.py:584 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            "\u001b[2m\u001b[36m(pid=4242)\u001b[0m /usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "\u001b[2m\u001b[36m(pid=4242)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "\u001b[2m\u001b[36m(pid=4241)\u001b[0m Time Spent 4--- =  0.08420111538333332  minutes, counter =  1\n",
            "\u001b[2m\u001b[36m(pid=4241)\u001b[0m Time Spent 5--(right before trainig)-- =  0.08420190224999999  minutes, counter =  1\n",
            "\u001b[2m\u001b[36m(pid=4241)\u001b[0m config_trianing =  {'MAX_STEP_COUNT': 1, 'POPULATION_SIZE': 1, 'N_GEN': 1, 'N_RL_TRANING': 1, 'TRAIN_BATCH_SIZE': 1, 'LEARNING_RATE': 0.001} individual= {'N_RL_TRANING': 1, 'TRAIN_BATCH_SIZE': 1, 'LEARNING_RATE': 0.001}\n",
            "\u001b[2m\u001b[36m(pid=4241)\u001b[0m 2020-05-09 22:56:42,967\tINFO trainable.py:217 -- Getting current IP.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-05-09 22:56:51,310\tWARNING worker.py:1094 -- The actor or task with ID ffffffffffffffff19e86ed90100 is pending and cannot currently be scheduled. It requires {CPU: 1.000000} for execution and {CPU: 1.000000} for placement, but this node only has remaining {node:172.28.0.2: 1.000000}, {memory: 7.080078 GiB}, {GPU: 1.000000}, {object_store_memory: 2.441406 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n",
            "2020-05-09 22:56:51,922\tINFO (unknown file):0 -- gc.collect() freed 60520 refs in 0.5107968550000805 seconds\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=4242)\u001b[0m 2020-05-09 22:56:51,649\tINFO (unknown file):0 -- gc.collect() freed 16 refs in 0.23870680999993965 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ztw5G6VWERA6",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "print(evaluate_individual([0.9298461960519508, 0.7449587149229808, 0.4628576259710946, -0.282921307700329, -0.7019321146761455, 0.618488821337605]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GS2i0F_qERBb",
        "colab": {}
      },
      "source": [
        "print(evaluate_individual([1, 0, -1, 0, -1, 0])) #Worst reward, selfish agent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PiWsv5EGERCQ",
        "colab": {}
      },
      "source": [
        "print(\"Time Spent = \", (time.process_time() - start)/60, \" minutes\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}