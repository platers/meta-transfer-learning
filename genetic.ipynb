{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sdi2UTA8vWEJ"
   },
   "source": [
    "A proof of concept showing using a genetic algorithm with our environment.\n",
    "It is similar to https://github.com/DEAP/deap/blob/a0b78956e28387785e3bb6e2b4b1f1b32c2b3883/examples/ga/onemax_short.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Qv1kWykfqRLl",
    "outputId": "eda4b75a-48ff-4852-a019-b40577db74e0"
   },
   "outputs": [],
   "source": [
    "# Run this cell if you're using colab. Otherwise, skip it.\n",
    "\n",
    "!git clone https://github.com/platers/meta-transfer-learning.git\n",
    "\n",
    "import os\n",
    "os.chdir('meta-transfer-learning')\n",
    "\n",
    "!pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.9.0.dev0-cp36-cp36m-manylinux1_x86_64.whl\n",
    "\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ojEPVGZmvTAe"
   },
   "outputs": [],
   "source": [
    "import array\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "\n",
    "from deap import algorithms\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "import gym\n",
    "\n",
    "from environments import SimpleEnv\n",
    "import importlib\n",
    "importlib.reload(SimpleEnv)\n",
    "\n",
    "from ray.rllib.agents import ppo\n",
    "from ray import tune\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "oP1G3iuuolJi",
    "outputId": "d957997c-cf69-42eb-c90c-91a90fdaa8be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-03 19:16:34,211\tINFO resource_spec.py:212 -- Starting Ray with 14.84 GiB memory available for workers and up to 7.44 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-05-03 19:16:34,334\tWARNING services.py:928 -- Redis failed to start, retrying now.\n",
      "2020-05-03 19:16:34,698\tINFO services.py:1170 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "\n",
    "creator.create('FitnessMax', base.Fitness, weights=(1.0, ))\n",
    "creator.create('Individual', array.array, typecode='d',\n",
    "               fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "toolbox.register('attr', random.uniform, -1, 1)\n",
    "toolbox.register('individual', tools.initRepeat, creator.Individual,\n",
    "                 toolbox.attr, 2)\n",
    "toolbox.register('population', tools.initRepeat, list,\n",
    "                 toolbox.individual)\n",
    "\n",
    "n_agents = 3\n",
    "\n",
    "def evaluate_individual(individual):\n",
    "    \"\"\"Runs the environment. All agents have the same policy.\n",
    "  It returns the total true reward as the fitness.\n",
    "  \"\"\"\n",
    "    \n",
    "    #Select random individuals from pop and create the reward weights\n",
    "    pop = np.array([individual for i in range(n_agents)])\n",
    "    reward_weights = pop\n",
    "    #print(individual, reward_weights)\n",
    "    \n",
    "    #env is only to get action space and observation space\n",
    "    env = SimpleEnv.SimpleEnv(config={\n",
    "        'n_agents': n_agents,\n",
    "        'n_vars': 2,\n",
    "        'reward_weights': reward_weights,\n",
    "        'max_step_count': 20,\n",
    "    })\n",
    "    class MyCallbacks(DefaultCallbacks):\n",
    "        #Callback functions to keep track of true reward while training\n",
    "        def on_episode_start(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                         policies: Dict[str, Policy],\n",
    "                         episode: MultiAgentEpisode, **kwargs):\n",
    "            episode.user_data[\"true_rewards\"] = np.zeros(n_agents)\n",
    "\n",
    "        def on_episode_step(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                        episode: MultiAgentEpisode, **kwargs):\n",
    "            env = base_env\n",
    "            #print(env.env_states[0].env.last_true_reward)\n",
    "            true_reward = env.env_states[0].env.last_true_reward\n",
    "            episode.user_data[\"true_rewards\"] += true_reward\n",
    "\n",
    "        def on_episode_end(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                       policies: Dict[str, Policy], episode: MultiAgentEpisode,\n",
    "                       **kwargs):\n",
    "            true_reward = episode.user_data[\"true_rewards\"]\n",
    "            for i, r in enumerate(true_reward):\n",
    "                episode.custom_metrics[\"true_reward_agent_\" + str(i)] = r\n",
    "            \n",
    "    config={\n",
    "        \"multiagent\": {\n",
    "            \"policies\": {\n",
    "            },\n",
    "            \"policy_mapping_fn\":  #all agents share a policy\n",
    "                lambda agent_id:\n",
    "                    'agent'\n",
    "        },\n",
    "        'env_config': {\n",
    "            'n_agents': n_agents,\n",
    "            'n_vars': 2,\n",
    "            'reward_weights': reward_weights,\n",
    "            'max_step_count': 20,\n",
    "        },\n",
    "        \"callbacks\": MyCallbacks,\n",
    "    }\n",
    "    config['multiagent']['policies']['agent'] = (None, env.observation_space, env.action_space, {})\n",
    "    trainer = ppo.PPOTrainer(env=SimpleEnv.SimpleEnv, config=config)\n",
    "    \n",
    "    true_reward_mean = 0\n",
    "    for i in range(10):\n",
    "        #print('TRAINING', i)\n",
    "        true_reward_mean = 0\n",
    "        custom_metrics = trainer.train()['custom_metrics']  # distributed training step\n",
    "        print(custom_metrics)\n",
    "        for i in range(n_agents):\n",
    "            true_reward_mean += custom_metrics['true_reward_agent_' + str(i) + '_mean']\n",
    "    true_reward_mean /= n_agents\n",
    "    #print('true reward', trainer.collect_metrics()['custom_metrics']['true_reward_mean'])\n",
    "    return (true_reward_mean, )\n",
    "\n",
    "\n",
    "toolbox.register('evaluate', evaluate_individual)\n",
    "toolbox.register('mate', tools.cxTwoPoint)\n",
    "toolbox.register('mutate', tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register('select', tools.selTournament, tournsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "k1cp_IzAwg5n",
    "outputId": "fc500615-efde-4668-e94a-685f9ef3df07"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-03 19:16:34,892\tINFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "2020-05-03 19:16:34,975\tINFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2020-05-03 19:16:42,085\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-05-03 19:16:42,086\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_reward_agent_0_mean': -0.3909149669976614, 'true_reward_agent_0_min': -14.030345596373081, 'true_reward_agent_0_max': 18.35329858958721, 'true_reward_agent_1_mean': 0.43086195279916867, 'true_reward_agent_1_min': -12.308409988880157, 'true_reward_agent_1_max': 15.415276186540723, 'true_reward_agent_2_mean': 0.20549897973389306, 'true_reward_agent_2_min': -12.40451440308243, 'true_reward_agent_2_max': 16.42576726898551}\n",
      "{'true_reward_agent_0_mean': -0.16364521468043677, 'true_reward_agent_0_min': -39.678820088505745, 'true_reward_agent_0_max': 34.041620284318924, 'true_reward_agent_1_mean': -0.757772793855911, 'true_reward_agent_1_min': -33.2704441845417, 'true_reward_agent_1_max': 37.44344946742058, 'true_reward_agent_2_mean': 0.5599362102228043, 'true_reward_agent_2_min': -38.129532903432846, 'true_reward_agent_2_max': 46.4374435544014}\n",
      "{'true_reward_agent_0_mean': 1.8796501991023251, 'true_reward_agent_0_min': -45.404886692762375, 'true_reward_agent_0_max': 32.31058908998966, 'true_reward_agent_1_mean': 2.490254999096651, 'true_reward_agent_1_min': -44.794004172086716, 'true_reward_agent_1_max': 44.01907596364617, 'true_reward_agent_2_mean': -3.2277981691491733, 'true_reward_agent_2_min': -46.78744786977768, 'true_reward_agent_2_max': 50.1492932587862}\n",
      "{'true_reward_agent_0_mean': 2.8288899641408354, 'true_reward_agent_0_min': -44.80220939218998, 'true_reward_agent_0_max': 37.78765127575025, 'true_reward_agent_1_mean': 4.496665825198725, 'true_reward_agent_1_min': -39.22873514145613, 'true_reward_agent_1_max': 43.250210330821574, 'true_reward_agent_2_mean': -7.876427468521288, 'true_reward_agent_2_min': -48.206314004957676, 'true_reward_agent_2_max': 52.75619853660464}\n",
      "{'true_reward_agent_0_mean': -0.10339878755225072, 'true_reward_agent_0_min': -43.82660421729088, 'true_reward_agent_0_max': 35.547034844756126, 'true_reward_agent_1_mean': 2.2679202078734852, 'true_reward_agent_1_min': -42.576541751623154, 'true_reward_agent_1_max': 45.00417306646705, 'true_reward_agent_2_mean': -8.68778630640988, 'true_reward_agent_2_min': -51.09461031854153, 'true_reward_agent_2_max': 47.2431580722332}\n",
      "{'true_reward_agent_0_mean': 0.41214240680063086, 'true_reward_agent_0_min': -42.38741680979729, 'true_reward_agent_0_max': 36.60293603804894, 'true_reward_agent_1_mean': 1.2665178882861527, 'true_reward_agent_1_min': -46.853105165064335, 'true_reward_agent_1_max': 44.70259040594101, 'true_reward_agent_2_mean': -8.678181360167706, 'true_reward_agent_2_min': -50.227982223033905, 'true_reward_agent_2_max': 53.237435042858124}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pop = toolbox.population(n=n_agents)\n",
    "hof = tools.HallOfFame(10)\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register('avg', np.mean)\n",
    "stats.register('std', np.std)\n",
    "stats.register('min', np.min)\n",
    "stats.register('max', np.max)\n",
    "\n",
    "pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=2, \n",
    "                                   stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "print ('pop', pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FHACUooCwi91"
   },
   "outputs": [],
   "source": [
    "pop = [[1, -1], [1, -1], [1, -1]]\n",
    "print(evaluate(pop))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "genetic.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
